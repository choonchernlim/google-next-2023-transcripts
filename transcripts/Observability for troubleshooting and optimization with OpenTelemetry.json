{
    "title": "Observability for troubleshooting and optimization with OpenTelemetry",
    "presentation_type": "Breakout",
    "categories": [
        "Application Developers",
        "DEV300"
    ],
    "video_id": "RjX7t-b04hE",
    "time": "Aug 30 06:45 PM - 07:30 PM CDT",
    "transcript": "foreign[Music]this is the observability fortroubleshooting optimization with openTelemetry sessionum real quick though I want to startwith just a quick like hand summary forfolks here how many of you are familiarwith open telemetryhow many of you are using open telemetryall right how many of you are using logsfrom open telemetryhow many people are using metrics fromopen Telemetry awesome how many peopleare using tracesvery cool okayawesome that's great yeah good to knowum so we'll introduce ourselves quickI'm Josh saret I'm a technical lead atGoogle Cloud I'm responsible forobservability and data collection I'malso on the open Telemetry technicalcommittee so I contribute to it day today that's kind of what I do and I'mMike Kelly I'm the CEO at observe IQ andat observe IQ we build we're thecreators of buying plane it's aTelemetry pipeline orchestrates openTelemetry agents and we also contributequite a bit to the open Telemetryprojectso uh today we're going to be walkingyou through three things basically kindof a what is a hotel so for half of youthat might be a little bit of refresherthe second is kind of major Winds ofadoption hotel that we've seenum and then the third is kind of anapproach to adoption for how you cankind of use it in your organization andget startedum so since I know it sounds like a lotof you already are familiar with openTelemetry we'll keep some of this briefbut at a high level open Telemetry isobservability framework and it'sdesigned to create and manage yourTelemetry data and what that means is itis a set of libraries sdks agents it hasthe goal of of providing a singlesolution for Gathering processing andtransmitting all of your Telemetry dataum and in the past that's been the achallenging thing to solve becauseyou're stuck with a assortment ofdifferent proprietary open sourcesolutions to do the same thing it reallywas the industry coming together tosolve this uh and that's why it's beensomething that is significant and thatwe want to talk aboutthe other waysothere are three thingswe're going to talk about with openTelemetry and there are this is a verylarge project obviously but the threekind of core pillars we're going to talkabout are the protocolso this is you know well-definedprotocols specifications it's really thetheum the base that everything else isbuilt onand it's uh the standards that areneeded for collecting and processing thedatathe next is instrumentation so talkingabout the sdks libraries that you usewithin your own code Autoinstrumentation and others and thenfinally the hotel collector this is theThe Collector or agent spender agnosticit's what what does the the bulk of thatprocessing of data and can be used tostream and create your Telemetrypipelines within your environmentsand then one last thing before we gointo the details because I think it youknow it's really important to understandwhyuh what problem is this solving rightand we think of this in a few there area few key areas a few big big challengesthat you run into with Telemetrycollection one is standardization andsimplification so that is something thatopen Telemetry provides and it'ssomething that without it can beextremely difficult and you know Italked with a customerum he's working in observability at avery large financial institution and Iask like I usually do at the beginningyou know what are you using forobservability in terms of platforms whatdo you use for application monitoringwhat do you use them for security andhis answer was all of it right and Isuspect that that is you know people inthis room know that feeling ideally wewant to consolidate tools we alsorecognize that the reality is thatthat's really challenging and theproblem is you know I think right nowthere's a lot of focus on theconsolidation of tools how do we solveit from that end but if you have a dozendifferent solutionsfrequently you have a dozen differentways to to collect that data and processit and transmit it and that is a verydifficult problem and that's one thatopen Telemetry solves so standardizingsimplifying that is coreand then the next in and this is anothercritical piece is that it's vendorneutral right so a core goal of openTelemetry from the very start was toprovide a vendor neutral open sourcesolution to Gathering data and that'sthat's what it's doneum and last without without industrysupport this isn't a solution that youcan really adopt in and count on andthis has open telemetryyou know it's a cncf project the secondlargest behind kubernetes and it has thethe support of the vast majority of theindustry more and more platforms theythey either have exporters to supportdata from open Telemetry or actuallyhave native support for open cylindryprotocolsyeah another thing I want to call outhere is you know with open Telemetry youcan keep your answer of I use all of thevendors if you want that's totally finewhat this does is let you change thatanswer as you wish going forwardI'm sure lots of people want to keepeverything rightuh so we're going to talk about theprotocolum what's really important about theprotocol is that it's the foundation toeverything this is what gives you thatflexibility and this is what lets youchange uh your decision of yourobservability technology over time isthat we have this lingua Franca of howto communicate the data how to get itfrom A to Bum there's a like three key parts ofthis one is you know it's a unifiedstandard this is a standard that webuiltum and and we looked at existingstandards and kind of tried to uh blendthem together so for example we spent alot of time with the Prometheusecosystem guaranteeing that openTelemetry metrics and Prometheus metricsare compatible if you've seen recentlyPrometheus adopted otlp for push-basedmetrics so now a Prometheus server caneven ingest otlp as well that's becausewe've spent so much time making surethat this protocol will be the standardand has pulled in all of theseorganizations and ecosystems aroundobservability we delayed a good bit tomake sure that that was the case buttoday the protocol is stable it's 1.0and it has logs it has metrics it hastraces coming soon we hope it hasprofilingum the other thing about this is It'scorrelated uh one of the very principledopinions of open Telemetry is you shouldbe able to use whatever observabilitydata you have and use it in a correlatedwayas opposed toum you know struggling to understand Igot this alert now how do I find my logsright you should have a consistent setof labels that are available across allof them should have consistent ways ofmigrating from one signal to the nextand the last bit is that it's statelessthe way that this should scale the waythat you should use this protocolinternally should match how you scaleyour own systems generally stateless iskind of a big deal you know for forservers we understand how to use loadbalancers that's a foundational kind ofcore component of most systems so let'smake sure this protocol scale is thesame way you're used to scalingand that's a key component of it we'lltalk about it more towards the end wetalk about deployment models but itallows you to do things that you youreally couldn't otherwiseso now I'll talk just for a second aboutthe open Telemetry collector this is acollector sometimes I'll I'll use thatinterchangeably with an agent that'sdesigned to receive process and exportdatathat's it but there's a lot there rightand it's for all of your metrics yourtraces and logs events it's reallydesigned to handle all Telemetry datait's also designed to be performantit's designed with usability in mindit's designed to be extensible either byby you and your organizations or thirdparties and it it removes the need tohave proprietary agents multiple agentsthat you're managing it really should becapable of replacing a lot of the agentsthat you have in your environmentand I will say you know when we talkabout the performance usability andextensibility if you go to opentelemetry.io you'll see that that isright in the primary goals that theentire team has agreed tothere are several components to thehotel collector I'm just going to talkaboutThe Big Threebecause these are the core pieces thatallow you to ingest to process intotransmit one is receivers and a receiveris is just a component in The Collectorthat allows you to ingest data it couldbe gathering logs it could bescraping metrics from an applicationlike in a Apache serverum yeah it could be ingesting data fromotlp or from syslog or other standardsbut the idea behind this is that youshould be able to collect all the datathat you need from common applicationsthere are over 90 official receiversright now that's in the those are in thehotel project and there are othersprovided by Third parties you can buildyour own and include them in your owndistribution of The Collector as wellonce you have the data inthen you want to process it and and thisis optional but frequently this is whereyou see a lot of benefit from opentelemetryand processors just transform the datajust like it sounds but they includethings like adding metadata so maybe youwant to add additional labels or keyinformation as it flows through yourTelemetry pipeline it also allows you todo things like redacting data so if youwant to remove identifiable informationright at the host you can do thatfiltering batching these are all thingsthat you can do with with processorsand then finally once you've done allthat you've you've ingested you'veprocessed you need to send it somewhereand that's what exporters are for andagain just like it sounds there areexporters for all the commonobservability platforms uh you can senddata to Google Cloud Kafka Cassandrareally I could name all the vendors andthey typically have an exporteravailable to send data from openTelemetry and I'll note that there is areceiver for otlp and an exporter forotlp the standard open Telemetryprotocol and that is critical forchaining these collectors together soyou have a true Pipeline and you cansend the data in a standard formatso next we're going to talk aboutinstrumentation right so this is youknow collectors is how do I take datathat exists and get it from A to Binstrumentation is how do I make thatdata to begin with right where do I getmy latency metrics where do I get mynetwork metrics where do those CPU diskmemory networks come in and there'sbasically three sources that we that wethink about within open Telemetry rightthe first is what we call external rightexternal would be if I'm using say aJava application how many people hereuse Java yeah awesome okay if you'refamiliar with jmx it was this managementprotocol right that was designed toactually directly update values and westarted tomay be misuse the protocol forobservability where we will call thisand look at the values that wereavailable this management interface andturn them into metrics open Telemetryprovides this in that collector as areceiver it's a way for you to get dataand observable data that you have todayright Prometheus exporters if you'refamiliar with the Prometheus Eco systemare what we would consider an externalkind of instrumentation right you runthem on the side of the thing you'reobservingthe next type is automated right or andthis is kind ofum sometimes we call it auto magic rightthis is where I can actually take aprocess that was not originallyinstrumented and I somehow manipulate itso now it is generating the Telemetrydirectly in open Telemetry we have a fewinstances of this there is a Java agentthat you can run that will automaticallygenerate Telemetry there is one for netfor Python and JavaScript we call themagents but really what they are isbefore we run your python we do animport where we inject a whole bunch ofinstrumentation before and same withnode.js we'll inject a bunch of stuffand generate signals there they supporta whole bunch of common Frameworks youknow flask forumuh python my bad and uh like express.jsfor JavaScript for example those areautomatically supported you'll getlatency metrics that sort of thing outof the boxum so that involves very little lift onyour part and the last is what we callmanual instrumentation and this is whereyou need some developer to write code togenerate a signalwhat we want to reserve this for alittle bit is that when you have to takethat level of effort it should beimportant to your business right likeI'm going to track maybe how manycheckouts I have if I'm selling stuff orI want to track something very specificright I will have to manually go createa metric and do that we have apis andsdks for that but ideally you shouldhave instrumentation from the Frameworksand of whatever open source you're usingIF where it's available and manualsshould be limited to things that arevery specific to your company if I havea specific framework or whatever weprovide all three of those and that'skind of how to view that spaceokay so next up we're going to talkabout major wins of using open Telemetrywe're going to give you I think threedifferent demos of some different thingsthat we've done and kind of talk throughlike why this is awesome or at least whywe think it's awesomeuh so the first piece and I I talkedabout this a little bit already but Ithink the big win from adopting openTelemetry especially if you're able todeploy this throughout your organizationis standardization and withstandardization comes the creation of adata plane right if you haveproprietary instrumentation other opensource Solutions all mixed together youreally don't have that data plan itbecomes really challenging to do some ofthe things that we're going to show youbut when you've standardized you have asingle system to learn one system todeploy one system to maintain that byitself is is a pretty significant win itreduces complexity significantespecially as you scale out so in thelargest environments are the ones thatsee the biggest benefit from thisit improves control of your data sothat's one of the big goals of openTelemetry is to give you control of yourdataand that is something that you gain withwith the data plane and there's novendor lock-in right one of the thegreat benefits once you have this inplace is the ability to add additionaldestinations really easily so one of thechallenges that people run into is ifyou want to add maybe you just want totest out a new solution a new securitysolution a new application monitoringsolutionfor some organizations that is a heavylift and I've talked to folks that saidthat's a six-month rollout for us weneed to reinstrument everything we needto redeploy all of this if you have a atrue data plane it's a matter ofredirecting rerouting that data soyou've already you already have accessto it you own it you know we're justsending it to a different destinationanother example that would be say I haveI have a cloud deployment and I have anon-prem deployment right and I have alocal observability solution for on-prembut I want a global view of everythingwell now I can route both places right Ican send what data I want globallyglobally and I can keep what I wantlocally locally so I can do bothso now we're going to talk a little bitaboutum a little bit about tracing andoptimization from open Telemetry so inthis example basically uh we we try tocreate aum an e-commerce site that does auctionsright and we're designing this usingmicroservices in gke autopilot and wehave a couple microservices runningum we have some user behavior that we'relooking at users are able to kind ofview items they're able to place bids onavailable items they're able to createnew auctions and they're able tofinalize an auction and delete things sowhat you'll see as we walk through thisdemo is you'll see those kind of show upas use cases that we'll see in tracesum but we were running into a problem aswe ran through this soall rightall right so what you see here is the umGoogle Cloud Trace API we're using thatoneum and uh you know obviously you can useeverything with open Telemetry but inhere you'll see uh just a summary of oftraces overall of like traces in thepast we're going to go into the traceExplorerwhat we see hereis a set of examples you know where auser actually talked to uh us and we'relooking at the latency that occurredthatum on the right here are kind ofattributes associated with with uh spansand things whereum that tell us specifics to dodiagnostics on the left here we actuallysee the latency of Individual Servicesand calls if we look you can see there'slike git slash there's git slash actionsget slash check those are actually callsHTTP calls to different microservicesthe forum and we can look at different ones ofthese and kind of compare things whatI'm looking at right now is I'm noticingthat read Json which is an independentspan I've had for some reason is takinga long time when we first weredeveloping this application we weren'ttracking how long Json reads were takingand we had these big long holes in ourtraces of like what the heck's happeninghere so we started instrumenting ourcode and adding new spans and we sawthat Json was taking forever so thenwhat we did was we created an experimentwhere we tried different rejsonFrameworks and this is me actuallysearching for those experimental spansso we created uh one experiment calledread json1 one experiment called rejson2and we have a a random Boolean generatorwhen a user comes in they get one or theother and we're actually looking at livetraffic live traffic sorry it's a demoum of uh the read json1 implementationversus rejson2 and kind of evaluatingthose two traces to see what's changedto see if there's any other impact fromusing a different implementation of Jsonso here you can just see us doing somefiltering and looking for particularspans and that sort of thing that wasgenerated by open Telemetryumso uh yeah right here you can see we'relooking for a really fast span at thebottom of that graph and at the topwe're seeing a very high latency span uhit turns out that you know generally wesaw all of One span at the bottom all ofOne span at the top now you don't haveto do this by hand you can actually seeit right here under frequent Uris itshows oh look read Json on average readjson1 is way slower than read json2right so I can in individually look atsomething and try to figure out why aparticular request was slow but I canalso see an aggregate and we learnedthat holy moly the framework we usedtheir Json reading library isridiculously slow so now with this newImprovement we ended up actually overallimproving the applicationhow many times have we found out that aJson Libraryis the culprit yeahhow many times has reading Json beenlike the Major Performance bottlenecksystem yes everyone's running into ittotally been thereum yeah so uh and a lot of people thinkwhen they think of open Telemetrybecause it's where it started they thinkof traces but wanted to do another quickdemo that just gets into logs andmetrics and this is another sampleapplication storefront application it'sinstrumented with open TelemetryGathering logs and metrics also has annginx load balancer uh postgres databasebackend and we're just going to take alook atwhat this looks like I'm also so in thiscase I'm using bind plane just to managethose collectors so we'll see that atthe beginningand so we have hereuhis a collector configuration and thisjust helps you visualize what we'redoing is we're Gathering log files fromour storefront APIwe're also Gathering host metrics andlogsand then we're Gathering metrics fromnginx as welland then all that's being going throughsome processors and we're sending thatto gcp or Google Cloud operationsyou can see the amount of data that'sflowing through thatand then just uhdepositwe have it in a few different instancesso in this case a small cluster of thosehosts that are running this could justas easily be in kubernetes and thenwithin Google Cloud you can see thosethose instances that are runningnowfrom Google Cloud operations and withinGoogle Cloud monitoring we're alsotaking a look at things like statuscodes latency by host latency by pathwe're sending all the log metric andTrace data throughum and what this allows us to do is andthis is a simplified example here butwe're going to create an alert we createan alert and we're just looking for aspike in 500sright so to keep it simple 500 statuscodes when we see a metric come in thatsays we've seen a certain number ofthese we're going to alert on thatand so I'm going to skip ahead a littlebit just so we seewhat that looks likeokaysonow we have the alert and what's reallynice about this we're keeping all thedata together we're sending it to GoogleCloud operations we can go in and lookat this alert that was generated from ametric and Google Cloud operationsautomatically combine or links that tothe logs that are being generated aswellso we see the alert and now we wantcontext to help us troubleshootand so from here we can just click intoview logsand what that's going to do is filterdown to that specific node that was uhthat was having the issue and it letsyou diagnose you can see when thoseerror logs startedwe can go in and look at the the logspecificallywe see there's there are 500s that arecoming throughand then we also have some detailedinformation on what the problem was andin this case it was specifically thisnode was running into issues connectingto that back-end databaseright so it lets you link this thisinformation together and that's one ofthe things we can do when we pulltogether all the signals and with thiswe can do traces we can do metrics andlogs all togetherand one of the benefits of using openTelemetry is out of the box you get thesame set of labels between the three soby default all the Telemetry will have aconsistent set of labels and then theadditional labels relevant to thatparticular data Stream So if you want tokind of look and do this correlatedJourney it's a lot easier to find andsee things another fun part is your logswill have Trace IDs in them if you'retracing and so you can actually searchfor logs and traces together so in thelog Explorer you can actually put atrace ID in and you'll get the list ofall the logs across all the servers todo that root cause analysis that youwantthe last uh short demo we have herethere we goso this issomething that I I love about openTelemetry is the ability to gen tocreate Pipelinesso I told you about receivers andprocessors and exporters and the powerthat you have with this you can you cancreate a pipeline within a singlecollector create a pipeline withinmultiple but the idea here is that whatwe want to do is isenable the user to enrich your Telemetrydata to filter out data that you don'tneed right so a lot of times you'recollecting something and maybe you justwant to remove that before it's sent toa specific destinationallows you to do things like redactingpii and then of course routing itwherever it needs to go and that'scritical because once you have a dataplane like we're talking about you don'twant all the data going to everydestination and so we're going to Routeit wherever it needs to goand so I'm showing an example here of uhyou know I'm managing a few hundred uhopen Telemetry agents I'm going to gointo a specific one or a few of theseand this is a configuration it'sGathering data from Microsoft IIsmetrics and logs Gathering host metricsand logs Gathering Windows events andsending it to two different destinationselasticsearch and Google Cloudoperations this is a pretty commonscenariowe can see how much data is flowingthrough but what we want to do is tostart to add some processorsand in this case we're going to add somesome filtering so maybe with elastic weonly want to send those critical errorsso we have filters a lot of these arefor filtering down reducing data some ofthis is for deleting attributes fortransforming adding attributes and evenyou can add a custom processor in herebut in this case we're going to look atfiltering by severityso the left side you see a samplesnapshot of the data that's flowingthrough on the right side you see asimulation of what it's going to looklike once this is appliedso in this case we've significantlyreduced the amount of data but we'regoing to put in a couple more as wellwith Windows eventsa lot of times you know you you don'twant pii data uh leaving the host ormaybe leaving a networkand so we're going to filter out IPaddresses that are available in the thatare visible in the Raw dataso in this case we can say we want tomask that out we have a lot of filtersthat are predefined we're going to applythat to logs specificallyand when we look at the output after thefactwe'll see that that's masked andanything any email addresses credit cardnumbers those would also be maskedand one last filteryou know sometimes we don't needfull detail and we can say we're onlygoing to sample data right maybe it's arandom sampling and that's enough forwhat we what we're looking for and so inthis case we're sampling it we'redropping half of those logs randomlyand so we'll apply all of these and thensee the resultsand I'm showing this in biplaneeverything that I'm showing here can bedone directly within a open Telemetrycollector configurationin this case what we're seeing isyou know in this case we've we'vefiltered out pii data going throughevents we've dropped about half of thethe data leaving IAS in terms of logsand then we've reduced significantly thethe data that we're sending to elasticand that's the control that you get withwith processors and something that we'renot going to talk about but connectorsas wellright so now we're going to talk aboutbasically ways to adopt open Telemetryin your organization and kind of youknow from helping a lot of companiesadopt it and from working through itlike lessons we've learned a little bitso uhthere's basically three steps right thefirst thing is just to identify what youhave if you have every observabilityproductit's important to kind of understandthat and then match it to open Telemetryopen Telemetry tries to give you theability to take what you have and pullit into the open Telemetry ecosystem sothe open television collector thosereceivers there's hundreds of themthere's ones for almost any protocol youcan imagine and there's a bunch provideddirectly from vendors for a lot of theirprotocols right and the idea is youshould be able to get that data intoopen Telemetry and into that format andstart leveraging this this standardprotocol without having to fundamentallychange your workloads right so identifywhat you have figure out how to pull itin right the second thing is craftingthat Telemetry plane this is somethingthat you can take your time doing youdon't have to do it all at once you cando the pieces that are most important toyou and and move them but you knowstarting to move your data into thestandard protocol giving yourself thatability to do this filter and enrichmentlive right and building that plane sothat you have the control that you needfor your observabilityum and then the last thing is you startfilling in gaps you started with whatyou hadand now this is when you start to liftand shift or replace you know I decidethat the observability I have is isfundamentally missing say I wantdistributed tracing I don't have it wellnow I need to go start adding thatdistributed tracing you know through mysystemum if I'm missing say at metric specificto latency on a particular service Ifigure out how to go add that as a newtype of instrumentation so that's whereyou start getting that autoinstrumentation and that manualinstrumentation right filling in thegaps of observability that you feel likeyou don't have but firstyou know start with what you have don'tjust try to from scratch Cut and Runthat's not what this is designed aroundthis is designed to kind of take youwhere you are migrate in yeahand that's a good segue into the nextum which we'll talk about a couplethings one is deployment patterns andthen one is some suggestions or bestpractices on how you might migrate toopen telemetryumso this is uh one of the simplestdeployment patterns not the simplest butwhat I would say is this is the firstone that I would suggest anyone wouldconsider in their environments andwhat's Happening Here is we'veinstrumented an application with an SDKso we're we're Gathering traces or logsof metrics directly from within theapplicationwe're then sending that from otlp to a acollector that's separate from that thathosts that application and then the datais processed and sent to the theobservability back endum you know there's some pros to thisright it's really simple to get startedit's a great place to start there's aclear mapping between that applicationand The Collectoruh the cons are it's not as scalable assome of the other options and it's notnot super flexiblebut one thing you still can do with thisis you can fan out so you can you cantake this model and you can start tosend it to multiple destinationsso that's that's sort of the step one oror phase one of deploymentthe next is to actually uh deploy acollector as an agentso sometimes we'll talk about thecollector that's deployed on a host orsay in a cluster as an agent it can it'soperating locally so it can ingest logsand metrics directly from that systemum it then is is going to do processingyou can do the processing and move thatoff to the actual host itself or in thecollector that we would call a gatewaywhich is happening before it sends thedata along to the destinationum you know this is I think a greatpattern especially as you start to scaleout it does require that you'redeploying a new application or you dohave to deploy that collector to all ofthe the hostsnow one thing one thing to call outabout this particular model of havingthe two collectors that's ratherimportant for observability right thatlocal collector is only responsible forgetting data off the machineand making sure that your applicationcan get data off of itself as quickly aspossible what's the most importantmoment of observability when you'recrashingso having something local to let you getstuff off of the process as fast as youpossibly can is critical right and sothat's one of the benefits that you getby having that local collector but itshould be minimalright and then its job is just to do anysort of local filtering and droppingnecessary for that machine and then getit out the doorbut it's it's about an error failurezone right it's not about oh you knowit's more flexible necessarily it'sactually giving you better observabilitywhen it's done wellthe next in this is this comes back tothe statelessness of uh open somethingotlp you know this we can easily scalethis out so when you have largerenvironments if you're dealing withlarger amounts of telemetry data youhave high processing needs this thehotel collectors can scale out easilywith the load balanceryou know if your volume is low this isadditional complexity that you may notneedand then kind of the ideal and state theway that that I think of it is once youhave this if you have a local collectoryou have your gateway collectors thesestart to string together and they canchain together as needed so you can doprocessing on your hotel collectorGateway they can be a Gateway if youhave say secure environments and and youreally can only expose one collector asthe the endpoint that's going to senddata outthese can all sort of chain together andthen you really at this point you'vetruly created a Telemetry pipeline adata layeryeah so now that you've seen kind of theyou know if you were to adopt opentelevision from scratchnow we're going to walk into okay whatif you're not I mean most of us aren'tright most of us already haveobservability of something at least logsokay let's look at how we take this andadapt it now right with that start withwhat you haveyeah that's that's the key thing that'swhere a lot of people will get a lot offolks get concerned and say well how dowemigrate from what we have because wealready have a large existing deploymentwe have proprietary agentsinstrumentationum and they are maybe tied one to onewith with the platforms that we're usingbut we recommend that you do this instages rightandideallythis isn't always possible but if youhave a new application new deploymentsthat you're starting with this is agreat place to to get started with openTelemetry you can deploy open Telemetryin the architecture of the deploymentmodel that you were planning on and thensend the data to the same locations thatyou were already expecting it to goright so this is a great spot if youdon't have a green field applicationavailable it's choosing what what is theeasiest in your environment and that canbe sometimes that's about the teamthat's easiest to work with right thatthey're the ones that want to try thisthe most but starting with with onepiece and thenthat that moves into phase twoand I think this is something that thatsometimes folks are surprised by but inthe vast majority of cases you can takethe instrumentation that you alreadyhave installedwhether it's sdks proprietary sdks or itcould be a different agentproprietary or open source you canredirect that data and there is verylikely to be a receiver in openTelemetry that will allow you to thenconvert it to otlpso if you start to shift those differentworkloadsredirect them into your new openTelemetry pipelineyou're getting a lot of the benefitsthat you would see otherwise you justdon't have that same instrumentation atthe application Leveland then the the last phase is reallystarting to decide what do we need toreplace and and like Josh mentioned youknow sometimes that's on a case-by-casebasis where you sayI need to fill in the gaps here I don'thave the visibility that I was hopingforum it's worth it to to switch to thisagent it's worth it to you know reallyconsolidate everything and make surethat we areconsistent across all of ourenvironmentsbut I guess the the key Point here is Iwant to make it clear that you don'tneed to do everything at once and reallymoving it moving through this in astaged approach is what we recommend andit gets you that same place but you cando it over timeforeign"
}