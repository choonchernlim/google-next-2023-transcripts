{
    "title": "Bringing machine learning to streaming data with a few lines of code",
    "presentation_type": "Breakout",
    "categories": [
        "Data Analysts, Data Scientists, Data Engineers",
        "ANA210"
    ],
    "video_id": "IQteQYtf_NU",
    "time": "Aug 30 03:00 PM - 03:45 PM CDT",
    "transcript": "[Music]thank yougood afternoon everyone thanks forcoming in hopefully you had you all hada chance to take a break grab some lunchAI is solving a lot of problems I'mhoping one of these days it will solvethe post-lunch sleepiness problemso with that my name is and I lead theproduct management team for Google CloudI have couple of co-presentatives withme I'll let them introduce themselvesbefore we jump in hi Reza rockney I'mdeveloper Advocate working for thedataflow team at Googlehi I'm Kong Tran I'm a ml engineer fromEtsyokay I figured out how this worksfantastic thanks gentlemen I'll call youon stage for your sectionsumso before I jump in let me quickly giveyou an overview of what we plan to covertoday I'm hoping to kick it off with alittle bit of an overview of the kind ofbusiness problems that users like youare interested in solving we call themreal-time intelligence you may becalling it with a different name sowe'll do a little bit of context settingtalk through those business kind ofbusiness problems and why it mattersI'll then give you a high level overviewof the kind of data architecture youneed in place to enable those use casesI'll then have Reza come over talkthrough in detail about many of thecapabilities that we have in the productthat will help you implement those usecases we'll then havewe'll then have Kang join us on stage uhwe'll do a you know q a totallyunrehearsed you a lot of Believers whenwe tell you that we'll do a q a whereKang will share this experience at Etsyimplementing these use cases and thenfinally time permitting I'll do a quickRoundup of some of the latest andgreatest capabilities in data flowso with that let's get started so let'stalk a little bit about the use cases sowhen I talk to users like you thepotential that streaming data andmachine learning has in being able todrive transformational changes in yourbusiness is abundantly clearinteraction that your mic Services haveevery interaction that your iot deviceshavethe engagement that you have with yourusers maybe your employees vendorssuppliers so on and so forth creates alot of meaningful dataif you have the ability to gather all ofthat get meaning out of that data anddrive insights now you have the abilityto build new transformational businesscapabilities or go build new delightfulexperiences for it for your end usersthat puts most relevant information andactions at their fingertips when theyneed itwe can see this examples in allIndustries and verticals I fivecustomers in healthcare that areimplementing use cases like real-timepatient monitoringfraud detection when it comes toInsurance we're all familiar with usecases on the retail side be it thingslike cart upsell product recommendationsor customer 360. and on the other end ofthe spectrum something likemanufacturing and iot which in many waysin fact kicked out the streaming dataRevolution you have use cases likepredictive maintenance supply chainmanagement so on and so forthso we've talked about the use caseslet's talk a little bit about the kindof data architecture that you need inplace to support these use casesyou know traditionally you know a lot ofdata analytics and machine learning wasbatch driven you ran you know batch ETLprocesses maybe you landed once a dayonce hour you got data from youroperational systems your transactionalsystems mostly SQL databases you knowyou run an ETL process took that yourdata lake or your data warehouse andthen you had you know machine learningfeature extraction happening batchtraining happening batch prediction andso forth so on and so forth what thisallowed you to do is get insights frombatch data or historical data dopredictions store them up maybe you hada nosql database where you where you hadall your predictions stored and then atthe time of serving you had your servingsystems look that up now this is goingto continue to exist right there's awhole bunch of use cases that needs thisarchitecture this architecture issufficient we are going to continue tosee thisbut for the kind of real-time use casesthat we talked about in the previousslide you need to move to anarchitecture where you're able to dothings likereal-time real-time features you knowsome call it online features orstreaming features you need to be ableto do depending on the use casereal-time predictions or real-timeinferencingso let's talk a little bit about how youget thereI hopefully by the end of this I wouldhave figured out which is move forwardand which is backwardsokay so the first first step forstreaming is of course you know obviousyou need to have streaming data in placeright no surprises we're not going to goon the details of the messaging systemsyou know I suspect a lot of you arefamiliar with things like Cloud pop subwhich is our partition list Cloud scalemessaging platform you also have Kafkaanyway the first step is to get yourUpstream systems to ingest data into oneof these right so once you have data inum you know Pub sub or Kafka you canstart to do a lot of interesting thingswith itnow very similar to the batch ETL worldwhere the data coming in from yourtransactional systems is not ready foranalytics or machine learning the sameis true for streaming data as wellyou need a mechanism by which you'reable to take that raw data in yourstreaming system or in your messagingsystemprocess it you know you may be enrichingit just cleaning it or conforming it toa specific schema or you're doing moreinteresting things like streaming youknow feature extraction but you need astream processing platform in some casesyou could go roll your own system butthere is a lot of underlying plumbingand infrastructure that you need to takecare of itproducts like data flow and open sourceproducts like Apache flank solve thatproblem for you these platforms give youan you know SDK higher level SDK amapreduce based SDK that allows you tojust focus on the data processing taskat hand and not worry about theunderlying infrastructureOkay so we've solved that problem we nowknow how to process the stream and youknow extract information from it dothings like enrichmentis that sufficient right so now you'vegot you know data you can make sense ofthat data now you need to figure out howyou go from processing to intelligencenow there are a lot of things that gointo that but there are three thingsthat we've observed in working with alot of customersso one even when you're doing streamingmachine learning you have a need toleverage batch data now that may be justfor you know backfilling things it maybe for you're filling gaps in your datain some cases you're enriching yourstreaming data and in other cases yourmachine learning use case may need acombination of streaming features aswell as batch featuresthe second challenge that we have seenis you need a way a scalable way for youto incorporate inferencing into yourstreaming data processing you may bedoing local inferencing or remoteinferencing there are some trade-offsinvolved we are not going to go into thedetails of that today I do have coupleof use case or architecture slides thatI'll show you but there are someinteresting trade-offs involved andwe've seen the need where customer wantto start with local inference saying butmove to you know remote predictions andgo the other way as welland then lastly you know one of thedefining attributes of real-timeintelligence is the ability to takeactions without having humans in theloopright traditionally with bi you got allthe data you got analytics and then youhad a dashboard you had a human look atit maybe once a week once a month andtwo day sessions with real-timeintelligence you want to get to a statewhere you know the machine is makingthis you know getting insights makingdecisions and activating that now thatmeans that you need to have an abilityto have those insights reach yourDownstream systems it could be again amessaging system it could be yourserving system or your transactionalsystemso that's where something like data flowcomes into play right I suspect a lot ofyou are familiar with data flow but forthose of you that are not familiar I'llgive you a quick overview data flow isyou know you know Google's Cloud nativeuh data processing technology both forbatch and streaming data processingit uses something called Apache beamwhich is the SDK for defining yourstreaming and batch data pipelines bothof these are based on a technologycalled Flume that Google uses internallyfor all of its big data processing needsanyway so dataflow is a serverless youknow service that executes yourstreaming and batch data processingpipelinesApache beam SDK gives you a and Kongwill cover some of this on why theychose Apache beam and data flow givesyou a developer friendly SDK that allowsyou to Define your data processingPipelinesthere are a lot of things about thesetwo that we don't have time to go intobut I want to highlight threecapabilities that makes it uniquelysuited for real-time intelligencethe first is the fact that both the SDKand the servers support unified batchand stream processing Reza when he talksthrough his section we'll show you somecode samples that show how it is easy toswitch from batch to streaming but youhave one SDK one technology to do bothbatch and streaming and more importantlyyou could have one pipeline that handlesboth batch and streaming datathe second aspect I talked about how youneed to be able to connect to yourDownstream systems to activate insightsso that's where having robust supportfor bi-directional Io comes into playand then lastlywhich is what Reza is also going to talkabout in detail having a set of out ofthe box ml capabilities means you're notReinventing the wheel you're not goingthrough the tile of writing a lot ofboilerplate code that that uses bestpractices and things like that so youhave out of the box capabilities thatmakes it easy for you to quickly getstarted and just focus on the CoreBusiness Logic on your siteso this is our favorite plop data flowinto the diagram that we saw earlierthis is our look now this may be alittle bit abstract so let's doubleclick and look at like few examples fewactual examples that customers haveimplemented so this is from a telecomcustomer of mine that has implemented uhNetwork intrusion detection use casebased on using anomaly detectiontechniquesso to do that the Angels logs Networklogs from their on-prem systems theirreal-time data comes in through Kafkaand Pub sub their historical data comesto Google Cloud Storage they have a dataflow pipeline that then inches both ofthat data does things like featureextraction and real-time predictions andin this case the model was local to thedata flow jobuh they also leverage something likeCloud DLP API so that they cande-anonymize or anonymize the data toremove Pi information from their locksto make regulatory requirements they'reallowed to remove pii from anything thatpowers their ml algorithms so given theflexibility of data flow they are ableto do thatnow contrastingis a different example where thecustomer was doing fraud detection inthis case they used a booster treesmodel that was hosted in vertex AI forvarious reasons like you knowflexibility and being able to manage themodel independent of the data pipelinethe model itself was hosted in vertex AIPipeline and the data flow job or thedata flow pipeline called into vertex AIfor doing predictionsnow with that I'm gonna have a Reza comeover join us and walk through some ofthe capabilities in the product that isgoing to make it a lot more easier tobuild use cases like thesethank you and see if I can figure outthe clickerno wrong waythere we go uh hey everyone thanks Shanso let's take a look at the role ofdataflow in this flow hereum the first thing the data flow in themiddle of the service has to do is itneeds to read from our sourcewe will then need to do somepre-processing of that data and then wewill pass that pre-sources data to amachine learning model in the first partof this talk we're going to mainlyconcentrate on where you have got apre-built model that you're going topull onto the data flow service forlocal inferencewe would normally have a post-processingprocess so you would take the inferencesmaybe join it with some otherinformation before you activate it atthe other end now the things that we'regoing to need to do to it for thisobviously thethere we go obviously the data flowservice which is the fully managedenvironment which is spinning up theworkers and executing the the work thatwe described for that data flow serviceto know what to do you're going to haveto build your pipeline within its SDKwhich is uses the open source Apachebeam SDK for those of you who may not befamiliar with Apache beam we're going tospend a couple of minutes just lookingat Apache beam and how you would build apipelineso beam relies on a few core Primitivesthat has lets us build up our pipelinefirst of all it's a p-collection aparallel collection of immutable objectsof the same type this is the data thatwe are going to do our Transformationson for the Transformations themselves wehave our P transform parallel transformallows us to process the data within theP collections creating new collectionsof course this is all done in paralleland at scale finally we need a way ofputting all this together for this we'regoing to use the pipeline primitive sohere the pipeline is a directed acyclicgraph of computation which tells thesystem how the P transforms and the Pcollections are interacting with eachotherto bring it to life we're just going toput together first of all these simplebatch Pipeline and then we'll make itstreaming so let's imagine that you havephysical stores and some online storeswhich both generate files that we wantto do some processing on there's somesales information we want to do somesales analytics so first of all we wouldneed to read those filesum to do that we're going to make use ofthe Python SDK here the p designates apipeline object and the pipeline pipeoperator means apply this transformationto the pipeline so here we're applyingthe read from text transform to thepipeline generating our store sales andonline sales P collections next we'regoing to use a primitive from beam tobring it all together so this is theflatten operator essentially this takes2p collections with data types of thesame type and brings them together intoa single P collectionnext we're going to do analytics now thedetails of this doesn't matteressentially it's just going to find thetop sales valuesafter we've done the analytics we thenwant to Output the results to a Synchere we're going to make use of bigquerysync which is bigquery IO right tobigqueryso with this we've built a batchpipeline how do we now change it to astreaming pipeline as Shan said earlierbeam was from the outset built to beboth batch and streaming so we don'thave to touch our business logic but wedo have to add one more primitive andthat's the window primitive so becausethe data is continuously arriving weneed to actually know how to do thecomputations when to Output a result andfor that we use the window primitivewhich essentially subdivides a pcollection into timestampsso let's change our pipeline so now atthe top rather than reading from fileswe're going to read from Google CloudPub sub of course this could be Kafka sowe now got a continuous stream ofinformation flowing through and ifyou'll note just before our businesslogic we've applied the window into uhtransform this is just creating thosefixed windows of 10 seconds which meanswe're going to get continuous outputevery 10 seconds that is then written tobigquery okay so now how do we turn thispiece of analytics here into a machinelearning model so we've built a trainedmodel and we now want to use that forthe inference rather than simplestatistics here well if we were to dothis with The Primitives of beam andthere are many more than the ones Idescribed of course and you wanted to doit in production there's actually quitea lot of work to do so first of allgiven model sizes we need to make surewe share them efficiently across thethreads on the worker this is astreaming pipeline we need to make surethat bad data is not only preserved byusing a deadly letter q but also doesn'tclog up your streaming pipeline we needto make sure that any pre and postprocessing that is done on the model isencapsulated within your model object soyou don't get training serving SKU weneed to make sure that when a new modelarrives in your streaming pipeline it isquickly updated so all of this soundslike a lot of code and far more workthan just a few lines in your streamingpipeline luckily for us our data flowEngineers have been working on addingthings to being known as a turnkeytransform the idea behind the turnkeytransform is rather than you asdevelopers having to take buildingblocks to generate your goal we providea single line of code a single transformand you provide configuration which isthe thing that you want to do and wetake care of all the productionizationsothe one that so rather than having towrite hundreds or even thousands oflines of production code which of coursethen needs to be maintained documentedtested you now do a single line and theone that we have first launches runinference no surprise given all theexcitement around machine learning andin this space so run inference with asingle line of code we have done all ourencapsulated all of the things that Idescribed in the how to productionize aa model within the beam Pipeline and youas a user use that one line plusproviding a model Handler what is themodel Handler in this code wellit is simply configuration this is anexample of one this model Handler istelling us that the model you are goingto use is a pi torch model it's locationin a Google Cloud Storage bucket andsome other information that's pertinentto that particular modelsince adding a run inference to uh beamwe've been continuously working it wasadded early in mid last year we've beenadding features to it I can't go throughall of the features that are availablebut I'll cherry-picked a few one issupport for lots of Frameworks soobviously this is very key and we areadding more and there is also somethingknown as a custom model Handler if yourparticular framework isn't on this listanother thing we've added is support forworking with model repositories so ifyou are using something from tensor Hubor hugging face the configuration youpass in in this case hugging phasepipeline model Handler gives us theability to know what your intent is tobe able to pull the model onto theworkersnext is this is my favorite featureum is streaming updates so in a machineif you're training your machine learningpipelines in your ml op systems you'regoing to be continuously updating themyou're going to be making them betterand you want them to be deployed to yourstreaming pipeline without having topause or create that pipeline so withthis feature when a new model arrives itgets hot swapped on the data flowservice for you without having to pausethe the pipeline at all it supports twomodes one mode it's looking for the filelocation if the file location the filechanges it's going to pull that model inanother mode is an event based modewhere for example you'll have in your mlop system where you usually have apusher component your pusher componentcan signal into the system that a newmodel is available and it will go do thenecessary to pull that model from forexample the Google Cloud Storage bucketokay so with these pieces we canactually now do inference with verylittle codefor our pipeline one of the things aboutthis particular pipeline is there's onemodel and it's a very synchronous flowwhich is of course some use cases butmany use cases are far more complex so Ihad the great privilege of working withthe very clever engineers at Spotifyand here and this is in the publicdomain in this blog we go over how thereare many machine learning models beingused and different Frameworks so in thesingle pipeline they're just they'reusing pi torch they're using tensorflowto actually achieve their goal of uhtheir business now how do we use runinferenceand the beam pipeline object to actuallybuild these complex flowsso let's take this spaghetti wellpipeline it really is spaghetti right ifwe imagine each one of these blue boxesis a modelthe piece of data needs to run thegauntlet of all these models before itbecomes useful to us how do we do thiswell if you look at this there's acouple of interesting characteristicswhen we decompose it first all of thearrows are pointing One Directiontherefore it's a directed A cyclographof computation and fits in with ourpipeline object second you'll noticethat actually there's only two patternsthat are continuously being repeated thefirst patterngiven a piece of data in RP collectionwe want that data to be have inferencerun against it against two differentmodels what you'll notice on the code onthe right there's very little to do toour pipeline object we simply apply twodifferent configurations for runinference and we can achieve thisbranching the next piece is sequentialmodels so here the data is being passedto model onethe output of that is then being sent onto another model for example you mightbe extracting uh text from voice andthen you want to do some sentimentanalysisum usually the output layer of the firstmodel is not in the right shape for theinput layer of the second model so youusually put a mapping function inbetween so here we have again on thecode on the right hand side very littlecode needs to be written to describethis path and if you repeat this you'regoing to get back to that complexspaghetti we saw in that first slidesoum uh of course with inferenceespecially in a streaming system latencymatters even in a system where you knowyou can tolerate one two minutes oflatency in a streaming system theservice rate of the transform that'sdoing the inference needs to be able tokeep up with input otherwise you're justgoing to get continuous backlog so oftenyou're going to make need a use of aaccelerator and again with the data flowservice I really haven't learned how todo this I'll go forwardum this is also straightforward weprovide a option that we pass in onPipeline creation time as well as acustom container that contains all thedependencies that you have for yourimagesum and this will install all of theNVIDIA drivers on the worker as well asthe the exit the Nvidia GPU for you wealso work very closely with Nvidia we'veenabled Nvidia MPS which is allows formore efficient context switching whenmultiple python processors are vying forthat GPU time again if you imagine inthe streaming system the gpus in themiddle it's getting talked to by a lotof python processors that are pulling inwork as they go forwardumthe other exciting thing souh up until this point we have beentalking about models that we are pullingonto the dataflow worker for localinference what if you are using a greatservice like vertex I online predictionswhichum you know brings in all of theSimplicity flexibility and assistivenessfor the that product's ability to deploymachine learning models for you wellhappy to announce that as of Apache beam2.50 which is going to be released inthe next few weeks we uh we have a newmodel Handler called the vertex that Imodel Handler again this allows you tomake use of run inference but now with afew lines of code you're connecting yourdataflow service to vertex AI onlinepredictions with no effort and we cansee customers making use of this inmixed mode as well so imagine you have asmall model that you use earlier on inyour pipeline like in embedding model orsomething like that and the next partnext call in that pipeline you're makinguse of a very large model so there youwill mix the local inference with theremote inference for the best inbreed ofyour pipeline so with that there's quitea couple of resources here for you youcan actually go on these links that arefully working code lab examples for allthe things we've talked about today andwith that I'm going to pass over to Shanthank you Rezaokay fantastic do we have audio hereokay I know we did a little bit of anintro can you tell us a little bit aboutyourself what you do and your uh youknow Etsy and and we will kick it offthat yeah so I'm a ml engineer on thesearch ranking team at EtsyEtsy is a global Marketplace for uniqueand creative Goods you can buyeverything from like handmade earringsto clothing to even Furniture so on myteam we train and deploy models to rankitems on search result pages sobasically when you type in a searchquery and you get a bunch of items whenyou see some things at the top and somethings at the bottom that's what myteam's responsible and we train ourmodels using tensorflow and we build ourpipelines on vertex AI with dataflowokay that's that's great can you tell usa little bit about severe you know thekey business priorities and sort of thethe path that you've taken so far yeahsoum ranking is pretty important forsearch Pages uh a lot of users onlyreally check the first few pages sogetting your ranking right is thedifference between someone bouncing orconverting so you really want to makesure that um you got this right so onereally simple way to do it is uhsomething called tfidf it comes bundledin with a lot of Open Source searchengines like solar and this basicallywhat this does is this scores each itemin your search results set so when youtype in a search query it checks howmany times those query terms appear inthe objectand it gives also a little bit moreweight to rare items or to rare Awardsless common wordsbut this is a pretty simple scoringmechanism and it's really easy to gamethere's something called called keywordstuffing so you might have gone on awebsite before you notice like a producttitle had like a hundred words in thetitle because um the manufacturer orseller is basically just trying to likeput in as much as they can so that theycan get into as many search results and for asmany searches as possible I deal withthat every day in Amazon yeahsoumwe really want to look outside of thisrealm of exact text matching andoptimize like for both the relevance butalso the likelihood of a customeractually buying an itemso luckilywe have a lot of search sessions and welogum like what people search for the itemsthat we show them the product titles theprices the review ratings things thatusers have bought in the past and so onso we have terabytes of data per day andwhen you have all this data you knowsomeone in the room is going to bring uplet's use mlokay very cool so you see them whattechniques do you use today and maybe ifyou can give us a sense for our whatyou've explored what you use today andmore importantly bringing it back to thetopic of the presentation how that hasinfluence your data architecture yeah soour architecturefor our model has changed a couple oftimes over the years we actually builtour first ml model in 2017 and the firstml model for ranking in particular andwe just started off with the seniorsimple linear model and we used Pi sparkfor our feature engineering which uh forthose who don't know future engineeringis just taking your raw training datathen prepping it for for input to yourmodelso yeah we use Pi spark and somein-house in-house libraries for that aswe started to put more investment intothe ranking model we decided to switchto a boosted tree model whichin in 2019 when we switched over to thismodel was kind of the state of the artfor ranking at the timeand through many a b tests of our modelswe were seeing increased engagement andincrease increased conversion but fromthe development side for both our linearand our trees model we were kind ofreaching a point of diminishing returnswe ran we were running out of ideas onhow to improve the model itself and weended up just doing more and morefeature engineering to try to improvethe performance of these ranking modelsso lucky for us um around around 20192020 uhdeep learning has become more popularbecome more popular in the ranking spaceand there are papers being publishedlike every year on people on companiesfiguring out how to use deep learningand their ranking modelssothis was a good time for usto also start investing in deep learningand in 2021 that's when we started liketo put a lot more time into actuallyinvestigating thisand while we were making the switch wedecided to re-look at our Pipeline andmake some changes in our ourarchitecture so we actually decided togo with tensorflow for our modeltraining it's just a really matureecosystem for both training anddeployment to production and we alsodecided to switch off of high spark ontodataflow for our feature engineering uhit just dataflow has a lot ofIntegrations what tensorflow that makesit easy to work with and also some ofthe other benefits of moving to dataflowwas um I don't know if anyone's evermanaged a spark cluster before but it'suhyeah it's it can be very opaque topeople who don't understand or peoplewho have less experience in what's sparkbut what data flow people are developerscould just focus on actually writingpython code and how the data flowed fromone place to another and in 2022 is whenwe were actually able to launch ourneural ranking model as our Baselinemodel in production in production yeahvery cool that that goes back to youknow the code samples that Reza wasshowing on screen right how easy it isto express your business logic with beamyeah very cool so how does your dataarchitecture look like today we talked alot about your ml but walk us throughhere sort of the overall architecturefor data today yeah so I have a prettysimplified diagram of our Pipeline onthe slide here and basically all of ourtraining data is stored in GCS or Googlecloud storage and we use tensorflowtransform for our feature engineering sotensorflow transform is a live area thatruns on on dataflow and allows you torun tensorflow operations aggregationsin statistics on all of your rawfeatures and then put it in the correctformat for tensorflow to actuallyunderstand and train withwe then pass those features over to ourtraining step which happens in vertex Aiand we currently just train on oneserver with multiple gpus on it andafter training we have the training stepgenerates a model artifactso that model artifact will be whateventually gets served in production butalso we take that model artifact and wedo an evaluation on it so for ourevaluation we actually also use dataflow so in data flow we can load in thatmodel artifact as well as loading in atest data set from GCS and then runinference onusing that test data set with the modelwith the model loaded into dataflow andthen we calculate something called ndcgwhich is basically just a metric to letyou know how well your ranking model isperformingand every the nice thing about thispipeline is every part of it is scalableso as we get more data as we like as weget more traffic and more dataall it just gets put into GCS and thenfor our feature Engineering in ourevaluation we can just up the number ofworkers to handle the additional theadditional trafficand we are actually able to add workersto our GPU training but we just haven'tfound the need for that complexity yetour training runs uh fine right now yeahvery cool let's now bring it back tosort of the business benefits that youtalked about uhyeah so to put quite simply uh thebenefit is just that search results arebetter so one example is aroundpersonalizationum so if you look at the screenshot onthe left this is what a new user mightsee I basically just opened an incognitotab and then I search earrings and thisis uh and this is what I got but then Istarted to click around on specificitems I started to like add like woodenthings to my card add like some rusticthings to my cart and I did the searchfor airing again so on the right sideyou can see like the updated searchresults so this is really catering tolike based on the user and what they'researching we can show them what webelieve they want to purchase and thisis also a benefit for the sellers in ourMarketplace because they get exposed toqueries that they might have not beenexposed to beforeand and overall as we've run our avtests with our different modelswe've seen increased search engagementand increased conversion okay very coolthanks for sharing appreciate ithopefully the audience was able to get asense for how you've done it and youryour journey thank you very much foryour timeoh yeah so uhare there any learnings or any takeawaysthat you want to leave with the audienceuh yeah so the first thing I would do ifyou're looking to use mlis to do res look into the research andthe trends in your industry I think thiswill help you like hit the groundrunning and see what worked for othercompanies and then see the things thatthey struggled withand also be prepared to fail it actuallytook us about a year of iteration on ourneural ranking model to get it to astate in which we felt comfortableputting it into into productionso it can be a it can be a long processbut we felt the switch was was worth itfor all the benefits that came with deeplearning and rankingand thenum just make incremental progress onyour models and learn and iterate fromany from like when you run tests andthings like thatand also really important is to buildyour pipelines for scale so understandwhere your data is now and where itmight be the future talk with your datascientists your ml engineers and yourdata Engineers to decide on like theright solutions for how to make mlmodels realistic for your use caseyeah fantastic thanksgwith that I think we'll switch gears andtalk a little bit about let me do aquick time check I think we've got maybeanother three four minutes I'll do aquick rundown of all the latest andgreatest capabilities and then we'llopen up for Q a we may not have enoughtime to take all questions but we'llhang out by the side or outside the roomso please find us if you have questionsso before I go in and talk about the newcapabilities to give you a little sensefor how we think about our roadmap andwhere we invest in there are three keyareas that the team is really focused onthe first is what we think of as Best inClass platform you know this in my mindincludes things like performancescalability reliability and so on and soforth this is the number one reason whycustomers come to Google Cloud you knowparticularly my product data flow andthis is an area that we we continue toinvest in we've launched a number ofcapabilities in the last year I've got afew that I'm personally very excitedabout that I'll I'll quickly highlightthe second area of course is developerexperience and overall usability of theproduct you know deploying you knowtuning operating and managing streamingpipelines requires a different set oftools than what you may have used forbatch pipelines so that's an area thathistorically we've invested in and youknow we've got a number of capabilitiesthat I've highlight today and lastly youknow we want to make sure that you knowfor Enterprise customers like you youyou're able to take our products ourplatform and integrate that into your IDsystems your I.T policies be able tomeet your compliance needs either fromyour own organization or RegulatoryCompliance requirements that you have sowe've launched number of capabilitieslike support for shielded VMSum you know support for confidential VMScompliance with regulatory requirementsand so on so forth this is an area wherewe want to stay lockstep with yourevolving needs as welland then we talked about ml that sort ofall pervasive both in terms of givingyou capabilities to do your own Emma butalso applying ml to bring newexperiences new capabilities within theproductso with that I'm going to quickly runthrough a few you know excitingcapabilities the first is somethingcalled Dynamic thread scaling this isvery unique in the industry we've beenable to bring that from if you recall Italked about the product called Flumewhich is used within Google so we areable to bring this technique whichdynamically adjusts the number ofthreads that are scheduled per core andthe and the objective here is to reallysaturate CPU you know reduce cost foryou and get the most amount of juicepossible from the underlying resourcesright of course you know we don't wantyou know developers to go hand tune thisleads a lot of operational tile so we'vegot a system where we can dynamicallyyou know adjust the number of cores thatare scheduled and then based on backpressure are just that as neededcontinuing on the theme of efficiencyyou know some of you may know thisthere's a component called streamingengine which sits at the heart ofstreaming and data flow which handlesall of the stateful computations andstreaming Shuffle there have been anumber of techniques a number ofenhancements that we've rolled out tostreaming engine in the in the recentpast I've highlighted some of those herebut most of those fall in the efficiencytheme right how do we automatically tunethe system based on your specificpipeline based on your specific trafficpattern so that we are able to optimizethe infrastructure for cost performancelatency and things like thatum on the developer experience side Iremember I told you how you knowdeploying and managing uh streamingpipelines require a different set oftools uh you know we've got a number ofyou know tools like you know uh our jobgraph detailed dashboard monitoringmetrics one of the things that you donot have until now is access to theactual data that was passing throughyour pipeline when a problem happenedright so we've got all sorts ofvisualizations and dashboards todaywhere you can you can see where theproblem occurred but not what data wasbeing processed when that problemoccurred so with data sampling you nowhave an automatic capability whereyou're going to be able to see data thatwas passing through your pipeline when aproblem happenedagain to help with Diagnostics ontroubleshooting that's an area where Isee customers spend a lot of time rightso straggler detection is an automaticcapability that is going to identifyhotkeys or in general elements that areslowing down here pipeline automaticallydetect and in some cases not in allcases in some cases give you the rootcause for stragglers and hotkeys in yourdatayou know this has been a favorite askthis yearum you know while the billing data thatis available as part of cloud billing isthe authoritative source for costinformationasks where customers want to give theirindividual practice practitioners theability to quickly look up costs sowe've now launched a job monitoring aspart of the data flow UI where you canget a sense for what the estimated costsfor your jobs are going to bewith that I'm going to skip a couple ofslides actually let me highlight thisyou know one of our developers you knowpublished a cookbook that has close to190 samples for a lot of common usecases it covers both Python and Java dotake a look at it and he's looking forfeedback and what else would be usefulas well it's all on GitHubto wrap it up couple of other resourcesyou know there's a whole bunch of dataflow ml notebooks right which you canjust you know launch hit run and try itfor yourself and the second link QR codetakes you to our data flow get startedtutorials where there are a number oftutorials depending on whether you'renew to data flow or you're anexperienced data flow user uh you couldtake those for a spinthank you"
}