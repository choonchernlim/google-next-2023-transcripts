{
    "title": "Building your ML Ops strategy for generative AI",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML211"
    ],
    "video_id": "UfUoG_Ijgq4",
    "time": "Aug 30 11:15 AM - 12:00 PM CDT",
    "transcript": "foreign[Music]good morning everyone thank you forjoining our session building your ml opstrategy for generative AI pretty lightnot so dense topic for Wednesday morningI'm sure my name is Mikhail an outboundproduct manager on vertex Ai and lateron stage I'll be joined by Arena ourproduct manager of our vertex AIevaluation Services before we jump intoour content I want to run a quick pollnumber one if you could raise your handif you or someone on your team hasactually built a really excitingcompelling demo with generative AI inthe last few monthsa lot of folks more than half the roomand I think we've really seen you knowthe pace of hackathons and workingprototypes really increase over the lastfew months number two how many of youhave taken said demo into productiona lot less a lot less and number threelast question I promise how many of youhave really spent the last few yearsdesigning thinking about and mayberolling out an ml op strategy beforegenerative AIokay get him out get him outso I think really I think we're going toreally tackleum these questions and really addresstwo key themes number one and based onall this this quick poll building iseasy deploying them into a Enterpriseready production environment is reallyreally hard and this quote from Chip Ithink a lot of us uh really feel thesame waysecond is really you know isn't it funnya lot of you raise your hands for thatlast question right as we were getting ahang of ml Ops personally have myprofessional ml engineeringcertification proudly in my LinkedInprofile right as we were getting a hangof this now we're really you knowintroduced to a brand new Mountain ofterms across llm Ops gen AI Ops thingslike vector databases prompt engineeringso really we'll touch on the bestpractices and really believe that you donot have to start from scratchso what's our agenda for today uh we'llquickly revisit ml Ops from the last fewyears we'll share an evolving frameworkI highlight evolving we are very earlyin this space and I'm excited to haveArena come on stage and share some ofour new evaluation Services launchingspecifically for generative AI use casesand then we'll end with a recapso first let's quickly start byrevisiting Emma lops from it from a fewyears ago and we'll actually start witha direct quote from our practitioner'sguide to ml Ops paper from early 2021 mlOps is a set of standardized processesand capabilities for building deployingand operationalizing ml systems rapidlyand reliably throughout this paper thereare a number of Concepts highlightedshorter development Cycles to acceleratetime to market Standard governanceprocesses to manage and monitor modelsand really increase reliabilityperformance and scalability from a firstglance really our hypothesis is that theobjective of ml Ops or generative AIshould remain very similar if not thesameand fun fact to the authors of thispaper Don and Eric are actually heresomewhere so uh see if you can find themand maybe pick their brains a little bitmore for the remainder of the conferenceso from that starting point we really dobelieve that you can start by buildingon existing ml Ops foundations andthere's really two big two big Ideasnumber one no need to throw away yourexisting ml Ops Investments there is apath to extend mlaps foundations fromyesterday's AIwhat we're going to refer to today aspredictive AI to today's generative AI alot of the Core Concepts we really feelhave a lot of similar characteristicsmanaging and evaluating modelsorchestrating pipelines evaluating andmonitoring models in a systematic wayhowever I don't think we you know weneed to acknowledge that generative AIdoes introduce a lot of uniqueproperties and characteristics that weneed to really rethink and reallyupgrade our ml op strategy largepre-trained models you know pre-trainedmulti-task with increasing AIinfrastructure needs I'm really new waysto customize models not just trainingbut tuning customizing those modelsevaluating and monitoring not a singleprediction but huge chunks of generatedoutput images chat logs and reallyconnecting these models to fresh andrelevant data from other systems sothroughout this talk we're going to gothrough each of these five bullets onthe right side and showcase some ideasaround this and how we have servicesavailable today on vertex AI platformand Google Cloudand by the end of this talk we willdemonstrate that you don't need to holdlearn a whole new discipline or build awhole new platform uh we don't think youneed to have a completely separateStandalone generative VI ml app stackreally ml Ops on vertex AI is the sameplatform upgraded to support bothpredictive and generative AIcapabilities so like a lot of you whoraised your hand on the last questionyou know whether you're developingdeploying models around regressionclassification object detectionforecasting or looking into new usecases around code and text generationsummarization chat q a really do believebelieve that you can use the sameplatform to manage that AI lifecycleso now let's move to really Bridging theGap from yesterday's ml Ops to today'sml Ops for generative AI and let'shopefully start with ideally a not socontroversial diagram a lot of youprobably have many different versions ofthis within your organization we havethe two big phases uh training andserving training generally has anexperimental experimentation phase andthen really operationalizing thetraining step serving usually includesdeploying a model to an endpoint andthen getting predictions through aninference pipeline throughout the lastfew years along the top we've learnedthat we need to evaluate and monitorthese models throughout throughout allthese phases as the world continues tochange and is very Dynamic andthroughout the bottom we've reallyconverged as an industry on how to logvarious artifacts models and modelRegistries features and feature storesmaybe pipeline dags in a in a templateGallery so now what we're going toactually do is we're going to extendthis framework and we're going tohighlight what we're layering on top ofthis and to make this easier to followfor the next 15 or so minutes we'regoing to layer on in blue some of theunique nuances that you need to thinkabout with uh with generative Ai andthen also highlight some of thecapabilities that were announcedyesterday or available for the last fewmonths on on our platformso first you know first thing we'regoing to do is we're actually going togray out some boxes and we're going toadd discover to the experiment box andwe're going to add and we're going toadd prompt to the predict boxin the past experimentation likely meantyou're exploring data you're runningdifferent training jobs and Hyperparameters you're really focused on avery specialized taskwell today the first step is verydifferent it's more around discoveringand finding a lot of differentpre-trained models that are capable ableto tackle many different tasks andfiguring out which which model is rightfor your task and you can really get avery quick head start the second thingyou'll notice is that there's a shortcutdirectly from discovering experimentingall the way to the model to be able torun predictions the only thing you needto provide are a set of natural languagepredictions or prompts and this isreally the reason why the a lot of folksraise their hand in the first questionthis has really lowered the barrier toentry to Ai and really a catalyst for alot of these excitements so firstquickly how do we handle this in invertex AIover the last few months and yesterdayyou've heard a lot about vertex modelgarden and vertex generative AI Studiouh you know quick primers on this modelGarden your Discovery portal your entryportal to a lot of different modelswhether they be Google models opensource models third-party models toreally get started and understand whichmodels may be fit for your use casewithout having to really start fromcollecting a lot of your data andlabeling millions of millions ofexamples and whether you're using use afully managed apis an open source modelor really abuilt model architecture a lot of thisis powered by really strongpurpose-built AI infrastructure withgpus and tpus now what's reallyintriguing to me is I know it's reallysmall on the right side but all thesetiles there's about 20 of them on thispage all these tiles today are actuallyin our prompt Gallery tiles around usecases like classification entityextraction summarization just last yearas you probably would have asked me howdo I get these types of capabilities uhinto my applications into my businessprocesses I would probably have aseparate ml pipeline a separate labeldata set for every single one of theseand now they happen to work magicallyout of the box uh with with simple textinstructionsso this is kind of the the first thefirst uh intriguing part of why we'reall excited however we all know thatthings are not as simple as they seemand not as easy as we always would likeso in the next iteration we actuallybring back the training and servingboxes and we introduce a few new termsin blue customize a tune distill and Ithink that the idea here is you don'ttrain these models necessarily in thetraditional sense the hundreds ofthousands of labels the millions ofexamples in this case you're rathercustomizing or tuning them for yourspecialized specialized tasks and tuningthese models we also added a curatedcurated data box in the very bottom inblue tuning these models will likely notrequire the same input pipelines orfeatures of of the past you'll reallyneed to rethink your existing data Lakeand data warehouse strategy reallypurpose built for these new types of AImodels and let's let's dig into this alittle bit by talking about some somespecific examples so you can get veryfar with prompt engineering but whetheryou're looking to further boost theperformance or optimize the cost andlatency of these models we really dobelieve that tuning will become verycommon across most workflows and yesthat that image was generated by vertexAI imagine of a dog that's beentirelessly prompting for the last fewdays weeks and then you know has toprobably not think about tuning tuningthe model so first we start withsupervised tuning this is really suitedfor tasks with a standard answer whichcan be proven or Justified for fairlyeasily and within vertex AI this isenabled through a simple Json L filewith two columns a lot of input andoutput examples that you curate for yourspecific task and tuning is nowavailable for text chat code as well asas well as image but for some tasks it'snot easy to define the expected outputespecially for summarization and chatuse cases you can't always you knowarticulate what makes one responsebetter the other however this is highfluency is still one of the key keyvalues of these larger models so rlhf orreinforcement learning with humanfeedback is really useful for thesetasks by really adding preferencesamongst options to update these modelsand yesterday again we launched rlhf inpublic preview for both our Google Pawmodels as well as open source modelslike llama2and then finally data curation at thebottom and I think we all start withthese pre-trained models we may not knowwhat you know uh what data was used forthem by various different organizationsuh but now we really need to add to thatand this is where we double click intowhat do we mean by data curation thismight mean augmenting really genericpre-training data sets with data setsthat are specific to your domain thismight mean really adding in differenttask mixtures that you want the model toreally understand or how curated dataexamples for a specific task and we workand I think this is actually a greatopportunity for a lot of Partners I meta lot of folks from the snorkel AI teamyesterday they have a data Centric AIplatform with programmatic data labelingdata curation and data management toreally accelerate AI developmentespecially once we move from promptengineering to the new phase of tuningas wellNow we move on to governing additionalartifacts I just have a you know alittle blue blue box around the governeddata and artifacts box and we have nownew artifacts tuning pipelines adapterlayers embeddings but how can weleverage existing best practices that wemany of us are familiar with and and Iwould say this is probably one of myfavorite topics because this is the oneI truly believe we don't need torecreate the wheel and I was demoat the demo Booth with a lot of folksand these are capabilities that existtoday on our platform so when you startuh and we really think that a lot ofexisting tools and best practices doapply so when tuning uh when we get intotuning these models with pipelines youreally want strong reproducibilitylineage tracking and metadata managementsounds very similar to our existingpipelines whether you whether you'reusing spark XG boost tensorflow pytorchand really tracking all your experimentsand all the artifacts throughout that sowhen you tune models on vertex AI forfoundation models this is allorchestrated by managed by vertex AIpipelines alongside your existingpredictive AI Pipelines once you get ahang of tuning you need to startmanaging these tuned models many tuningtechniques that we leverage today usewhat we call adapter tuning the onlyupdate a small set of Weights withinthose models and managing these adapterlayers alongside your existing modelsthey may be pickle files sklearn filesyour tensorflow save model files it is aclear win in Model Management to useexisting model registry capabilities andwith our adapter tuning service theseadapter layers are actually only tens ofmegabytes in file and then get passed toour foundation model alongside yourprompts to generate an inference soagain this is something that's availabletoday I was demoing it to many of youyesterday come take a look as well ifyou want to learn more about that andthen finally embeddings I thinkembeddings you probably heard embeddingsacross a lot of sessions over the lastday and a half they are really one ofthe most powerful and magical tools toreally extend llms even further theytake unstructured text images videos andreally create a vector representation ofthem to be able to do use cases likesearch and recommendations and andsimilarity matching well why not storethese embeddings alongside all yourfeatures so we actually are areintroducing embeddings as a as a featuretype within vertex's feature store sohopefully getting a sense of againespecially for artifact managementreally build on a lot of thecapabilities and toolingum that you that you're familiar withtoday and just extend them for the needsof ml Ops for generative AInow don't worry I did not forget aboutprompts I mentioned prompts in thebeginning but we do know that promptsare probably a very uh very new artifacttype some folks are maybe using uh gitthey're using maybe newer tools that areout thereum and you know vertex AI does come witha prompt Gallery it comes with a way toshare prompts across team members butwe're really excited to partner uh withreally strong innovators in the space inthis case we're excited to partner withweights and biases that actually has anintegration with the vertex AI Palm toreally manage manage and analyze yourprompts pairing weights and biases whichis really you know over the last fewyears known for their experimenttracking their link their artifactartifact track and capabilities we'reable to log and analyze the inputs andparameters of our of vertex Palm promptsyou're able to better understand anddebug your llms as as it goes throughthe different inputs outputs and this isa really great way to support evaluationworkflows I want to give it kind of abig shout out to Thomas from the weightsand bias team he published a reportyesterday for us that is available livetoday on that link you can you canprobably Google weights and biasesvertex Palm that actually walks youthrough a lot of examples on the topright we have a screenshot of logging alot of the parameters inputs outputs ofvertex Palm directly into weights andbiases and my favorite example isactually in the bottom right but weactually have a Vertex Palm Lang chainintegration I'm logging those loggingthe the flow of data into weights andbiases to be able uh using the agentintegration so being able to reallyunderstand when you get into slightlymore complex multi-chain agent use caseslike what is a nice way to visualize itand really troubleshoot and understandhow those input and output prompts areflowing from one place to the other sowe're really excited about that as wellfrom a prompt management perspectivenext we want to think about the top thetop layer again evaluation andmonitoring very important but a new setof metrics and challenges for forcertain use casessoum you know most of us love highlyquantitative very clear metrics mainsquare error Precision recall accuracybut how the heck do we evaluate andmonitor again large chunks of text maybeone part of that text has a numberthat's very important you know what isthe right target objective of of youruse case is it accuracy is it fluencyfor chat use cases is it factuality forAutomated Business decision making ifit's externally facing what is the brandand reputation Risk Arena is going tocome on stage and really double clickinto this and this really has talking toa lot of internal teams and Enterprisesevaluation is really one of the probablythe one of the top top challenges if notthe most challenging part todaytwo is capabilities on the monitoringside this is also a fast evolving spacewhat we want to highlight are twocapabilities that are available todayone thing is safety scores so anytimeyou provide an input get an output a lotof times you may actually delegateproviding the input to external orinternal users you want to really havecomfort and understanding what type ofcontent is being inputted or generatedout and within vertex Palm for everyinput and support request you actuallyget an automaticpayload with safety scores across 10plus categories that then you can relyon to Monitor and set appropriatethresholds for your use cases and thirdis recitation checking you know a lot ofus maybe are top of mind is uh know whatwhat if the output of this model is uhusing something someone else's originalcontent so we have built-in recitationCheckers that actually scan the outputof the model and then assess it versusexisting code repos existing webarticles and if there's a large enoughchunk that matches exactly we eitherblock it if it's very large or if it'swithin a certain certain threshold oftext characters we actually provide arecitation in the output of the API callso again different things that we're notused to from model monitoring space thisis very different than the traditionalthings around training and serving SKUor tracking feature and prediction driftvery different ways to really againaugment your model monitoring strategythe last part I think that introdu thatwe introduce in the blue is again thelast phase about about connecting toEnterprise data I think these Foundationmodels are still frozen in time strandedon on an island I think what we'rereally excited about is really that thebig opportunity maybe lies in retrievingdata outside of the ones that existeddirectly in the model itself during itspre-training phase as well as enablingcertain actions and once you connectconnect them to more data that makesevaluation monitoring even morechallenging so this again the thehorizontal boxes actually stretch acrossthis and probably you know addadditional complexity to thatum you know and for for this one we youknow I'm going to touch on this verylightly there's a lot of great sessionshere we'll touch on on the last slidebut to start exploring this new approachI want to really touch on really threeconcepts and patterns one is probablythe you know fastly adopted commonpattern to process your data throughembeddingsstore them in a vector database andquery and analyze them with your largelanguage model could be a base model itcould be a tune model that you put insome uh some effort into improving aswell now we want to highlight thatvertex has three core Services one wenow have a suite of embeddings acrosstext and image now you can actuallyproject both text and images to the sameto the same Vector space to really beable to compare uh compare the thesemantic knowledge within those we havevarious options to store theseembeddings and retrieve them we haveboth a Vertex Vector search asvertex feature store with new supportfor embeddings alongside your existingml features and of course variousoptions from vertex Palm to open sourceto third-party Partners so skintis really looking at a built-ingrounding capabilities really theability to generate responses based onyour own data providing citations toreally give you some risk mitigationaround again everyone's favorite topicaround hallucinations uh hallucinationsand here we have a joint solution bothintegrated into vertex palm and leverageby vertex search which you're able toactually do grounding on your ownEnterprise data and third yesterday iffolks attended that session we didlaunch vertex extensions with theability to authorsuccess and manage extensions to reallyconnect to real-time data and to realworld actions so these are some of thesome of the various Concepts andcapabilities to look into when you'rereally again extending generative AI toconnect to Enterprise dataso as you can see we've really taken youyou know through a journey how to buildon an existing framework that hopefullywas not too controversial and reallyextend it to support both predictive andgenerative AIum and and you know I think this diagramactually for folks who maybe werekeeping tabs of our of our launch blogsyesterday you'll you'll see a doubleclick into this this diagram and some ofthe specific launches that you can mapto each one of these sections so youknow we've you know we've introduced aframework of course you know we'veshared some some product capabilities uhbut what are some of our partners andcustomers doing so one of our one of ourfavorite Partners data tonic I got tomeet them last year uh based out ofLondon uh they're a data and AItechnology Services Company reallyhelping clients bringing AI models andsystems into production and they'vereally applied again their existingexpertise with ML Ops and vertex AI forthe new needs of llms and they've beenworking with a gaming company socialgaming company in Sweden called hyber uhto really bring gen AI powered gen AIpowered game creation into productionI really love the approach they sharedwith us on the right side if you noticefor those who may be keeping up withresearch paper the last few years thediagram on the right side is a view verysimilar to the 2015 paper hiddentechnical depth debt in machine LearningSystems where the LM box it is core butit is very small a small fraction ofreally the broader system you know thethe required surrounding system toreally actually get this into productionmaintain it monitor it is is it just asimportant and it really highlight somekey extensions to ml Ops including guardrails Vector databases human feedbackand maybe user ux user experiencechanges that are needed to really createthat data creation pipeline as well asprompt Registries luckily we actuallyhave a lot of folks here today andthey'll actually be presenting with withhybrid tomorrow for creating the gamingmetaverse with hybrid data tonic andgenerative Ai and again we're excited toreally see how many of the earlyadopters many partners and customers aretaking a taking generative VI intoproductionso uh um I think uhwrapping this up I think I'm going tohand it over to Arena to really go intowhat we again believe is probably one ofthe most challenging parts of bringinggenerated AI to production and that'sthe evaluation the evaluation processall right[Applause]thanks everyone for being here thismorning my name is Irina and I'm excitedto announce our new gen AI evaluationservicethis service helps you make decisions ithelps you understand if all of theexciting gen AI announcements that youlearned about during this conferenceactually solve your problemsevaluation is key along the entireenvelope's life cycle especially when itcomes to Jenny Idol because think aboutit the very first step in yourdevelopment workflow is to pick theright pre-trained model to work with andto make that decision you need dataso that data is an evaluation resultat the same time the generation of newcontents is just notoriously difficultto evaluatelet's look at this examplefor all of you I think it's easy to spota bad summaryit might be wrong it's too long it lacksstructure but what about outputs B and Cthose are two good summariesif I ask each and every one of you youmight have a preferencebut it will get difficult when I startto ask you about telling me the specificreasons why you chose one output overthe other and when you ask your neighborthey're going to have a differentpreference so there is no clear winnerherewhat makes it so difficult to evaluatethis generative outputswe identify three reasons first there isa lack of datain predictive ml we all started bycurating a sufficiently large data setwe kicked off every project withbuilding this data set then we have ourtests our trained test validation splitand that's all you need for evaluationnow with these generative models thebeauty of that is that you can actuallyget started with next to no databecause there are few and zero shotlearners but the downside of that samecoin is that you don't have the data toshow what a good output looks likethe second issue we face is that it'snot easy to evaluate these models byonly looking at metricsright think about the summary there isno one clear metric that describes howgood summary should look likehow a good news article should look likeor how a good painting should look likeif we think about it if we think aboutthe summary sample we can all agree on aset of proxiesno longer than 50 words truthful andconcisebut these preferences these proxies willdiffer person by personso preferences come into play when wetalk about gen AI evaluationand ultimately and you've all seen thisthroughout this conference the decisionspace suddenly becomes very large and Iwouldn't want to double click on that soI just talked about it you start bypicking a pre-trained model to work withfor this you need to understandperformance and you need to understandefficiency you need to understand costthen there is a number of tuning optionsto work withthen you might want to evaluate if youcan distill the model to get a bettercost performance trade-offand then ultimately evaluation does notend with performance you need tounderstand if your model is trustworthyto really put it in front of yourcustomerswhat we offer you today to solve thischallenge are the initial buildingblocks of our gen AI evaluation serviceyou can choose different evaluationtargetsto evaluation methods and access innumber of metricsand so if there's one thing you'retaking away from my talk today it's thisthis slide pleasewe're launching three services todayfirstautomatrixthis service it gives you a number andthis numbergood job on a specific task or notsecond order side by sidethis service helps you choose betweentwo modelsit helps you pick which one does best insolving your problemand as I said evaluation does not endwith performance there's also a trustelement to this which is why we launchedsafety biasthis service helps you understand ifyour safety filters are robustand I'm gonna discuss each of theseservices in a bit more depthlet's start with automatrixI think this is more similar to how wehow we evaluate predictive modelsit's a set of metrics it allows you toquickly and efficiently evaluate modelperformancethis is the standard method we see inAcademia and Industry benchmarkingand for this to workto provide us withdata setand we provide a pipeline that outputsemetrica number between 0 and 1that tells you if your model isperforming good on a set of proxieshere's how this looks likeso while this is coming up you can havealook at the slide no okay it's alreadyhere sowe have our this is the vertex AIpipeline view that some of you arefamiliar withwe have our model you provide us with areference data setthis reference data set is ultimately aset of prompts and the ideal outputwhat happens and this is why it's notlive it's taking a bit of time here ittook 14 minutes we now run the modelpredictions to get the new model outputwe compare this new model outputto the ideal output that you provided inyour reference data set we compute ametricwell and this is itfor us to choose the right metric youneed to specify which task you'refocusing onso in that case let me scroll down a bitit's a summarization taskthere you goandfor summarization theappropriate set of metrics is the Rougefamilywhat this set of metrics doesis in a nutshell measuring the degree ofoverlap between this reference text thatyou provided us with and the newgenerated contents and because I'mdealing with a situation here in my taskthat has multiple sentences I'm choosingroot elsamso here I see rujal Sam is at 0.12this model is performing really poorlyon a summarization taskall right let's go back to the slideswhile we go back to the slides you cankeep okay here it isnext slide pleaseokaysometrics are important as you needsomething that's quick and efficient andthat helps you guide model developmentlet's think about some if there is nooverlap between the text you provided uswith and the new text that's generatedthere is something wrong with this modelbut how to capture the the nuances of agenerative taskhow to get closer to human evaltoday I'm really really excited tointroduce auto side by sidethis is one of the most advancedevaluation capabilitiesthis method is first of a Kind it'sbuilt by Google research it's how ourinternal teams evaluate modelperformancethis service is comparable to humanevaluation but it's faster and it'savailable on demandwhat happens here in a nutshell is thatauto site by site allows you to a B testyour llmyou provide us input promptsand auto side by side then compares theperformance of two models against eachother and it's doing that by using athird model that we call the Arbiter sowe have a situation where it's llmagainst llm and there's a third llmthat's judging the outputslet's again have a look at how thislooks likeall right I'm back in this pipeline Viewthere's a lot going on here butbasically on the left side you seeeverything around Model Aso Model A in its prediction resultsthis is the model I've been tuning andplaying around with and then there'smodel Bsame story Model B and the predictionresults but this is the pre-trainedmodel that I started with and theexciting thing is down here that's theauto side-by-side Arbiter and again thistakes a while so this I already ranand the output that you see down here inthe right hand corner is a win rateso what is the win rate the win ratetells meis Model A or model B winning and inthat case the judgment is very clearmodel A is winning in 93 of all of thecases that the judge sawso Model A is the clear winnernowit's difficult to to trust this Arbitermodel right becauseit's running through all of the cases itproduces a number but then summarizationis such a nuanced task how do I actuallyunderstand what the Arbiter is doinghere comes myvery favorite feature of this service isthat this service is actually producingexplanations along with the outputlet's have a closer look at how thisworks and I'm going to zoom in a littlebecause I'm aware that this is verydifficult to read but I'm gonna readparts of it out loud so don't worryso I have a prompt here this is we'reagain back at our summarization task andthe prompt istalking about a roommatewho tries to choose a game to play andit's talking about risk and thenspecifies that well risk but with noluck something competitive and goes onand onand it seems that model A is doing agood job at summarizing diskit mentions the games that that I saw inthe prompt and does this in a quiteconcise mannerand then there's something not workingwith model B it tells us make a list ofthe best games you can play and repeatsthat a number of times then comes a listand the Arbiters agreeing with me hereit tells us that yes Model A is theclear winner but it also provides me theexplanation for why it thinks that modelA is the winner hereit tells me that response a provides agood summary of the contextit follows the instructions ofsummarizing in less than 64 wordsresponse B does not follow theinstructions right it's not really asummary and instead provides the list ofword games sothis explanation helps you reallyinteract with the Arbiter and understandifthis reflects your preferences or not sothis is what I'm very excited aboutother side by side but I'm specificallyexcited that we can give you auto sideby side with the explanation bitback to the slidesso our job of evaluating models does notend hereit's not enough to test that your modelis actually producing a good summaryyou don't want to put something that'sproducing a good summary in front ofyour customers you want to understand ifthis model this summary is trustworthyour safety evaluation bias serviceallows you to test the robustness of thesafety filtersfor this to work you need to provide uswith the model that you want to evaluateand that's itand what you get as an output is a tableit's showing safety filters overallMikhail already talked about these andit's also showing safety filters slicedby identity group and in this table Ipicked the identity group gender butthere's other identity groups to choosefromso this service really tells me twothingsoneis the safety profile of my tuned modelworse than the safety profile of mypre-trained modeland twois my model biased against a certainidentity group is my model producingmore toxic contents for say womenagainst menso after running the safety biasevaluationI can look at this tableI can see if I have safety issues if Ihave introduced the bias and then I cantake actions to mitigate that likelooking at my tuning dataall right that was it for the Jennyevaluation service thank you and I hadit back to Mikhail[Music][Applause]bring it home and I think as as a youknow tools guy that really was excitedto actually see a lot of that run onvertex pipelines and I think that gaveyou a sneak peek into again usingexisting tooling existing best practicesexisting orchestration of pipelinesagain for generative AI tasks includingevaluation so I'm going to kind of walkyou through again some some key closingthoughts around really building your mlop strategy for generative AI number onewe really again talked about no need tothrow away your existing mlopsInvestments and really learn from andbuild on your ml Ops knowledge andinvestments from yesteryearhowever we actually walk through ajourney of extending an existingframework about augmenting it with theunique unique needs of generative AIprompting tuning new metrics and andtechniques for evaluation monitoring andagain new new Concepts like datacuration and extending data to theEnterprise world now we really kind ofuh kept this talk a little bit morefocused on the ml Ops technology but onething again it just is important whenreally starting this strategy is alsowhat are the use cases uh that you startwith I think really our ourum our thoughts are around starting withvery simple single step use cases thatmay likely have a very risk low riskprofile don't jump into multi-stepcomplex complex chains or complex agentsthat you're letting loose and and reallyhitting on automated decision makingum you know again we we really suggestthat you start with use cases that don'timmediately need high accuracy and havea list risk profile maybe they have highfluency needs they may have highcreativity ideation objectivesum and those are great ways to getstarted I personally really like thisframework um that's that's out therepublicly that looks at two axes aroundaccuracy and fluency and really focusedon the top left where a lot of the keyadvantages of gender of AI with highfluency but again maybe accuracy is lowto medium with with again human in theloop exceptions throughout that finallywe also wanted to highlight that our ourorganization launched a tool forEnterprises Partners customers toquickly do a self-diagnostic on their AImaturity this is actually building onour existing AI maturity assessmenttakes about five to ten minutes it'sacross 22 questions and givesrecommendations and best practices onhow to across people processes andTechnology we'll actually we'll actuallyum this is live and available onlinetoday we also have some cards that youcan take home with you at the vertex AIBooth down in the AI Pavilion the verylast thing that we really want tohighlight that you know today's topicwas a little bit forward-looking but notfully a forward-looking perspective thisis Avail a lot of this is availabletoday for you to try out on Google Cloudspecifically on vertex AI platform thisis a sample architecture in dark blueagain the key components of discoveringexperimenting and deploying a model soplease take a look at vertex modelGarden generative AI Studio collabEnterprise to really dive into a lot ofOpen Source Code like a lot of our opensource models there in light blue thekey ml Ops capabilities that reallysurround that to allow you to governthisscale for both predictive and generativeAI again vertex pipelines vertex modelregistry vertex feature store vertexevaluation services are all there now tosupport both predictive and generativeAI needs and in green newer tools formonitoring and connecting your modelsonce they're deployed and again thisincludes the safety scores the built-inrecitation Checkers grounding on yourEnterprise data as well as extensionsuh you know some suggestions uh we kindof laid out a map here this was reallymeant as a broader high-level frameworkin the first column in Gray where a lotof sessions that either happenedyesterday or in the early 8 A.M slotthese will all be available for you towatch On Demand but what you do for thenext later today and tomorrow so on thelast two columns we've recommended againa few 100 level talks along the top thatthat double click into EnterpriseReadiness common use cases for those whomaybe want a little bit more of anindustry use case spin spin on thingsand then we've highlighted a few 200 300level sessions that double click intoour vertex feature store into promptengineering and announcing our newembeddings that now support multimodalimage and text embeddings I'll be aroundactually from 12 to 3 today at thevertex AI booth in the AI Innovationsection so please come say hi we'rehappy to give you a demo of a lot of theconcepts we've talked about today and wealso actually for those of you are handson keyboard we really strongly suggestyou go to the innovators Hive led by adeveloper relations team where you canget you can actually go through a lot ofcode labs and really get some hands-onexperience with a lot of these toolingslike every other session we really wantyour feedback any feedback is is greatfeedback so if you could log into theGoogle next app give us a score let usknow what we could improve and we'reexcited to maybe share a lot more aboutml Ops for predictive and generative AIin the future through other channels uhwhether they be blogs or webinars andwe'd love to hear your stories so pleasefind us today and tomorrow share yourstories your best practices yourlearnings maybe some horror stories thatyou may want to share share with us sowith that being said uh thank you somuch and uh yeah come find us and uhhave a great great rest of yourconference[Applause]foreign"
}