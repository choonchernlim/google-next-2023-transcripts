{
    "title": "Unlock new video and image use cases with Imagen on Vertex AI",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML113"
    ],
    "video_id": "F4YKREYPkEI",
    "time": "Aug 31 11:00 AM - 11:45 AM CDT",
    "transcript": "foreign[Music]good morning everyone and welcome I hopeyou all are very well caffeinated wehave a bunch of exciting content for youin this sessionso let's start with introductions firstmy name is Avanti Sani and I'm a productmanager in the Google Cloud AI group andI have with me two amazing guestspeakers today who represent some verycool Industries and they will be talkingabout how they are using our image andvideo gen AI capabilities to power theirbusinessesso let's spend a moment chatting brieflyabout why image and video content ismore powerful than textual content andthe answer is very simple it's because80 of human Learning Happens visuallyand 65 percent of us are primarilyvisual learnersin fact the human brain takes about 13milliseconds to process an image whichis about 60 000 times faster than textand this is validated by what we see inthe world today right look at the sheernumber of images and videos on theinternet so here are some fun facts uhvideo has a 92 percent reach for theworldwide audience and 3.2 billionimages are shared almost dailyand U.S Enterprises feel this effectright whether it's in how you'recommunicating with your employees or theimpact it's having on your customersso at Google Cloud we understand thisand we started exploring how image andvideo generation capabilities could helpbe a game changer for your businessso keeping that in mind the firstproduct we recently announced wasimagine on vertex AI this is ourEnterprise class image generation andunderstanding solutionfor this session we will be focusingmore on the uh understandingunderstanding part which is basicallytwo features image captioning and imageq alet's start with image or visualcaptioning so what does it do it's verysimple you give it an image and itcreates a text caption for itso on the right there's a really goodexample for retailers or e-commerce solet's say today for retail or e-commerceone of the biggest challenges is havingaccurate product descriptions they havemillions of products they have to dealwith and typically they have very lessinformation to work with so usually itis some small description a humanhurriedly typed up or they'll have maybea pack of photos a product ID and maybelike a very small description so withthis using an API and you know and youneed no prior knowledge of AIML ormodels or fine-tuning or any of thatit's a simple API call you could veryquickly build out a very rich productcatalogthere are some other use cases alsocaptioning can help it so one of courseis social uh when you look there's a lotof images that go out in your socialmedia marketing and you may wantcaptions for it which you can use todrive hashtags for example you can alsouse it for any kind of Auto tagging orcategorization now that you know whatthe image is actually about and then ofcourse accessibility it makes it moreaccessible for all human beingsso let's look at a demo these are thetwo images I will be using in the demoand just by the way they were created onimaginethank youall rightso if you go to a generative Studio youshould have a capsule you should have atab called caption and you can go thereand you can upload an image now thisimage does not have to be generatedspecifically by imagine it could be anyimage you haveon the right we have some parameters soyou can choose the number of captionsand you can you have a choice of fivelanguagesso let go let's go ahead and generatethe captionthere you go pretty accurate huhlet's try another image and let's trydifferent parameters this timeso let's go for the sushi one I showedearlierlet's choose three captions and let's gowith Frenchso the latency should be about 20 to 30secondsand there you go anybody who speaks andreads French and can confirm I didconfirm with the French colleagues so Ithink we're goodso that was image captioning for youlet's move on to image q aso this is again very simple you canupload any image again it does not haveto be generated by us and you can ask aquestion about the imageso for example on the right uh you coulduse this let's say if you were thegovernment and you wanted to make atourism app for disabled folks thiswould be a great way uh in which theycould navigate people processes objectsask questions about it and make theirwaynow when it comes to visual q a thereare some other great use cases for it sonumber one if you have any in-storeexperiences so you may have kiosks andtouch screens you may want uh yourcustomers to be able to interact withyour products better and even if youdon't have in-store experiences you maywant to be able to enable that on yourdigital websitesso great way to capture q a impactso let's look at this demo again so thesample image is on the rightagain in a generative AI Studio you havea tab called visual q a where you can goand upload an imageso let's start by asking a questionagain you have parameters like number ofcaptions and right now we only supportEnglish for this feature but we willhave multiple support multiple languagesupport coming soonso let's start with a question where wetry to see whether it generally knowswhat's happening in this imageyeah it got right there is recordinggoing on now let's see how well it doeswith Countingright there you go now let's see if weask a specific question of about anobject in the image so let's ask themwhat the man is wearingand got that right now let me ask ahigher level semantic question like sayhey what's the vibe in the room forexamplecasual so it is kind of getting afeeling of the image and is able toanswer that very wellso that was visual q a now let's move onto video description so the two featuresI talked about are more on the uh imagenow this will be about video so videodescription vertex AI is a feature withwhich customers can get semanticunderstanding of a videouh in the form of textnow the thing to remember is this is notvideo summarization where you start tolose information for longer videosand this is not speech to text or closedcaptioning where the model focuses onlistening to the video to make sense ofit in video description the model isseeing what's happening in the video tomake sense of it now this will becomeclearer in some of the examples I haveso let's take an example on the right sotoday when you go to any video platformright when you type in a search like inthis case a car chase typically thevideos that come up will either have carchase in the title or in the moviedescriptionbut with video description now it willalso show videos which don't have thatterm in the title or the description butmay actually have a scene of a car chasein it so this is very powerful from arelevancy perspective it does two thingsso as an end user when you search itcreates more meaningful content andmeaningful searches for you and from aplatform Builder perspective now you cankind of cross-sell more of your videocontent and surface more and more of itso it's a win-winthere are some other great use casesvideo description helps unlock so one ofthem is of course metadata now that youhave such detailed information about thevideo of course you have richer metadatawhich you can use to power you knowbetter recommendation engines orsearchesanother one is automated captionssimilar to the one we saw earlier withimages so if you have short form videoslike 15 seconds 30 seconds you cancaption thoseaccessibility you saw the tourism appexample earlierand the next one is understanding andsearching important moments so this is areally cool one so for an entire videoif you had this detailed rendering inthe form of text now you could dosomething with it so now that I have thetext format I could search for importantterms in it so for example let's imaginewe have a 6r baseball game video rightand we have its rendering now I couldsearch for terms like home run strikeaudience steering and then it once Ihave those moments I can stitch themtogether into say a highlight reel soyou see how that really helps youunderstand your information and evenchange it into different formatsanother great one of course would be umalerts or workflow based actions so forexample let's take a surveillance videoyou could set up alerts saying heyanytime a word the word knife shows upor gun shows up or people running showsup you can inform the local policeauthorities so this is a very powerfulway in which you can connect it to someother actionthen there's a better ad placement ofcourse now that you know what the videois about you can insert more relevantads so if I have a video of bakingcookies I can insert an ad for milkthe next one is driving deeper videoanalytics this is a super cool one sotypically anybody any organization whodeals with videos and has a large amountof videos is always very curious totrack how those videos are being used byusers so for example let's take drop offpoint which typically most customerswill measure so today when you do adrop-off Point all you understand is theuser stopped watching the video at let'ssay one minute and 29 secondsbut what if now you could crossreference that with what was happeningat one minute 29 seconds now you've gotan insight about what the user likes ordoes not like and this can drivepowerful personalizationand then finally some of you might besitting on reels and reels of Legacyvideo content and with no understandingof how you're going to figure it outright so with video description youcould catalog it which could lead you toI don't know discover golden nuggetsfrom the past or repurpose your contentso as you can see there's a whole lot ofvalue we can get from video descriptionslet's see a quick demo againso similarly same studio now we have atab called video description you goahead and select your videoagain for the two minute video it'sabout 20 to 30 seconds latencyand there you go for every 15 secondchunk it gives you a descriptionand in the UI experience you canactually click on the timestamp and goto that portion of the video so you canget you understand the relevancy and youmaintain contextyou can save this as a Json file and bythe way all the three features I talkedabout today all have API support rightout of the boxokay so oopsso that was the demo so we talked aboutthree features here and I talked aboutdifferent use cases for all of them andyou could you see how beneficial thosecould be but honestly their true powerreally comes together when you use themin partnership with each other so I havea hypothetical case study here uhthrough which I would like to demo thatso let's suppose we we are a real estatecompany called symbolright and you know as in any industry weare trying to stay relevant with thetimes we try to stay updated we try tostay relevant to our customershowever like any industry we strugglewith some headwinds so let's pick threehereso one of the biggest pain points inreal estate is lack of inventory now ofcourse this could be because there'sjust not enough homes on the market butit could also be because you're just notfast enough in creating listings whichis a pretty detailed process you'll keepto sit down and think about it andcreate thatthe second one is long buying Cyclesobviously people are spending hugeamounts of money when it comes to realestate so they want to be very carefulthey do a lot of due diligence and sowhat that means for you is longer leadtimes and maybe low conversion ratesthe third one is stiff competition sofor our buyers and sellers now there arenewer models of transacting property andso there's always new businesses poppingup how do you create Revenue with all ofthis going onso let's see how a combination of thefeatures I discussed earlier can helpwith some of theseso first one lack of inventory what ifwe provided a capability for our realestate agents where either using amobile experience or a websiteexperience all they had to do was takepictures and video of a house and thenit auto creates a listing for it whichincludes a beautiful description and italso extracts information so like thingslike hey hardwood floors pool HVAC whathave you because that's what being usedby your end users when they search forhouses and it creates all of that inabsolute secondsthe good thing is as an API developer itwill be easy for them to set this upthat's one and then it also from an enduser perspective so the buyers or therenters when they are searching they cando more open-ended searches like todaywhen you go to any real estate site andtry to narrow down your search it's likea lot of selecting a lot of filtersright but here I could maybe even put ina search saying hey show me a housewhich has hardwood floors in the livingroom and carpet everywhere else and apool in the back here so it creates acustomer Delight experience just becauseyou have that much knowledge about yourcontentlet's look at the next challenge longbuying Cyclesso like I mentioned earlier typicallythat's because most people want to do alot of due diligence which means theywill make multiple visits to theproperty check it out again and againand that is what creates the longestlead timewhat if you could provide thisinformation maybe in a website or amobile app where they could askquestions on their own time as requiredum and so that will reduce the time theyspend on evaluating it you can also flipthis feature along so now that you havethis information you could use it forsellers when they list their house yousay hey we notice there's mold in thispart or there are some tiles missing onthe roof so they can get ahead of it anddo those improvements before they run itinto it during the buying cycle so againshaving of valuable moments of time fromthe from the lead timeand then of course more inclusivitythat's always a plusthird one so stiff competition so weprobably cannot stop other businessespopping up but what we could do is wecould create either productdifferentiation or even launch newstreams of Revenueso in this example one way we could dothat is because we have all of thisinformation about the house and itscondition and its state and so to helpthat drive the value of the house wecould provide e-appraisal services forexamplesome other use cases we could use thisis for uh with the Q a chatbot I showedearlier maybe now you have online agentswho are available all the time to answeryour questions you know whenever theuser wants it to and then finally youhave information about your customerswhat they want you have informationabout the houses with images and videosyou could create personalized brochuresso that creates product differentiationand again customer delaysoto summarize using the three features Idescribed above which were imagecaptioning image q a and videodescription and especially using themtogether you can save immense time andeffort in understanding the content youalready have that's one once you knowyou have that information it will helpyou get deeper understanding and insightof not only the content but also howyour customers are interacting with themand then number three all of thistogether you could create new Revenuegeneration streams or create productdifferentiationso with that it comes to I come to theend of my part of the session we'vetalked a lot about hypotheticals but Ithink we should talk more about reallife customers are using this in theirbusinesses and with that I'd like tohand over to Tomaso who will share alittle bit about how they are usingthese features to drive morepersonalizationgo aheadawesome thank you Avanti thanks for thebeautiful information that you sharedwith us about vertex AI imagedescription of your description so I'mThomas I'm the general manager andco-founder of connected storage andtoday guys I want to walk you through areal real life Enterprise use case ofhow we use image description and videodescription within our platform uh whatis our platform uh connected storiesessentially sold for buildingpersonalizing uhum optimizing optimizing advertisingcreative at scale by combining the powerof creative regenerative AI toessentially remove the complexity andtherefore democratizing access to thedriven created so why personalizingcreative is important essentially mediamarketers have only focused onoptimizing targets that'll be watchinginventory whereas the majority ofsuccess of a campaign actually runsthrough and is derived by theeffectiveness of that creative sotherefore coming up with effectivecreative is key in order to have asuccessful marketing and advertisingcampaigns however in order to do sopersonalization and continuousoptimizations are actually key elementsin order to enable that sort ofeffectivenessbut in order to fully harness the powerpersonalization and uh being able tocontinuously optimize such creativeunderstanding and measuring creativebecome key ingredients to success sothat's why when we launch connectedstories since day one we wanted toutilize AI across all the different treemodels that you see up here on theplatform so the one on the left isactually our creative Studio which isgenerative AI powered it's amulti-tenant solution that allowscontent to be brought to life throughthe use of a canvas like interface forContent creators to mixum content that are generated bymajority of AI through the integrationof palm and image and within theplatform as well as interactive widgetsto which we can give data-drivenconditions on and therefore applyingthem in real time which is thenorchestrated by by our AI baseddecisioning system the users Uh custommachine learning models that are trainedwith our own data real-time signals aswell as first part of the clients andthese essentially acts as theorchestrator that selects the differentvariations so that jgi power Studiocomes up withand finally everything is then measuredin terms of Effectiveness through ourcreative intelligence dashboards they'reusing for example Vision AI tounderstand very generally how users areinteracting with the different pieces ofof the content that they're exposed toso this brings me to the use of nowimage description actually yesterday onon this stage we launched a newcomponent of uh our platform calledJenna Suns it's our brand new creativestudio and essentially allows for a newchat like interfaceto help as a co-pilot content creatorsto strategize campaigns starting from acampaign brief so campaign briefcontains not only obviously text butalso images and and videos so thoseimages and videos are now through imagedescription and video description can besynthesized obviously into into testsand described into text allowingtherefore ourum new chatbot like which is called Leoshort for Leonardo da Vinci to thenorchestrate all the different promptsthat are then utilized within thecreative Studio to come up with the morepersonalized creatives so this hasbecome something that was impossible forus to do before especially manuallywhich now to the use of image and videodescription we can now doso this in turns allows to create muchmore and much better variations of thepersonalized creatives that our securityStudio comes up with so even in thiscase we are able to actually utilizevertex AIS apis in terms of imagedescription and video description tobetter describe very granularly whatusers are being exposed to and thereforethe content of each single personalizedcreativewhich are then extremely helpful for usto come up with recommendation becausebefore we we didn't have the chancenecessarily to be able to understandgranularly why users were being exposedto uh whereas now every single contentis better described and therefore itallows us to reallyunpack better insights which thereforelead to recommendation and betteroptimization so this creates anextremely positive feedback loop thatstarts with the studio this the thenum keeps on going with the AIdecisioning system and back to creativeintelligencewhich is poweredum all uh with image description andvideo description underneath even thoughagain you cannot necessarily see it isnot super tangible but this leads to amuch more scalable process way lessresource intensiveand therefore these essentiallysummarized in Greater information thenbring better performance thank you overto you uh Avanti thank you thank you somuch to masu that was amazing and nextWe Have Bryce Perez who is uh speakingwho's a googler but he's speaking onbehalf of theta labs and they are usingsome of these features to drive bettersemantic search and highlight creationso let's hear from pricethank you Avanti appreciate it wonderfulworking tomorrowwellbasically I'm A Cloud engineer customerengineer for the web 3 team and I amhonored to basically be presenting onbehalf of theta Labs so shout out to theTheta lab CEO Mitch Leo and ctoj longalong with our team at Google Cloud web3we have been working closely together tobring exciting meaningful applicationsfor video to text AI other efforts todescribed today will only scratch thesurface on things to cometoday I want to talk to you about themodel pipeline video text applicationsthe goal is to set out to accomplish isharnessing the computational power ofboth cloud and Edge nodescloud is distributed Edge ourcomplementary complementary Computingplatforms cloud has the advantages ofoffering stability uh at Cloud at scaleand high availability while the edge isubiquitous with with virtually unlimitednumbers of nodes these nodes mainly areless powerful just by themselves butwhen aggregated they can provide massiveparallel processing power while locatinguh more closer to the users a hybridComputing infrastructure could combinethe substantial benefits of Both Worldsthe Theta ads network is one of theLeading Edge Computing platforms theTheta Edge Network provides ininfrastructure for decentralized videostreaming and delivery the four keyadvantages of data Edge Network arescale availability resource and simpleuxthe simple ux allows a non-tech user toSimply download the data Edge nodesoftware and run it in the background noactive interaction is required theprovided of Rich UI experience basicallyallows for job status to be shown andallow you to earn some feedback as wellhere we introduce the concept of modelpipeline running on a hybrid platformconsisting of Google cloud and data EdgeNetwork a model pipeline allowsconducting a tasks using multiple deeplearning models consecutivelyso some of these models can be Googleproprietary models and there'soffering accessibility through GoogleCloud's API others can be open sourcemodels that can be served by Theta EdgeNetwork leveraging massive parallelprocessing power as mentioned as shownin the diagram that you see the inputdata versus transformed into anintermediary representation use cloudhosting models then the intramural datais processed by models hosted in theedge Network and turned into a finaloutput although the diagram only showstwo models the real application tasks toutilize multiple models as shown inlater slidesnow that we have introduced the conceptof the pipeline let's use a coupleapplications for an examplebasically we'll utilize the videoentertainment industry to illustrate itspotential the first application insemantic video search traditionallyvideo search only allows searchingagainst title description and possiblytags but these descriptions usuallytypically provide the video uploaderswith some core quality that may not ableto capture the full details of the videowith model pipeline we can achieve muchbetter experienceso two images are shown it depicts thecontent creator Alice uploading videosto a video platform and after the rawvideo is processed by the model pipelinea user Bob can type in the naturallanguageum into the search bar for semanticvideo searchthis could be allow Bob to accuratelySearch interesting moments inside thelong video a massive video library isautomatically locate the replay positioninside these these typically long videosfor an example we are in the Bay Areaand by default for today we're all goingto be Golden State Warrior fans is thatagreed upon so we're going to querySteph Curry's three-point buzzer thatwas in the 2018 NBA finals this coulddirect you to exact repay location ofthe three-point inside the two hour longrecording of the first game of theWarriors versus Cavaliers in 2018 NBAfinalshow can we Implement semantics videosearch using model pipeline so the modelpipeline for this use case can beconsisted of three stagesthe video the text capability providedby Google cloud model video via thevertex AI or API API Suite the vertex AImodel takes a raw video spit out asummary of the each 15 second clips ofthe video next the llm runs the the onthe in the Theta Edge Network andcollects the clip summariesand generate tags and also a high levelof detail but the detail summary is canbe for the entire video as well theembedding of the tags Clips summariesand video summary are stored in thevector database hosted by The EdgeNetwork finally when the user types insearch for with natural language toembed it a query is computed in realtime which is matched against thesummary and other information found inthe vector database for the best videoin the exact repay location for thatqueryso here's another exact example so wewant to be able to generate an automaticgame highlight real generation thisencompasses not just extracting excitingmoments from a video game stream or atypical football game screen but alsogenerate natural language descriptionsof the Highlight clipsas shown in the figure a game streameruploads video and then the systemautomatically transcribes the penaltykick and the Epic goalkeeper safehow can we implement this feature usingmodel pipelines similar to the videosemantic search we use case we use thevideo to text model and the clipsummaries in the raw video uploaded bythe game streamer next the llm hosted byThe Edge Network perform Eclipsesentiment analysis based on when weassigned a score of each clip judgeswhether the clip has a highlight momentand then finally we can feed the rawvideo in the beginning and end time ofthe clips with top scores by offeringinto a FF impex software running in thedata Edge Network and extract thosehighlights and combine them into ahighlight reelfurthermore we can feed the HighlightReel back into the vertex AI model whichcan create natural language descriptionsof the Highlight rule which could beuseful for end users and contentcreators I hope that what I share withyou inspire you to build new ways tocloud with vertex AI Bronte back to youthanks thank you Bryceso I think with thatwe come to the end of the session I hopeyou enjoyed it please spend a fewseconds giving us some feedbackso thank you all for coming andlistening today it was a real pleasuretalking to you all hope you have a greatrest of the day foreign"
}