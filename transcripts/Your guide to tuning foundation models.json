{
    "title": "Your guide to tuning foundation models",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML301"
    ],
    "video_id": "_bdz8A-_XLI",
    "time": "Aug 30 10:00 AM - 10:45 AM CDT",
    "transcript": "foreign[Music]everyone welcome to oh I think I wenttoo far all right welcome to your guideto tuning Foundation models thank youfor showing up at 8am my name is Nikitaand I'm joined by my teammate yarik andtoday we'll be talking a little bitabout how to train Foundation models onvertex Ai and then Hussein from flybitzwill take the stage to share with yousome practical lessons about tuninglarge language modelsbut before we get into talking aboutdifferent methods and ways of tuning Ijust want to spend a minute talkingabout why we might want to tune afoundation model on a custom data set inthe first placeso if you've had a chance to experimentwith some of these Foundation models youmight have noticed that one of thethings that makes them so powerful isthat a single model can be used for somany different tasksso depending on the way you structure aprompt you can use an llm for somethinglike brainstorming if you write adifferent prompt you can use that samemodel for classification and that samemodel for extraction and that same modelfor summarization and you get the idearight depending on how you write aprompt we can use these models for allsorts of different tasksbut depending on the task you're tryingto solve coming up with prompts can betricky small changes and wording or wordorder can impact the model results inways that aren't entirely predictableyou can't really fit all that manyexamples into your prompt and the moreexamples you try and shove into yourprompts the more eats into your inputcontacts window it's a very iterativeprocess and even when you do find aprompt that works well for your use caseyou might notice that the results aren'tentirely consistentso what I'm saying is that if you spendenough time trying to experiment andprompting large language models you kindof start to realize that a single modelthat can do absolutely everything hassome practical limitationsbut luckily if we do task specifictuning we can make these models morereliable for certain use casesSo today we're going to introduce you tothe two different tuning services onvertex AI we have supervised fine tuningwhich is well suited for tasks where ahuman could kind of relatively easilycome up with a standard or correctanswer that could be kind of Justifiedor explained but there are some usecases where it's not so easy to come upwith what the correct answer is you canthink of that as like a chat use casewhere maybe if you were given a coupledifferent examples of answers you'd beable to pick which one was better but itwould be hard to say exactly what makesthe correct answer correct and for thosetypes of scenarios reinforcementlearning from Human feedback is reallyusefulso I said there were only two servicesbut actually since you all showed up at8am today I'm gonna let you in on asecret we have a distilling step-by-stepand experimental we won't be talkingabout it today very much but this is atuning technique where you transferKnowledge from a larger model to asmaller model so stay tuned for thatagain we'll be focusing on supervised bytuning and rlhf today but just wanted tolet you know we have this anexperimental and stay tuned for moreinformationokay so broadly speaking when it comesto tuning models we can kind of take twodifferent approaches so if you'realready familiar with machine learningif you're familiar with transferlearning you might know about full finetuning and this is where we take a modelthat's been pre-trained on some genericdata set we make a copy of that modeland then we retrain the model and weupdate all of its many weights but withthese Foundation models they're veryvery large probably larger than modelsthat most of us have been used toworking with in the past and so a fullfine-tuning job requires a very largetraining job it's a lot of compute andthen at the end of that you have thisgiant model that now you have to serveum so in a lot of cases this isn'tnecessarily the best option and soalternatively what we can do issomething called adapter tuning wherewe're only updating a small subset ofthe model weights and not all of themodel weightsso on vertex AI currently for bothsupervised fine-tuning and reinforcementlearning from Human feedback we arereleasing adapter tuning which enablestuning the model to specific taskswithout having to rebuild the entireFoundation model and this happens viaparameter efficient techniquesso these small subset of parametersmight be a subset of the existing modelparameters or an entirely new set ofmodel parameters my diagram here is alittle bit simplified but hopefullyconceptually it gives you the ideawhat's going on here the exact set ofparameters that are updated depends onthe technique you're using and the kindof optimal methodology is an air openarea of research it keeps changing butregardless of which kind of methods youchoosethere are some big benefits to adoptertuning so the first is that you know youget faster training because there'sfewer parameters to tune and you alsoneed less training data than you wouldcompared to full fine tuningand then parameter efficient or sorrydoctor tuning also makes serving modelssimpler as well because you don't haveto serve the entire base model you justhave to worry about the specific tunedweights you have and then attachingthose onto your base model and you'regood to goand the way that this works in vertex AIis that adapter tuning jobs run on cloudtpus or gpus via the vertex AI trainingservicevertex AI creates a separate tenantproject for each customer project andthe training workloads run on VMS inthat tenant project so the key takeawayhere is that your training data yourinput prompts and your tuned weightsnever leave your environment and theyare never used to tune the foundationmodels so we have a whole white paperabout this if you want to learn more andlearn more about how that works Iencourage you to check that out I've gota screenshot here it's called adaptationof large Foundation modelsokay so full full fine tuning andadapter tuning aside let's talk aboutthe Practical things how do you actuallyuse these services on vertex Ai andwe're going to start with supervisedfine-tuningso like I mentioned before this is wellsuited to tasks where a human can kindof come up with a standard or correctanswer that can be justified orexplained relatively easily so you canthink of good use cases as you know youwant to teach the model like specificstructures or formats formats forgenerating outputs specific behaviorslike when to respond with a terse outputor maybe being more verbose and the wayyou structure your data is in each inputrow or record in your data set is yourinput prompt followed by the expectedoutput from the modelso one way I like to think about theseuse cases is they're really supervisedfine tuning is well suited for exampleswhere you want to take data in oneformat and convert it into anotherformat and like while that might soundvery boring there are a lot of differentuse cases that kind of fall under thispattern so something might be taking aconversation and removing all the pii ortaking some unstructured descriptionabout a clothing item and thenrestructuring the key attributes as Jsonor taking a user request extracting thekey pieces of information and thenreformatting it in some domain specificlanguage and all these use cases thereis some notion of like what is thecorrect answer and they make for verygood use cases for supervised finetuningand you can kick off with supervisedfine-tuning job from the cloud consoleor you can use the vertex AI python SDKif you like but regardless of whatmethods you choose all of the vertex AITraining Services are built on top ofour existing ml Ops tooling and so thismeans that all of your trainingworkflows are orchestrated as vertex AIpipelines like this one here so you canalways monitor them in the cloud consoleand all of your tuned models will beavailable for you in the model registryso with that I'm going to hand it overto yarick who's going to talk more aboutthe rlhf service on vertexokay[Applause]so as Nikita mentioned there is a set ofNLP tasks which can be very welladdressed using techniques describedbefore so supervised fine tuningespecially tasks where mapping betweenoutput input and output is very welldefined and outputs are unambiguous sotasks like classification some forms ofextraction and the extraction extractiveq a even some cases of summarizationlike extractive summarization when it'spretty well defined what's very rightanswer but there's many other scenarioswhere it's difficult to say what's agood answer let's say a summarizationtext may be summarized in many differentways and each of our ways may be correctbut may be preferred by different peopleby different audiences so there is thiskind of loosely defined element of apreference and a preference for maybeeven a specific audience or a specificgroup of peopleand the reason behind it is that when wetrain pre-train the models and when wefind Universe using supervised finetuning the training objectives in a caseof pre-training predicting the nexttoken or in case of supervised tuningmapping the input to the output do notreally reflect what we really want to dowhich is to train the models whichfollow user instructions user intentionsand do it in such a way that they arehelpful they areharmless and they are honestthose three words are not random butit's in fact a paper which explainsexactly what we mean here when modelshave those characteristics and it's verydifficult to capture those objectivesusing the standard objectivefunctions just when we use pre-trainingor supervised fine-tuning scenarios sothere's a lot of techniques this is infact a very exploding field of trying toalign the models better with userintentions user preferences or with someUniversal values which we as HumanityMay value and respect one of thosetechniques is RL HF and in my talk I'mgoing very briefly describe rlhfalgorithm as this was introduced by openAi and as we implemented in our solutionand then we'll go through how to use itin vertex environment so let's startwith a what is rlh so our lhf techniquewas described in pretty good detail infact very good detail it's between themain part of the paper and appendicesclose to 70 pages in this paperpublished by openai training languagemodels to follow instructions with humanfeedback in this paper they refer to thetechnique described in another paperfrom 2017 deep reinforcement learningfrom Human preferences this is a paperwhich was the written jointly by adeepmind and open Ai and in this paperwe introduced technique of introducinghuman feedback into a standardreinforcement learning Loop in this casethey were training small robots andtrying to build the models which playAtari games but the bottom line is thatthe key innovation in those of thetechniques was incorporation of this ofa human feedback and human intentionsinto the training process so let'squickly go over rlh R lhf usuallycomprises three stages you start with amodel you want to fine tune this modelmay be a base pre-trained model it maybe the model which was instruction tunedit may be a model which was evensupervised tune on a specific set oftasks yes but you start with a modelum and values this model to generateoutputs you have a set of problems anduse this model to generate the outputsand for the single prompt you knowgenerate multiple outputs like if youread the paper open AI generated betweenfour to nineresponses to a given prompt and bychanging some of the parameters likenuclear sampling on of those commonparameters you can control whengenerating outputs to the prom you willget different answers another is a keystep in the process those answers arenow rated by humans and the way they arerated to avoid some issues yes withabsolute scales where human laborerswill compare one answer to the other andthey will specify which they prefer andthis is your data set yes so you createbasically a data set sometimes referredto as a preference data sets and youwill see the exact form format of hisdata set as we require that in ourservices but the conceptually the dataset has a prompt it has two answerspossible answers to the prompt and ithas a label which defines which of thoseanswers was preferred by human labor sothis data set captures preferences ofhumans now to be more precise it doesn'tcaptures preference preferences of allhumans it captures preferences of aspecific labeler or laborers who wereused to create this data set now thisdata set is used to create somethingcalled a reward model this is the modelit's effectively a regression modelwhich takes normally an input sequenceand retains a square scalar value andthis valuethe nodes how well the modelpreferred an input sentences that's whyit's called a reward model so the higherthe number the better learn the inputsequence or the or response from themodel is aligned with the values andintentions captured in very World modelnow as you can see on this diagramrlhf and reward model the way I Definethis only one way of incorporating humanfeedback into the fine-tuning loop saysthere are some other methods there issomething which is pretty hot these daysin research called constitutional Ai andmost vendors apply a few differenttechniques which go beyond thestraightforward rhyp in theirimplementations with the detail yesthere's a little bit of a magichappening in those implementations Icannot talk about yes but there's it's avery very active area of research andthere is a constant improvements almoston a daily basis okay so now we have avery old model so this reward model isnow used in the second stage of trainingwhen we now fine-tune our original modelutilizing this reward signal from areward model as part of an objectivefunctionand the algorithm which was described byopen Ai and which we also usein our implementations called PPO aproximal policy optimization so this isa pretty old well understood very wellperforming algorithm from reinforcementlearning and one of the key features ofthis algorithm is that in addition toincorporating reward signal into thefine tuning process it also make sure sothat you don't overfit the model to thereward signal so it kind of there is apenalty component in the objectivefunctions which makes sure that as youtune the model it stays relatively closeto the original model whileincorporating those the feedback andassessment of the responses to theprompts as defined by the reward modeland why I'm mentioning that as you willsee when we go through our pipelines andthere is a key parameter you have toconfigure which controls this panel yesand controls basically the PPO algorithmyeah so that's the process again youstart with creating a preference dataset in reality this is probably one ofthe most complex part yes how do I findlaborers who can properly create thisdata set and if you read the paper itmay be in fact a pretty complicatedprocess any expensive process to createthis data set when we use this data setto train the reward model and then weuse very reward model in a reinforcementlearning Loop just to fine-tune our llmby the way reward models also start asan alarms in in this context yeah theyare usually a little bit smaller ormaybe a little bit smaller than an LMwhere I find tuning openai mentions 6billion parameters for for the base fora reward model our reward models areabout the same size yes when you whenyou look at implementation but they canbe as big as the model you tune if youreturn Tropic papers or some of thepapers from deepmind they used reallylarge models as reward models modelswith 70 billion parameters and so onyeah so it is still a choice yes of yourimplementation and a little bit ofexperimentation which is going on inthat space okay so this is the algorithmhow did we oh first of all does it workno it does work because this is a coupleof charts one chart the chart on a rightside of on my right side of the picturethe diagram is from original open eyepaper which shows you performanceimprove movements you can get afterEarly Child tuning the model the measureis Win rate so basically the if somebodywould assess now two answers to the sameprompt one generated by a base and tunedmodel and one generated by the modeltuned with r lhf one answer wins yeah soas you can see the win rate for thoserlhive tune models increasesdramatically overnon-tuned models and in fact in thepaper the quad with the small rlh tunemodel1.3 billion parameters perform better onthe stairs than a 475 billion gpt3 modelso there's definitely Improvement insome cases now not everything is perfectbecause there are also some side effectsof rlhift tunings I don't have time totalk about it but there's always alittle bit of a win-lose situation aswhen you when you perform both tuningsand there's a chart the upper chart aresome of our experiments with differentmodels so as you can see we see verysimilar proportionately very similarimprovements in the performance of themodels for different types of models yesdifferent sizes of T5 models and ourtext bison model which is kind of aflagship model in our APIokay so how is it implemented similarlyto the supervised fine-tuningservice it is implemented as a Vertexpipeline now you see the vertex Pipelineon a chart it's relatively complicatedso I'm going to drill down to specificparts of a pipeline in the followingslides but this is the pipeline whichimplements the full rrhf workflow as Idescribed a few minutes agoand how do you access it again yes it'sa pipeline which by the way we opensourced as of this week you can look atthis pipeline not kubeflow pipelinesGitHub repo here we have implementationof this Pipeline and we also includedthis pipeline in a Google Cloudcomponents SDKso you can create the pipeline byimporting rlhs pipeline from from thispackage and then following the standardprocess with coupon pipelines youcompile the pipeline into theintermediate representations which willbe then processed by vertex pipelinesand you submit a pipeline job and thepipeline job will have a set ofparameters there is quite a few of themI only listed a few examples first ofall obviously you specify location ofyour data sets there are parameterswhich control how many steps you willexecute for tuning for creating trainingvery word model for tuning your basemodel and there you go the lastparameter is this coefficient KLcoefficient this is the coefficientwhich controls the span out here whichcontrols how far the tuned model willdeviate from the base model yes and youknow the lower the valueof the lower the penalty if you set thevalue to zero it will notit will not force any kind of alignmentwith the base modelokay so let's drill down to a fewaspects of this pipeline these are theformats of a data set for the preferencedata set as I described both data setsby the way are Json L files thepreference data set will have a fieldcalled input text this is your prompt itwill have twopossible outputs so possible Generationsfrom the model for office promptcandidate 0 candidate one and it has afield Choice which specifies userpreference in this case but candidate 0was preferred by the human laborer thesecond data set this is the data setwhich is used in our reinforcementlearning Loop to tune the model fromdata set is very simple it's basically aset of prompts now it is important thatbothprompts in the preference data sets andin your prompt data sets they shouldcome from the same distributions butthey should not be the same because youknow it's standard machine learning yesyou don't you want to avoid Targetleaking and all those other effects andyou have to make sure that they aresampled for the same distribution so youhave a meaningful training workflownow when you drill down to the pipelinethis is kind of the tuning the trainingpart of the pipeline so the key steps isthis reward model trainer this is thestep which encapsulates the code whichtrains very old model and then thereinforcers step this is thereinforcement learning step which finetunes the base model there is a bunch ofhelper components this result machinespec component is kind of importantsimilarly to supervised fine-tuning wecan tune the models using gpus or usingtpus and it kind of depends on a regionin which you want to tune so if you wantto tune on gpus we will use eight a100gpus model with 80 gigabytes of devicememory and if you tune on tpus we'll use32 slice dpov3 yes of a TPL slice V3 TPUslides with 32 cores this is the minimumrequirement to run by pipelineso after the model is tuned the modelsare storedin Google Cloud Storage in your Thailandprojects and then there is a subgraphwhich deploys the model to vertexprediction and so this is a standarddeployment process we register the modelin the vertex model registry and then wedeploy the model to vertex onlineprediction endpoint and at the end hereyou will have a model deployed to vertexprediction and you can now use Bethunemodel exactly the same way you would useany other model in vertex using exactlythe same API where you would use toaccess models like text bison or versionaverage generative AI models which areexposed no difference nothing specificto that and there is another part of apipeline patch in France so so what thisstep in the pipeline does we if youprovided a test data set we will run thebatch influence of this test data setand generate responses so you can lateruse some form of evaluation if you wantto compare those responses to othermodels we do have other pipelinesevaluation pipelines which can help youwith this task if you want to do the winrate analysis if you want to use againhumans to validate your responses or ifyou want to use some kind of automaticevaluations like when snx pipeline whichis covered in some other talks whichuses an oracle model to make thosedecisions so the bottom line is a prettycomprehensive easy very easy to usepipeline which implements quite complexworkflow and quite complex algorithmsand that's it for my part of apresentation and now I will inviteHussein to the podium to talk aboutreally interesting stuff yes how it isall applied in practice[Applause]about nine years ago I had a similar butdisappointing experience as a graduatestudent I hosted the workshop at 8am atan academic conference about contextualfine tuning of models and almost no oneshowed upuh but look at you you know it's greatto have you here at 8 A.M so maybe a biground of applause for you the audiencefor showing up at 8am[Applause]my name is Hussein random I'm founderand CEO of flybitz headquartered inToronto I'm also a professor at MITmedia lab in Cambridge and representingthe work of my team today that they areworking on some unique capabilitiesallowing large language models to beused in very specific granular privacypreserved settings when you don't wantyour data to to leave your premises andyou want to contain their integrity andand privacyso umlet me just go to this one languagemodels are great we can do so many coolthings with them from answeringquestions through translating toreasoning to code completion and you'rejust getting better and better andbetter as we are having more parametersin these modelsbut we may agree that they lack a fewthingsit's sometimes difficult to apply themin a very specific domain the morespecific they become you require moretuning capabilitiesand also they lack context rightumthe big thing is do you understand theintent of those prompts and questionswhy are you asking that question who areyou asking that question as are you astudent are you a an engineer what isyour intent so those are some of thethings that our team members are workingon to figure out if we can captureintent when you are creating theseprompts and also they are not very goodwith multi-hub reasoning like when youhave sequences of logic that will needto come together and infer languagemodels are not good with thosefew examples fly bits does a lot of workin the financial space you know forthose of you familiar you know bankshave a lot of data distributed and siledin multiple repositories they havedifferent semantics different ontologiesso if you have a pre-trained languagemodel and someone wants to understandthe relationship of home financingversus the risk guidelines just imaginethe types of graphs and nodes you needto put together to really personalizethat answerthe moment yes we can use veryfine-tuned language models but they arenot very scalable and they're veryexpensive to maintain and and scalethink about other examples like twoindividuals one is student one aneurologist not that neurologists make alot of money but my team came up withthis example are more or less asking aquestion about a trip to Greece butthink about the context of that questionwhat one may have budgetary confinementsthe other one is actually looking for aplan so although the prompt and the thequestion is the same there's a lot ofcontext in that question that you needto address and how do you really bringthat embedded personalization into theseprompts so that's another thing that ourteam is is working onand you know most of you are familiarwith this language models are very badwhen it comes to multi-hub reasoningright the classic example in theliterature a farmer has a certain numberof chickens and rabbits on this Farm thetotal number of heads is 20 and thetotal number of Lex is 58 how manychickens and how many rabbits does thefarmer haveyou are super smart people you cananswer that question relatively quicklylanguage models can't the reason is wellthere are lots of unknownsum they need to formulate the contextthey really need to figure out theequations and then they need to explainthe output so explainability and auditalso becomes an issue and there's a lotof literature these days about how theyare addressing these issuesso uh our team is working very closelywith Google these are you know summariesof how we are using tuning most ofyou're familiar with this usually I candivide divide them into two key areasone is in context learning in which thecommonality in them is that you reallykeep your model Frozen you don't touchyourum you know language model and then insome cases you may provide some examplesand you may get an array as an outputthat's the first one sometimes if youneed a vector as an output in embeddingsyou can still keep your model Frozen getan embedding and use it for retrainingor or training it with other models thisis usually useful when you know tokensizes are important for you you have youknow confined questions and answers andthat's usually how you use one of thesetechniques and Google as the Ericmentioned they have some very goodmodels with with text bison and textembedding with that the other approachis when you have a lot of data andusually that later data is labeled tokensize is no longer that much of an issueso you actually make your model tunableI so it's not frozen anymore it canbecome sentient and it can change and itcan become dynamic doesn't mean any ofanyone is better than the other one butusually you can address some of therequirements in terms of which one touse based on which context so one of thecore examples I share with you we do alot of work in the financial space a lotof credit card companies so we have theontology and the semantics of the creditcard businessour prompt Engineers provide about 20examples more or less for a particularworkflow we fine-tune the the dlm andthen the expectation is that a newrequest will generate a new response inform of natural language or even if youwant in form of a script so this is anexample if you just let me tell you alittle bit about what what flybits doeswe are very good in generatingcontextual experiences for our customersin the old days we also had a thinkabout a visual Journey Builder put thestructure in place put the entities inplace build their workflows now a lot ofthat is getting replaced by by languagemodels so instead of expecting amarketer or a designer to go and buildthis from scratch they can just type orthey can just talk even using audio andthen the experience will be created forthem so in this caseThe Prompt is you know someone's is Iwant to create an experience that canreduce fraud I want to make sure that ITarget transactions more than aparticular dollar value and then youhave elements mapped that understand therequest type request objective requestRule and then the action so just byabout 20 examples you will be able togenerate a script and then that scriptcan be used for you know any type ofworkflow that that you want this is aclassic example that we use some ofthese language models so the pros andcons you know they're relatively easy touse they're becoming easier and easierand easier they have data efficiencythey have versatility and they have fasttraining but what are the negativesvery very prone to biases right you needto really find a balance between howgeneralized or how fine-tuned you wantthe model to be if you generalize toomuch you may have ambiguity if youreally fine tune it then you really maymiss that generalization capability sothose are all the things that you knowyou can address in yourprompt engineering exercisesuh the other thing that I would like toshow you is a capability we createdcalled Nestor anyone here familiar withthe Tintin comics the comic bookit was more popular in Europe there wasa character in that book called Nestorwho was the concierge for Captainhaddock and but he was very good at itwas that before the captain asked forsomething he was he had it ready and hehe delivered it so that's why we calledthis uh Nester and the premises of thatare two things one is toI really use a neurosymbolic approachesto reason using graphs so the the ideais the input comes in as naturallanguage we have a fine-tuned palm modeland then using reasoners and usingsymbolic representations we have theability to go and consult a large datasets that are being formed in form ofgraphs and then you can query the graphreturn it as a symbolic representationor script and then you can translatethat intointo a natural language again so theexample of that isa marketer an experienced designer comeson an interface and say I would like tobuild an experience that provides 15bonus to my loyalty program providers inCanada right that's that's cool you knowthat that I will benefit from that sowhat then happens is that you will useyour tune model you will create asymbolic representation you reasonthrough querying your knowledge graphsbut the problem with this example if youconsult your location model is that youhave a lot of ambiguity that the tunemodel understand so it comes back againwith a prompt asking for moreinformationtell it asks the user tell a little bitmore about who do you need to address inCanada people in Canada outside Canadapeople who travel and just going throughthat elicitation and feedback you willgetresponses that are more personalized andmore relevant to uh to what you want todothe last thing I want to highlight whichis a big strategy for us is you knowprompt engineering is is amazing it'sbecoming more and more advanced butwouldn't it be great if you can haverecommendation on auto prompts what arethe good questions you can ask from yourdata that's where we spend a lot of ourtime you know it's what is the rightquestion to ask and that also goes backto uh to to some good philosophy so ourresearchers working on knowledge graphsto really combineum semantic networks and semantic graphswith AI models graph and combine it sothat we can actually you get reasonableor Reason ready graphs for for queryingso the the process without going throughthe details is you have these thinkabout a knowledge graph that representsI don't know some sort of an ontologyBanks or Healthcare or even moregranular like credit cards or or loyaltyyou use some sort of a pre-trained graphneural network to really understand it alittle bit better and add to it if itcan then you also have an AI model modelgraph an AI model graph you know theyour your semantic graph could be justnodes and relationships your AI modelgraph can also have weights and and andrelationship and function so for exampleyou may have a model graph that has aBayesian node for calculating risk andit may have a causal inference node andit may have a fuzzy logic node and thenit really understand in what situationin which context which node to use forthat type of inference when you bringthese two together kind of the semanticgraph that you have and the AI graphthen you're going to have a reason readygraph that is going to be very powerfulwhen you uh when you query it so firststep is have an AI model graph thenumber of them available your datascientists can build some then you use apre-trained GNN against your ontologiesand semantics you basically combine themin some sort of unified graph and youmake your graph reason readyso that's becoming this is one of theapproaches that we are using uh forgenerating Auto prompts that's a veryinteresting field for us like tell meabout your data tell me about your graphbut also tell me what are the 10 bestquestions that I can ask from it and ifyou wear a contextual lens on it as wellthat becomes more interested I aminterested to ask questions about mydata in a risk situation I'm interestedto ask questions about this data when Iwant to cross-sell an upsell to mycustomers in this location so now you'regetting to a model that of course youwill need prompt Engineers but you canreally accelerate your workflow whenyou're building such systems so tosummarizeum I can really categorize it as firstand foremost there is a human elementhere it was great to hear from ourcolleagues that you know there arehuman-centered approaches for this and Iwould say at least our experience isthat your even your your fine-tuningteams your your ml teams are veryinterdisciplinary if you just have ateam that are all AI scientists youreally miss the context of the types ofknowledge graphs that you got to createand the context so go back to the oldsaying that build teams that you canfeed them with a large pizza you knowand really you know have complementaryskill sets especially for thosequestions and knowledge graphs to becreated uh effectivelyum I think where where things are goingis that it will be a fascinating fieldto see how llms are becoming morepersonalized like really understandintent and also understand who's askingand why they are asking I think uhdisability of building reasoners totranslate between objects and scriptsand natural language and learn from thatis also becoming a Super interestingareaandum yeah just you know just we all knowllms are great they continue to getbetter there's one thing to keep addingparameters but there's also anotherthing to build ecologies and ecosystemsaround your models so you can reallycontextualize and essentially build a aninference router that you may have tonsof models and it can know how to combinethem and if you come and if you have auser interface allowing you to do thateffectively uh I think that will begreat and the last thing I say is thatwe are all fascinated by AI but I thinkyour data strategy is also superimportant I I was impressed yesterdayduring the keynote to hear about I thinkthey were called extensions that you cankeep your data in your premises notmoving it into a centralized repositoryI mean you used to have T's trustedexecution environments and all that Ithink these are advancements of thatthat you can now have a price privacypreserved local environment to benefitfrom L Adams but then the concerns oflegal and data portability datatokenization will become much mucheasier to address so if that strategywhich is data decentralizationexplainability auditability portabilitygoes hand in hand with some of thethings that you know I shared with youthat our team are working on I think youwill really find scalable Frameworks foruh for AI strategiesum I think that's all I had to sharewith you and with that I'll pass it backto Nikita and yeah happy to take youaway thank you[Applause]"
}