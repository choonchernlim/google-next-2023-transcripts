{
    "title": "Powering the next wave of AI applications with Google Cloud and NVIDIA",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML116"
    ],
    "video_id": "MehopWLHX2U",
    "time": "Aug 30 03:00 PM - 03:45 PM CDT",
    "transcript": "foreign[Music]good afternoon and Welcome to oursession powering the next wave of AIapplications with Google cloud andNvidiamy name is Hannah Youssef and I'm agroup product manager on vertex Ai andI'm joined by JohnnyConway so I help with product managementNvidia for some of our deep learningsoftwareawesomewe're very excited to be talking withyou today about a very critical topicwhich is infrastructure and how can 3Dscale to meet all of the challenges thatwe're seeing with the growth in Aiworkloads and specifically in generativeAI workloads we've heard from all of youabout the amount of scale that areneeded with these new capabilitiescoming to Market all of us see howyou're challenged with moneyand today we'll try to answer some ofthese questions and show you how Googlecloud and Nvidia are working together tobring you solution to these verycritical and demanding problems thatyou're going throughwe'll start by talking about some of theAIML challenges then we're going to moveon to give you an overview of AI andGoogle Cloud some of the best practicesand announcements that we're sharingduring this eventand then Joey will move on to cover someof the Gen AI capabilities that Nvidiais bringing to Market and how that ishelping you power many of these newapplicationslet's start by looking at the challengeswe go through and hear about from ourcustomers every dayour customers run many differentworkloads on Google Cloud whether theyare traditional predictive AI workloadsor newer generative Ai workloads andacross the board we hear every day aboutsome of the challenges you go throughthese can include having an integratedend-to-end ml platform that enable youto have one pane of glass and seeeverything and operate everything fromdata to training and prediction it alsoincludes the concerns around the cost oftraining specifically and the cost ofserving and how utilization of yourinfrastructure and cost reduction isbecoming really critical as your growyour usage of AI within yourorganizationswhen it comes to training and servingcosts we also hear a lot from ourcustomers about the fact thatinfrastructure can sometimes beunderutilized and that itself is one ofthe challenges that you're trying to gothroughwith the growth and generative AI manycustomers are looking to use first partyor third-party models and while thatmight abstract some of theinfrastructure challengeswe still hear a lot of requirementsabout global availability about speedabout latency and also a lot of problemsthat modern providers are now having todeal with as the scale of the workloadsand the training they need to do becomeseven more intensive and even biggerif we take a deeper look at that we willsee that there are challenges across thedevelopment the training and the servinglife cycle whether we're talking aboutthe cost and the time to set up adevelopment environment and get startedand onboarded or whether we're talkingabout your ability to use your favoriteFrameworksor just some challenges around stabilityand reliability of the environments wehear our customers tell us about howsometimes experimentation can be tooslow for what they are looking for andwhen they move to training we all knowabout how sometimes it can take a longtime to run training jobs how sometimesthat can lead to that being veryexpensive for your organization but alsosome of the challenges introduced bydistributed training which can be hardto set up to scale and to manageyeahscaling to hundreds of training jobs andwasted time and resources in cases offailures and retries are among the otherchallenges that we're trying to talkthrough todaywith serving latency is really criticalso dealing with high latency and howyour infrastructure and your cloudprovider and environment can help youwith that is one of the things we'll becovering and also again the cost ofinfrastructure and the how to minimizeunderutilized infrastructureso let's take a look at how Google cloudand Nvidia are working together to helpyou deal with these challenges andprovide you with scalable performanceand TCO optimized infrastructureGoogle cloud is trying across the boardto offer integrated AI software withelastic infrastructure that we deliverthrough a vast portfolio of AIcapabilities that you've been hearingabout through today and yesterday thatinclude Cloud native instances our deeplearning virtual machines and containersml accelerators as well as managedplatforms like vertex AI for examplethat try to simplify the process ofrunning ML workloads on Google cloud andI think that there have been manyreports with analysts like Forsterhighlighting Google's leadership inthese dimensions and it really Builds onour vision to have a unified data and AIsolution that provides you witheverything you need for the end-to-endRun training and deployment of your mlinfrastructurenow let's talk about some of thepractical ways that you will see this onGoogle Cloudthese are four key components that we'reusing maybe to summarize the differentareas we're focused on and investing instarting with Hardwarewhether you're looking to run yourworkloads on CPUs on accelerators andgpus specificallyum and as you've heard yesterday in thekeynote having the latest generationgpus is really a critical area of focusfor us and this is because it's onegreat way to improve your priceperformance over time and make sure thatyou can get like you can get the latestand greatest capabilities acrosstraining prediction and other types ofworkloadswhen it comes to software we really knowthat a lot of you are looking to use thebest of the open software X systemsupporting ML and a lot of our focus ison providing you with these toolsFrameworks and apis on Google cloud inan easy to use way that doesn't requireyou to do a lot of setup andconfigurationSpeed and Performance is another keypillar and over there our focus isreally on ensuring that our platformsand tools are fast to set up provide youwith the tools needed to automateorchestration like our pipelines forexample and lower serving latency andwe'll talk about some of these in moredetails but just a couple of examplesthat I wanted to mention over here isthe capabilities that vertex AI bringsyou specifically for things likereduction server on training and we'llgive a few examplesthe last pillar of this is really newinnovations that enable you to do thingsin a much simpler way so you can thinkabout automl the capability to run manyof your ml workloads without having toworry about infrastructure altogetherwithout having to choose and configurethings yourself another example is Alphafold or for example nvidia's Merlin andTriton the Joey will cover todaynow let's have a deeper look at vertexAi and the power of Google managed mlplatform to solve many of the challengeswe just talked aboutfor those of you who are not familiarwith the platform vertex AI is GoogleCloud managed end-to-end ml platformthat helps data science teams acrossbusinesses Fast Track ml modeldevelopment and deployment it includeseverything from feature engineering tomodel training and inferenceand it also comes with Enterprise classgovernance and monitoring so that youdon't have to worry about any of thoseprerequisites that your organizationwill always be worried aboutour ml platform focuses on trainingdeployment and managing custom ml modelsum and provides faster integrated datato AI environment but also enables youto ship your models faster withend-to-end ML Ops as well as scaleefficiently with open and flexible AIinfrastructureon top of the traditional capabilitiesthat we've provided for years we'rehappy to be adding many new capabilitiesthat support gen AI use cases on top ofvertex AI some of the examples you'veheard about yesterday include modelgarden and generative AI studio and apismodern Garden is really about enablingyou to jump start your ml production andyour ml workloads by having one placewhere you can discover the models toolthem and run them while generative AIStudio enables you to Fast Track thedevelopment of large language models andgenerative AI applications by using outof the box models with little code or mlexpertisenow let's talk about some of the mostcompute intensive components of theplatform and of your workloads and I'llstart with trainingforeignvertex training offers a platformoptimized for ML model training withfaster performance and access to thelatest Google AI hardware and softwareincluding gpus to simplify your trainingexperience vertex AI provides a varietyof tools and features it also includeautoml where you can train your ml modelwithout tracking any code or preparingdata splits they include custom Trainingwhere you can providewhere we provide a variety of briefbuilt containers and custom Trainingoptions for popular ml Frameworksas well as hyper parameter tuning toeasily tune the hyper parameters of yourml modelssome of the other capabilities andbenefits of vertex AI training is thatyou can have a serverless experiencewhere you don't need to worry aboutmanaging the infrastructure or turningit off after you're done with the job wedo that on your behalf we provideumclusters on demand and we provideautomated job queuing where you don'thave to worry about the retries andagain we take a lot of that hassle offyour back and manage it through theplatform and there are some exampleshere about how that benefited some ofthe customers that have used theplatform improving their TCO improvingtheir speed and simplifying theiroperationsI mentioned reduction server as anexample of some of the Innovation thatwe've been trying to bring to youthrough vertex AIand reduction server really is aboutcatering to the problems where modelsbecome more complex and the size of yourtraining data sets keeps increasing andthus training time becomes one of thekey bottlenecks in the development anddeployment of ml systemsto speed up the training of large modelsengineering teams are adoptingdistributed training using scale outclustershowever that brings another problemwhich is the networking problem and howto deal with that and reduction serverspecifically is a vertex AI feature thatoptimizes bandwidth and latency ofmulti-node distributed training onNvidia gpus for synchronous dataparallel algorithms by optimizing thatwe can help you decrease both the timeand the cost of large training jobsand in the example we have over here youcan see some of our examples showing 42reduction in cost and speed up evenwithout having to do any additionaleffort on work and there are moredetails and examples available on ourwebsite and blogs that talk about how touse reduction serverthe second example I'll talk about isvertex AI predictionand with vertex AI prediction our Focusreally is enabling you to optimizeeither for latency or for cost dependingon the kind of workloads you have and wedo that by offering both online andbatch prediction where batch predictioncan help you lower the cost ofdoing a prediction on a large amount ofdata in a batch model while onlineprediction is optimized for your morelatency sensitive workloadswith high performance low cost anddifferent options vertex prediction alsoenables you to benefit from many of theother benefits available on vertex AIincluding security compliance andintegration with the rest of theportfolioso let's summarize our talk abouttraining and prediction and some of thebenefits that they provide that can helpyou deal with some of the mlinfrastructure challengeswith vertex training our focus is onhelping youhave more complex and accurate AI modelsand we do that by providing you with afully managed service for running yourtraining jobsthat is supported by queue management tosimplify some of the operationaloverhead but also provides a wide rangeof infrastructure capabilities acrossstorage CPUs and gpusit is framework agonistic enabling youto use many of our pre-built containersor bring your own and it is supportedwith the latest Nvidia gpus includinga100s available on the platform ingeneral availability with prediction ourfocus is enabling you to serve yourmodels in the way that you wantoptimizing for cost or latency as peryour requirements and that comes withthe ability to serve online endpointsfor low latency prediction or predictionor on massive amounts of batch datait scales automatically based on yourtraffic so that you don't have to worryabout underutilized infrastructure andbrings you fast inference on gpusand it also supports the latestinference optimized gpus from NvidialforceI'll move to a few best practices andupdates about what's coming next andthen that will conclude this part of ourpresentationso over here we just try to summarizesome of the best practices that werecommend you look at in addition to thebenefits that the platform provideswhile these vertex components helpalleviate many of the infrastructureinfrastructure challenges there are alsosteps you can take on your site thereare many recommendations available onour website and and in our docs but I'llcall out just a few choosing the rightGPU and VM size for your training andprediction workloads is one of thethings that we always recommend youspend time onimproving the performance of your GPUworkloads by looking at higher Networkbandwidth options and leveragingreduction server where appropriate couldadd a lot to your ability to optimizeyour cost and optimize the time it takesto trainfor training we always recommend thatfor smaller models you use single workerinstancesand for large data sets use distributedtraining and of course all of that comeswith using the right VM Android GPU forme for your model for prediction themain point I would call out is justbeing able to use both our online andbatch prediction according yourrequirements so that you can optimizefor the priority for that specificworkloadin terms of what's new we've actuallymade many new announcements yesterdayand today but I'll just call a few weare expanding vertex ai's open sourcesupport with the announcement of Rey onvertex that you might have heard aboutyesterday and there's a more in-depthsession happening about that today aswellrayon vertex provides managedenterprise-grade security and increasedproductivity improved cost andoperational efficiency and integrateswith all of the other vertex AIcapabilities like colab Enterprise forexample training prediction and ml Opscollab Enterprise is another vertexproducts that we announced yesterday aswell and again there is a follow-upsession later today if you're interestedto know more but it offers you a managedservice that combines ease of use ofGoogle's collab notebooks withEnterprise level security and compliancesupportour data scientists can use collabEnterprise to collaborate acceleratetheir their AI workflows whileintegrated with all of our othercapabilities and both these products nowsupport the latest Nvidia gpusand the final announcement is that evenfor our training and predictioncapabilities we are bringing you thelatest hardware and we're happy toannounce that in addition to G2 VMS thatenable you to use l4s gpus forprediction we're now also enabling youto use the newly announcedh100 gpus on gcp a3bm specifically withboth training and prediction in privatepreviewum I'll hand it over to Joey Conway toshare more about nvidia's Innovationsall right thank youso I'm going to give a quick overview ofgenerative ai go through some exampleuse cases and then I'm going to talkabout some of the software we haveavailablefor generative AIthe excitement here is thata lot of the new workloads we'vediscovered have been things where we cantake neural networks and a lot ofexisting data oftentimes unlabeled dataand the neural networks are able to findpatterns among the data to then producenew contentwe think about this generally on theleft hand side here many different typesof data so this could vary from saymusic videos textcould be three-dimensional modelsanimations and then in the middle wehave neural networks that are able tolearn patterns and on the right they'reable to produce new types of contentsome of this new content could be in theform of text could be videos could beimages could be 3D models or animationsand so a lot of the excitement here isthat previously it often took a lot oflabeled data and a lot of uh specificefforts to be able to create content andwith a lot of these breakthroughs I'llgo through more detail we're able to dothis today at a much larger scale and beable to allow many more people access tothis type of contentthe excitement here is that theseworkloads are very universallyapplicable and we've seen from the lefthand side an example here in chat Botsoften times in the past theseinteractions were very scripted and verylimited or constrainedwith generative AI models we're now ableto allow neural networks to create veryconversational interactive content thatappear much more engaging and generallyuseful as well as being much morefactualin the middle some examples ofgenerative art where we've seen in thepast that deep learning models have notbeen as strong in being able to createnew forms of contentwith many of the breakthroughs herethese large models have such a generalCorpus of information and examplesthey're able to create new types ofcontent this can be both from images aswell as videos or animationsand then on the right hand side you cansee there are many applications workingon integrating these Technologies andbringing them to many differentaudiences and many different contentareas and languages and domainsspecifically some examples we've seenespecially on the Enterprise side havebeen from left to right in things liketext generation and so a common modelarchitecture there would be GPT there'sother variations things like T5 as wellthat many people have started to gainexperience with and now been using todeploy out into Services where they canhandle different scenarios of cases likesummarization or it could even be storycontent creation marketing contentcreation email generationand then another example here is inTranslation we've seen the ability forthese large models to take large Corpusof unlabeled data between differentlanguages and then be able to translatecontent back and forth between many manylanguages whereas previously in the pastthis often took verygenerally very granularly labeled datathat often took a lot of time to gatherand createand in the middle here we have codinggenerally speaking we think of it as atext use case but it's a very uniquedomain where there can be many differentprogramming languages and often what wesee are the use cases where people areinterested in being able to write codebut not necessarily need to know all thenuances of a programming language and sooften being able to send in a a requestsomething like create a function thatcompletes a few tasks these models arethen able to generate code Snippets thatcan run live in systems and can coverdifferent programming languages fromvariety of things like python or C orJava as well as other languages thatmight be more domain specificin terms of visual contentwe have a model architecture listed herefrom Nvidia called edify there's manypublic models as well where we've seenthe ability of being able to take alarge Corpus of images as well as textbeing able to describe those images andthe model is able to then pair both ofthese workloads together so they're ableto take a Text corpus of Generallanguage understanding and then take animage Corpus and pairing these twotogether people are able to use text toprompt or make a request and ask forthings like create an image of a dog whois eating lunch with in a red room witha window in the background and themodels are then able to learn from andcreate a new image and many of thesevariations that meet the request in thepromptthe last one here we have in lifesciences is an area at Nvidia we've seena bit of interest and spent quite a bitof time focused on often things likedrug Discovery and being able tounderstand proteins and be able to makepredictions we've seen very similartechnology to the text-based models beable to deliver compelling results aswell on this domaina quick overview of what Nvidia offersand then I'll go through some veryspecific software Stacks so from thebottom Nvidia is well known forproviding gpus we have more recentlyintroduced arm-based CPUs called Gracewe've also paired those up with gpuscalled Grace Hopper and then on thebottom right we have dpu which isanother type of platform used often innetwork deployments where we canaccelerate some of common Networkworkloadsabove that we have referencearchitectures we have dgx from Nvidiaand then we have many different waysthat people can consume these platformsthey could be through a traditional OEMproviders they can be throughworkstations they could be throughdifferent Cloud platforms as well andI'll get to more of that in some of thelater slidesin the middle section we have a lot ofwhat we consider libraries or keycomponents in our software stack thereare common components in here that dealwith a lot of math acceleration on gpussome of these I'll touch on like Tritonwhich will be coming up shortlyand then the layer above we have threemain domains we think of the software soone of them is on the left hand sideNvidia HPC where we've had a longpresence in a lot of the super computesuper Computing as well as many of theuh domains of research and we have somelibraries I'll mention on those we haveNvidia AI where I'll spend most of thetime today and then we also have NvidiaOmniverse which for uh is another placewhere we accelerate a lot of therendering and interactions especially in3D worlds and and Concepts like digitaltwins where you can recreate eitherEnterprise or manufacturing type ofenvironments and be able to runsimulations thereacross the top we have a wide variety ofsoftware available some of the softwarehere can help with things like weathersimulation in some of the the morehistorical HPC domains and then we alsohave software in there around physicssimulationsand then in Nvidia AI where I'll spendmore time we have software in here fortext-to-speech and speech to text forrecommendation systems and for largelanguage models we also have software inthere for things like creating digitalavatars where we can now move intothings like an Omniverse where we can dorenderings and simulations for sayvehicles or robotic use casesand then on the following slides I'll gointo more detail on Nvidia AI so morespecifically I'm going to cover Tritonour open source inference server I'lltalk about Riva which we use for speechto text and text-to-speech and then I'lltalk about Merlin which is ourpersonalization systemso for triton our open source inferenceserver that's fast scalable andsimplifies inferencing deploymentsthere's five main categories we thinkabout here going from left to rightOne Challenge we've seen is thatoftentimes deep learning or even machinelearning workloads can be performed indifferent Frameworks or differentarchitectures and so there's a challengeof being able to deploy say a tensorflowor a pytorch or a Jax model or even somemore traditional machine learningworkloads likexgboost or scikit-learn and what wewanted to ensure is that there was onesoftware stack and specifically oneinference server that could support allof these workloads no matter where theycame from they could be differentresearch teams they could be differentcompanies they could be different opensource repositories and so inside Tritonwe have multiple different inferenceengines we have many of the communityones I've mentioned we have one fromNvidia called tensor RT that is what weview as the best for GPU acceleration wealso have others from other companieslike Microsoft contributes Onyx runtimewe have openvino from Intel as well andso there's a wide variety of Frameworksthat are accelerated inside Triton toensure it's simple and easy to have onesoftware stack that can deploy any ofthese workloadswe also support a variety of query typeswith different model architectures thatcome different use cases there's avariety here of being able to do thingsoffline like a batch workload where sayyou might have many medical images say acardiologist needs to get throughhundreds of different lung X-ray imagesand they need to run inference in allthese workloads and so we can run thatas a batch workload on the real-timeside we also have support for thingslike streaming which could be in theform of audio streaming you could havechunks of audio streaming in you couldalso be streaming chunks of audio backout say for a text-to-speech type ofworkload on the tech side we also havestreaming support often times where youhave longer use cases where say you needto generate a story or an article we canstream that text back out so the userdoesn't have to wait for everything tocome in a large batchin the middle we have a support for avariety of different platforms Tritonruns on CPU x86 and arm it also runs onGPU as welland then it runs on different operatingsystems as well as Linux and windows aswell as in virtualized environmentsin the fourth area for devops or ml Opsready we realize that in thesedeployments oftentimes data scientistsand deep learning teams have to workwith deployment or production teams andso we've we've worked in Triton to makesure that we provide things like metricsaround usage so the metrics can be permodel they could be per workload andthey can be very granular they can alsobe exported and summarized up in othertools things like Prometheus or grafanawhere they can be renderedand then on the right and as well asthings like kubernetes support as welland then the right hand side we spend alot of time ensuring that theseworkloads are optimized so we get greatperformance the motivation there isensure things like being able tomaximize the cost of ownership minimizethe latency and maximize the throughputfor these workloads so when they aredeployed at scale it's not just onemodel it could be many models and manyworkloadsand then most importantly across thebottom we have support for triton acrossa variety of areas inside gcp andspecifically we've had vertexintegration for a year or two now andit's a great place to be able to pick upand get all the advantages of runningthe Triton software in your vertexdeploymentnext I'm going to talk about Riva whichis our software stack for speech to textand text-to-speechacross the top we have a workflow herewhere we focus and think about things interms of an end-to-end workflow so onthe left hand side we think about thingswhere data comes in this could apply totraining as well as inference so oftenthere's some set of in a text-to-speechworkload you might have text that youneed to generate synthetic voices forand so you'll be training a model you'llhave some type of audio and some type oftext with that and in training thatmodel we can then go through thisPipeline on the speech to text side alsowe would start with audio as well assome transcript as ground truth and thenwe can train the model in Revo weprovide a a little bit over 10 languagesout of the box and then we also have theability to customize and so in workingthrough this workflow the first area isthis neural networkinside Revo we provide the tooling to beable to improve the accuracy fortranscriptions as well as improve theaccuracy of the synthetic text-to-speechworkload in terms of transcriptionsthese could be things likedomain-specific terminology they couldbe acronyms they could also be certainterms and different dialects of alanguage or they could be completely newlanguages Riva supports all thosedifferent workloads in terms ofcustomizationand then on this text-to-speech side wealso support the ability to createcustom voices and so historically thiswould require a lot of data from aprofessional voice actor those workloadsare supported we also have the abilityto take smaller amounts of data sayhours or 30 40 minutes and be able tocustomize and create your owntext-to-speech modelin the center area we have places wherewe can then organize these workloads andhave a full pipeline so often there'smany components we we think of these asneural networks but there's often preand post processing that's important todeliver both the performance and theaccuracy Reba provides that fullpipeline and on the acceleratedinference side on the right hand side wedo have the full software stack fordeploying these models and being able toensure things like on the transcriptionside we can do hundreds of concurrentstreams across one GPU and the same onthe synthetic voice generation side wecan also ensure we can produce uh usesay one voice or many voices to producemultiple concurrent streams at the sametimeand we make sure those things are verycompute and efficient so they scale upand can handle large workloadswe also have this available in in therow across the bottom here called NvidiaAI Enterprise where we provideEnterprise support for these workloadswhere if there are issues or challengesthat people encounter there's a place inNvidia you can contact and get supportfor these workloads right awaywe do have these things available aswell inside gcp and many of this will becoming to vertex as wellthe the last one I'm going to touch inthis section is about recommendationsystems and then I'll spend a minutetalking about large language modelson the recommendation side we haveNvidia Merlin which is our end-to-endframework to both build optimize anddeploy recommendation systemsthe diagram across the top is often acanonical workload we think of it bothin terms of the training that's requiredto create these models as well as theinference deployment that's used inproduction environmentsfrom the left hand side we think ofthings like retrieval where theobjective is to take potential manycandidates these candidates could bethings like news articles they could bea product catalog they could be ads theycould be any variety of workload whereyou have many options and we need tonarrow those options down for a specificsay scenario of a user or a certain typeof usersin this pipeline retrieval is generallythe first stage we generally try and getdecent accuracy but make sure this is avery quick step and the goal is to takemany examples and narrow them down to afew say tens or hundreds frompotentially hundreds of thousands ofMillionsthe next stage we have some filtering sothere could be business logic in herethings like perhaps there's a sale or apromotion happening in certain region orthere might be other constraints thatyou want to narrow down the optionsfurtherand then after that we have another stepwhere we go through it in a much morefine-grained approach and we'll oftentake an input of say 10 or hundreds ofcandidates and we'll rank them into thetop say three or four candidates and inthis scenario we're also combining sayuser inputs these could be demographicsthey could be information about your endusers and we're essentially narrowingthe potential candidates to the onesthat we think are most likely to workfor these set of customersand in this pipeline we support both thetraining stages so there could be saytwo or three different neural networksused here we also support the deploymentof them so at inference runtime theseuse cases generally are in the very lowmillisecond ranges say 10 20 30 40 maybe100 milliseconds generally becausethere's a pipeline they're part of wherewe need to provide a recommendationquickly back out to the rest of thesystem to then render the content backto the end useron the bottom of the slide we have anexample here of how we've integratedthis into vertex Ai and there arespecific components inside vertex we'vetaken advantage for so we have things inin kind of the middle section from leftto right the feature engineering step isoften a common Step At the beginningwhere some research needs to happen andwe need to think about both the theoptions of what we want to recommend aswell as the potential user segments andso we'll spend time in here preparingthe data set and these could be thingsexperimenting around what type of dataresults in the best type ofrecommendation and we GPU acceleratethis because at Large Scale you couldhave millions of users or hundreds ofmillions of items in a catalog and to beable to run these experiments thequicker you're able to run them thequicker you're able to get to a betteroutcome with better accuracyin the middle we have the software stackand we show in here both using pytorchas well as tensorflow and we have allthose optimized on gpus specifically forrecommendation workloads we we havethose available inside vertex AI as welland then in the last stage we providethe prediction or the inference stageand I'll mention to the inference searalso builds upon Triton I mentionedearlier as well as Riva so we use thesesoftware components throughout oursoftware stack and then across thebottom we do have Integrations if youhave data inside gcp these are some ofthe more common tools we see peopleusing we have ways to integrate so youmight have a large Corpus of data youneed to pull out an extract and runthrough feature engineering that'savailable and those those interactionstoday are set up in vertex AIthe the last section I'm going to touchon here is a software stack we call Nemoand it's focused on large languagemodelsso for large language models theworkload in the center here we think ofit from left to right as an end-to-endCloud native framework to buildcustomize and deploy these generative AIworkloadsand so from the left hand side we thinkof use cases where often people willhave some set of data it could beproprietary or private or important datato your company or your Enterprise andthe goal here is to be able to get themodel familiar so it it understands yourdata and your domain and is able toprovide generative content based on thatand so the first tool we have is what wecall data curation it is a tool wereleased a few months back that allowspeople to be able to take a large Corpuspotentially of unstructured unlabeleddata and be able to do things likededuplication and filtering so you canensure that you have the right blend ofdata exposed to the model it's importantbecause the model learns from what youshare with it and so being able to comeup with the right blend of Whichlanguages which domains say sports ornews or legal content and whatpercentage of that you want to expose tothe modelthe next section we have is fordistributed training so the ability thatonce you prepare the data to be able torun the training it could bepre-training a foundation model it couldalso be customizing an existing one andin the software stack we have tools todo this very efficiently at Large Scaleso you can take a data set and train abrand new Foundation model and get thatdone very quickly you could also take anexisting Foundation model say forexample something like llama2 and beable to customize that we have a varietyof customization techniques they oftendepend on the use case so we have thingslikeuh continuous pre-training where youmight want to extend the knowledge ofthe model there are other things likesay instruction following where you wantto be able to teach it how to followinstructions on a specific set ofcontent there are other tools like uhLaura where you can replace parts of theweights inside the model and be able tocustomize the knowledge and haveflexibility to have many of thosevariations so you can take the power ofa general large language model but beable to customize it for say differentdomains across your company or differentuse cases so you could imagine somethinglike a customer service department mighthave different needs or expectationsthan say a developer team than say amarketing team as welland then we also have recently availablerlhf or reinforcement learning withhuman feedback so the idea there isyou're able to collect data as peopleare exposed to the generative responsesthey're able to give feedback on howwell those responses meet theirexpectations you can do things liketoxicity you can also do things likeverbosity or creativity and capture thatand then be able to further fine-tunethe model based on this human feedbackwe then move from customization todeployment we have inference softwareand a full software stack that builds ontop of Triton where we're able to deploythese models at scale and you canimagine a large language model takes abit of compute it can also take a bit ofnetwork as well as memory speed as welland so depending on the size of themodel these these essential requirementswill change and the software stack webuilt supports both single GPU models aswell as multi-gpu and multi-node modelsas well so the these neural networks canbe very large and we acquire a moreunique software stack to be able toefficiently run them at large scale wedo focus on things like low latency aswell as high throughput there are newertechniques things like in-flightbatching where we're able to thinkthrough these large language models arewhat we call Auto regressive where theygenerate say one token they'll use thattoken to generate the next token and soas they generate a sentence the sentencebuilds upon the previous parts of thesentence this requires some uniqueaspects of how we handle inference andmake sure we can do many of theseconcurrently at large scaleand then the last piece we have here iswhat we call guardrails it's an opensource component we released a fewmonths back it's focused on bothensuring the content going into thelarge language model so these could bequeries they could be chat queries theycould be programming queries butensuring that that content going inmatches expectations soyou can think about things like toxicityfiltering or keeping things on on topicit also helps with content coming out sothe generative models are very capablethe challenge often is making sure thatthey're generating the content in theDom in the domain you need most and soin those scenarios we think about thingslike you you might want to make surethere's accurate accuracy in the contentcoming out so you could do things likefact checking there's also things ofmaking sure it's within the domainyou're expecting say if it's a customerservice chat bot you might not want itgenerating content about sports or newsrather you want to keep it on contentabout your product areaon the next slide here I've just swappedout the bottom area and this is part ofthe announcements we've made the lastday or two around some of the newinfrastructure available and so we dohave all this everything I've mentionedhere being able to run it in gcp on thenew A3 instances for h100 we're veryexcited about this because the newergeneration of the of these gpus in A3has specific compute engine in there forgenerative large language models and sowe're able to see something around a twoto three x speed up on the training andon the inference side of these modelswhich will help us be able to scale toget higher accuracy with larger modelsand ensure we can maintain some of theslas and expectationsthe the last slide I have here is onsome of the more specific componentsinside Nemo framework there's a lot ofdetail and things available this isprovided open source we do have this upon GitHub we do also offer NvidiaEnterprise support where you can you canattain a container where we package upthe software stack we go through thetesting and releases and make thatavailable and ensure a good experienceboth in terms of accuracy of models aswell as Benchmark and performancesome of the components I've mentioned inhere we have things like the datacuration we have accelerated trainingthere's different techniques likesequence parallelism pipeline tensorparallelism we also have a newertechnique around sequential activationand then in the middle we have some ofthe different customizationsI mentioned things like supervisedfine-tuning we also have spent some timeon things with information retrieval sonvidia's spent a bit of time onaccelerating Vector search databases andthen being able to connect those up tolarge language models so if you have aknowledge base or a corpus of contentbeing able to quickly retrieve that andthen augment the generative capabilitiesof the large language model with thatcontentand then on the last we have deployingthese models at Large Scale depending onthe models we've spent time both on theGPU optimizations as well as across GPUand across multi-node so there are thereare actually communication Networkbandwidth optimizations in here as wellto ensure these large models are veryefficientforeign"
}