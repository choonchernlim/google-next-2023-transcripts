{
    "title": "Scaling Memorystore to new heights with sub-millisecond latency",
    "presentation_type": "Breakout",
    "categories": [
        "Database Professionals",
        "DBS204"
    ],
    "video_id": "J78PFUHxBr4",
    "time": "Aug 31 12:15 PM - 01:00 PM CDT",
    "transcript": "foreign[Music]just to get started here my name is Kylemeggs and I'm one of the productmanagers for cloud memory storewith me today is Matt geerling anotherproduct manager for Google Cloud memorystore and joining us in a few minutes isanimesh from Palo Alto networks who's adistinguished engineerso let's start by uh talking about theagenda we're going to talk through todayso first we'll start with the memorystore Service as a whole we'll go oversome overviews service that exists todaythen we're going to talk about our brandnew redis cluster launch highlightedthat in green because we're superexcited about that we had a launch bloggo out yesterday that you might haveseenthen animesh is going to talk about PaloAlto Network's use of memory store intheir cortex data Lake application whichis a huge scale security data processingapp we're talking trillions of eventsper day at petabyte scale and then we'llgo into closingyou may have seen this slide before ifyou've attended other database sessionsbut this is a overview of our portfolioon the left there you see memory storein Blue this is a managed service forredis and memcacheum customers love memory store for threemain reasons one is that it is a managedservice so you don't have to be a redisexpert you don't have to be a memcacheexpert you don't have to worry about ismy node is my replica in the same Zoneas my primary we handle all of that foryou so it makes a lot of things simplerthe second is that it has predictablelow latency so you can rely on that lowlatency building your applicationspotentially downsize the computerequired for your database behind memorystoreum and the third is that when you dothat you can actually save money sowe're going to talk to anamesh about howPalo Alto networks uses memory store todownsize their Cloud SQL and actuallysaves money overall and improves thespeed of their application which ispretty greatmemory store provides microsecondslatency and is already used by over 90percent of the top 100 Google CloudcustomersI have a couple example use cases hereon the right uh the most common oneyou're probably familiar with is just asimple cache so putting memory store infront of your persistent database sothat when you fetch data first you go tomemory store if it's there you're goingto get it with this ultra low latency ifit's not there then you go to thedatabase it's going to be higher latencyyou can then populate memory store sothe next time you need that data you canfetch it with that ultra low latency sothat's kind of the biggest use case wealso have session store here so imagineyou're a retailer and you're doing anonline checkout we want to keep thatuser's session data in memory so we canaccess it with super low latency andgive a really nice user experiencethird I'll mention leaderboards so youknow obvious for gaming companies youwant to have leaderboards you want totake advantage of redis sorted sets andyou get these nice data structures tomake this all really easy or maybe yourfintech company and you're showing stocktickers at the top of the page againleaderboards just makes this super easyand because all this is so easy withredis it has become the most popularin-memory store so you can see over timeredis has only grown in popularitybecause it is fast versatile it offersHigh availabilityand developers have just come to love itso we're excited today to talk aboutmemory store for redis cluster becausewe think it's going to take memory storeto the next levelso let's start and talk about the memorystore for redis service that has existedif you're on the platform today you mayalready be familiar with this memorystore for redis that we have today isfully managed this means no need toworry about the underlyinginfrastructure you just say that youwant to cache and we'll give it to youmemory store for redis is also highlyavailable you can scale horizontallywith up to five read replicas with thesefive read replicas you can support northof 1 million read requests per secondwith a single instancein addition to this it is verticallyscalable while online so you can resizeyour cache to meet your data needs goinglarger or smaller as needed in additionto thiswe are offering an integrated databasemaintenance experience we will handlepatching for you we do that in a rollingupgrade fashion we're integrated withthe maintenance window framework thatyou might be familiar with that othergcp services use we are integrated withCloud monitoring there is Rich metricsupport so that you can monitor thehealth of your instance you can alsoexport that to whatever MonitoringSolutions you might be using todaywe've optimized open source redis andadded improvements such as betterfailover as well as added memoryprotection for your instance when you'renearing memory exhaustionand then finally we are backed by thesame 24x7 Google Cloud support thatyou're already used toso those memory store for redis serviceas it has existed let's talk about someof the recent launches the focal pointof today's conversation is the memorystore for redis cluster launch so thisis a new service that supports the opensource redis cluster protocol you canhorizontally scale now so no longer areyou working with uh you know thesesingle node memory store deploymentssingle node redis deployments where youhave to replicate the entire data spaceto every node now you're horizontallyscaling and sharding out this key spaceso that you can reach a much largerscalein addition to that a few other launcheswe've had this year cuds these arecommitted use discounts you may befamiliar with this from some other gcpproducts this is a way to pre-purchasecommitments to the service gives youadditional savings you can do these in acouple different year increments and thelonger your commitment the more thatyou'll save off of the list price wealso launched support this summer forredis 7. this is going to be the latestmajor open source version with thislaunch came the added functionality ofredis functions it's kind of a spiritualsuccessor to redis Lewis grips and wealso made significant performance gainsfor TLS so when you're comparing againstTLS on Reddit six or older memory storeinstances you're going to seesignificantly lower latency andsignificantly higher throughput so ifyou are using TLS on on memory store westrongly recommend upgrading the red S7you're going to get the best possibleexperience with no added costthen the last one axt and DRZ these areaccess transparency and data residencythis is going to give you audit loggingof any access events to your instance aswell as some data locality guarantees asfor theresidency of data that you store withinyour memory store instance these werekey features to help unlock memory storeand use cases and some of the moreregulated Industries so we've beenseeing more and more customers in thisregulated space who are now able toadopt the serviceso let's talk about memory store forredis cluster which is now available inpreviewso what is memory store for rediscluster give you gives you consistentperformance at scale we've got a UI shoton the left here when you deploy amemory store for redis cluster youchoose a region A Shard count and thenumber of replicas per shardand when you do this you get an opensource redis cluster managed for you thefact that we support the open sourcerediscluster protocol means a coupledifferent things one you're not going toneed to perform any type of expensiveapplication rewrites and to yourmigration to the service is going to bevery simpleuh as you do this the benefits of thishorizontally scalable sharded deploymentyou can you can deploy up to 250 shardsfor a given cluster and this meansmultiple terabytes of of key space andmemory footprint associated with aninstance over three terabyteswhen you do this you also are able toscale in and out while the cluster isonline and as you scale in and out youbasically can size to your needs and wewill automatically rebalance the keysfor you so imagine you provision acluster of 100 shards for whatever yourdata data needs were yesterday over timeyou might be expiring Keys you don'tneed that full 100 shards anymore youcan shrink that cluster to whatever sizeyou need now and we will automaticallyrebalance keys from the nodes thatyou're removingin addition to this as you scale outeach additional Shard is going to giveyou more throughput for your cluster soyou can continue to scale with yourtraffic needsuh and so what does this do this reallytakes memory store to new heights in ourminds so over the past few years we'veheard your concerns about the memorystore for redis service which maybemeets the scale requirements that youhave today but it doesn't meet the scalerequirements that you might haveTomorrow based on how you expect yourapplication or workload to grow overtime and so with the new service memorystore for redis cluster we're seeing a60 times Improvement in total throughputthat can be supported by an instance anda 10 times Improvement of amount of datathat is supported by a single instanceas well and so we see tons of Headroomto scale here again you can also scalein and out as needed so you can pay asyou go and you can pay for the size ofthe cluster that you need as you scalein and out you're going to getconsistent and predictable PerformanceBased On The Shard count associated withyour instance and what we're hearingfrom customers is that they're startingto consider taking their many smallermemory store for instances that existtoday on the Standalone service takingand consolidating these instances intoone large cluster and they're going tobe able to realize huge cost savingsfrom doing this while also givingthemselves all this additional Headroomto scale in the futureand one of the things I alluded toearlier was the high availability sowith memory store for redis cluster thisis super easy when you provision acluster we make sure to put theprimaries in different zones when youadd replicas which is a single click ofa button we make sure the replicas arein a different Zone than the primary soyou can get four nines SLA extremelyeasily and with all that as well ifthere's a VM failure or even if there'sa zonal failure the failover isautomatic and fast to the replica sorather than having to do this yourselfon GCE memory store for redis clusterjust makes this super super easyI'll also mention we're integrated withthe best of Google Cloud so we'relaunching with support for privateservice connect or PSC which means thatyou can have your client and your VPCand you can connect to memory store inthe Google VPC and your traffic neverleaves the cloud it's easy to provisionthese clusters one-click automationagain and we also addressed one of theprevious issues with IP addressallocation so even if you have 250shards you only need two IP addressesfor your cluster so a lot of quality oflife improvements here with PSCand then of course we're integrated Ithink Matt mentioned some of these wereintegrated with security and monitoringso you get I am out of the box you cancentrally manage you have redis auth uhTLS audit logging which you mentionedand last of course Cloud monitoring soit's easy to set up monitoring on yourcluster you monitor is the cluster toobig is cluster running too hot maybe Ineed to add shards maybe I need toremove shards and again that's reallyall about making sure you're spendingthe right amount of moneyand now this is the end of ourpresentation we're going to bring onanimesh chaturvedi from Palo Altonetworks where he's a distinguishedengineer[Applause]hi everyone my name is Anime chaturvediI'm representing Palo Alto networks hereso Palo Alto networks is a global cybersecurity leader we deliver Innovationswhich enable secure digitalTransformations our mission is to becyber security partner of choice and wedeliver basically these Transformationswith our best of breed platformsworld-class threat Intel and expertServices which bring to the table what'snext in cyber security and cortex datalake is one such platform which is thefocus of our presentation today and I'mgonna talk about it so animesh prior tothis preview launch that we're talkingabout today we had a smaller privatepreview for a few select customers likeyourself to try out the product earlyyou had a chance during that period howdid that go well it was awesome and Iwas very impressed with the results Iwas looking for basically three keythings first is simplicity so it wasvery easy to deploy a new cluster addand remove shards as needed performanceand scalability as we add as I addedmore Shard the performance basicallygrew proportionately and in terms of asa managed service basically we do nothave to worry about the deployment sideand operations of it we don't need tohave radius expertise uh in-house andthen it also integrates very well withvarious Google ecosystem like IAM whichmakes it easy to adopt okay so it soundslike testing during private preview wentwell now that we're announcing publicpreview and expanding like this do youplan to adopt redis cluster yescertainly so we have a number of microServices which constitute this cortexdata Lake many of them use their ownprivate deployment of memory store someof them are sort of over provisioned andwe have some challenges with consistencyin terms of uh in terms of like securityas well as in terms of like thecompliance and other aspects of it byconsolidating all of these smallerclusters smaller deployments into managecluster I expect to save cost as well asget much better SLA I think with ourredis manage cluster we get four linesof slas so we are really looking forwardto ityes awesome so now let's dive deeperinto how animesh is using memory storetoday in his production use case ofCortex data lake or CDL all right sothis is basically a classicalmemory store use case as a caching forour application cortex data lake is avery large scale data platform whichbasically processes data that comes fromvarious parallel to firewalls andsecurity applications to dig a littlebit deeper basically firewalls areprocessing traffic in a given Enterprisecoming in and out and as they processthis traffic they apply securitypoliciesthe application of the security policyon the traffic generate huge amount ofdata the second service that Palo Altohas is network connectivity as a servicethis is basically known as Prisma accessand the platform is called secure accessservice edge this is where we connectusers to Applications these applicationscan be in like a third-party SASprovider or on-prem and we also connectusers to internet via this secureGateway so now this basically alsogenerates huge amount of data and thenthere are many other Palo Altoapplications which also produce eventsto this data platform so the amount ofdata that we receive is insane you canimagine there are thousands of customerswith many thousands of firewalldeployments they are generating hugeamount of data and as such we need adata platform a robust data platformwhich can ingest and process up totrillion events per dayso it does sound like a great use casefor redis but I'm sure a lot of theaudience knows as well that there's manydifferent options for redis on GoogleCloud what led you to choose memorystore so of course there are manyoptions available you can roll up yoursleeves deploy a redis on GCE or gke oruse a third party one of the third partyproviders of redis on Google Cloud butwe went with gcp first of all we have agreat partnership with Google and uhbeing a managed service we do not haveto worry about uh like when you deployanything in the cloud you have to expectthat the underlying node on which yourworkload is running at some point willget recycled out or patched and so sothat's something as a managed service isabstracted from us the Simplicity andthe performance and scale all that weget from a memory store basically we canfocus on our business and rely on Googlefor the operations of this criticalservice for usnice now let's Deep dive into the CDLarchitecture itselfall right so CDL as I mentioned is avery large scale security data platformwe leverage a number of Google cloudservices uh it's a micro service basedenvironment so we use Google uh gkequite a bit GK gives us a lot of Autoscaling and resiliency uh we use anthosservice mesh this is the service meshthat basically helps us manage variousmicro Services we also use enthusservice mesh as an API Gateway and weuse various Google data stores likebigquery Cloud SQL bigtable Etc and ofcourse memory store to speed things upwhen we query these various data storesall right so in terms of memory store wehave a number of different deploymentsfor various micro Services I'll groupthem into four categories first isingestion service then we have thecontrol plane and then the third one isour data processing service and fourthis the query servicenextso from an uh from an ingestionperspective from a onboarding workflow acustomer basically goes to Palo Altonetworks portal they create a virtualdata Lake for themselves in a particularregion then they associate theirparallel to applications or firewallswith the particular data virtual dataLake that they have provisioned at thispoint firewalls connect to a serviceasking for where should we send data wegive them a unique fqdn for that virtualdata Lake the firewall connects to thisingestion service injection services arevery large scale service running in gkeand gke gives us Auto scale andresiliency or daytime to night typetraffic is typically uh daytime isalmost twice as night time so Autoscaling is very important for us tobasically scale down and save cost aswell as we get few spikes so we want tomake sure we are able to address thosespikes so gke basically gives us all ofthat and uh and and with this basicallybecause we are running this datapipeline for many thousands of customersthis is very large scale and we have toprocess up to trillion events per dayso you have this gke service that's Autoscaling for the event load that you havecoming in you're enriching that datawith the cache data that's in memorystore trillion events per day how do youmanage that scale past that point sovery a very very good question so inorder to manage such a large dataservice you need a very robust controlplane the robust control plane basicallyneeds toextract various data pipeline componentsthe firewall metadata the connectivityfor various firewalls making sure thatthey are all able to send data and weand the customer metadata we are able toenrich the logs with contextual datafrom from customers environment all ofthis metadata is kept in Cloud SQL andwe put memory store in front to speed upthe pipeline thereby reducing ourcompute as well as making our pipelinelot more efficient right so the logicalnext question might be what if youdidn't have memory store to speed thingsup what if you had to rely only on thedatabase base behind all right so ifmemory store was not there then ourpipeline would have to basically makemetadata query calls to the underlyingSQL store which typically is much higherlatency now with that higher latency thethroughput of per parallelization iswould be much smaller but we still needto maintain that trillion events per daykind of speed so in order to do that wewould have to increase ourparallelization that means we have tobring in lot more compute and and thatmeans the cost of the pipeline would bemuch higher right so by using memorystore you can both speed up theapplication and actually save moneybecause you need fewer resourceselsewherethat's correct and the third service isthe data processing service we use Kafkaas our messaging layer we have put lotsof innovation and tooling around Kafkato work at our scale we are able to on aspike we are able to increase partitionswe are able to add our retired topic sowhen we have to let's say reducepartition we'll transition to a newtopic and then clean out the old topicwe are able to seamlessly add newclusters as our demand grows and uh andall of this information is kept in inbasically the metadata service or thecontrol plane the control plane knows uhwhere for a given customer which Kafkatopic and cluster the data resides inthe control plane also knows which datastore the the data should be transferredover to so to the right of the KafkaKafka cluster is our various workloadsthe workload up top is our persistentjob this reads reads data data fromKafka it talks to the control plane viathe memory store to look up which Kafkatopic or cluster to read data from andthen looks of the control plane again tofigure out what uh what their tenantproject and which data set should we bewriting the data to at the bottom arevarious Palo Alto streaming applicationsthese are real-time applications readingdata from Kafka applying filtering andenrichment and then sending the Matcheddata streams to the applicationendpointsand the fourth in the final service isthe query service query service providesthe abstraction for underlying datastorewhen a query from a UI or API comes init looks up the control plane with thememory store in front of it as to figureout where the data for that particularquery resides in our in our datawarehouse it runs those query jobsexecutes the query jobs caches theresults and presents it to the UI or APIas needed now in this case also you cansee we are using memory store so net netwe are we have memory store sprinkledacross all of our services and thishelps us make our pipeline much moreefficient and scalable and costeffectivegreat well thanks everybody we'reclosing now so that's that's all we hadfor today just to recap we're superexcited about the memory store for rediscluster launch so please give that a trythat's now in public previewum and we have our emails up here so ifyou have questions or feedback pleasefeel free to reach out to us directlyforeign"
}