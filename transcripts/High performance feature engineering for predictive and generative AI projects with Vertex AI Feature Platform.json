{
    "title": "High performance feature engineering for predictive and generative AI projects with Vertex AI Feature Platform",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML303"
    ],
    "video_id": "vWs2Q4gzO5g",
    "time": "Aug 30 06:45 PM - 07:30 PM CDT",
    "transcript": "[Music]all right hi everyone we're at one ofthe last sessions of the day so I figurethat people are a littletired I'll probably need a little bit ofyour concentration uh this is going tobe an important topic we're going totalk about data in machine learning thetitle of our session is high performancefeature engineering for predictive andgeni workloads and I'm going to bedemoing vertex AI featurestore my name is Alex Martin I'm theproduct manager for vertex AI featuresstore and with me is Cass s all the wayfrom Japan he's our uh developerAdvocate and also a resident generativeAIexpert today we'll talk about data inmachine learning the many problems thatthat you may encounter as you try to usedata in a m then I would introducefeature store as a solution to many ofthoseproblems Cass will do a cool demo of howyou can actually use feature store andwe'll wrap up with some feedback fromthe customers who already tried futurestore and also helped us develop it solet's kick itoff now in ourindustry models get the spotlight if youlook at all of the new newspaperarticles all the blog posts they talkabout the over complexing architecturesand all the different ways that we'remaking the tensorflow however if you ask actual AIpractitioners who ship projects almostall of them would tell you that thefastest most sure way to improve yourmachine learning outcomes is to focus onyour data easier said than done becausethere's a lot of challenges in how touse data for ML it's right there at thecenter of everything you do in mlops mlfeatures are compiled into a trainingdata set that actually is the foundationfor your model to learn from and act asyou wanted toact and then the same features need tobe passed into the training endpointoften at a very very low latency andwith exact same Transformations just sothat you can avoid train serving scoopand I've heard some tell me that oh butthis is a generative AI ERA this isfoundation models all the data is encodathey kind of like apis it's no longer asimportant well if you heard terms likerag grounding fine-tuning these are alltechniques that are actually used tomake llm apps work in production and allof those techniques are rooted ineffective use of data and if you're oneof those few companies that actuallytrains the generative models thennothing change for you you still have tocompile these massive data sets toactually improve your models now it'salready pretty clear that very fewcompanies are actually going to betraining the models training the largemodels and most companies in order foryou to Future proof your businesswhether you're an ml company or anycompany for that matter the best way isto compile a really good predictive dataset so there are some challenges inactually having that data and using itfor your ml models number one is beingable to centralize all of the featuresthat you're using across differentprojects and differentorganizations I've seen many many teamswhere you'd have exactly the samefeature something trivial like averagerevenue per customer per day the samefeature being created in multipleprojects held even in the same projectsometimes where it's diligently therewith an ETL pipeline removingoutliers um cleaning up various uh uhvarious attributes and all of this isdone in exactly the same way for exactlythe same feature twice or three timesbecause the data scientists didn't knowthat this feature already existed andanother team has created it has shippedit there is a model running it but theyhad to create it fromscratch next up is train serving skewprobably thesingle most popular reason why modelsdisappointing production train servingskew is any time when whatever you'vedone in training cannot be replicatedinserving an edge case of train Sur skewis data leakage and this is probably theshest way to crush a data scientistspirit is to show them that the reasontheir model is finally behaving well isnot their grit or their smarts butbecause they leaked some data into thetrainingvariable this happens all the time manyof us have stories to show how you'vedone everything right but then youforgot that this one variable was notcorrectly looked up in point in time andthe data leaked into your model and thenwhen you deployed it in production itdidn't it didn't work well servingenvironments actually really constrainedso train Sur SK is often inevitablebecause whatever you've done at yourtraining time you've used some commonlibraries maybe you've used somethinglike pandas that's not super performanceoriented but it's just very nice to useyou can't really use that at servingtime with latency constraints and manyother uh problems that you just don'texperience when you try to use your dataum in in a common environment like anotebook which brings me to my secondpoint my third point that realtimeserving is veryhard just about any team that I talk tothat has engaged in creating a realtimeserving pipeline has told me that thiswas not their most funactivity and uh they would share somestories on how they've been setting upthe infrastructure how the teaminevitably Grew From one two people tonow dozens and seemingly the problemsjust keep keep coming much faster thanthey can address them reconciling memorybetween multiple nodes dealing with readwrite congestions with hotspotting withload Spikes all of these are problems ofreal-time suring and there is a reasonwhy real-time data processing istypically the last topic of any dataengineering curriculum it's really hardin fact the amount of hurdles that teamshave to jump through to make sure thattheir data can perform in real time withlow latency is so high that many teamsjust don't go there and they onlyoperate on batch workloads and I've seenmany projects which can clearly beimproved by replacing some kind of batchworkload with a real time end point butthe teams just don't go there they theyfeel like this is going to be way toocomplex of an architecture to maintainum and even though batch serving in alot of times is an edge case of realtime um they still they still don't doit the worst part about it is that theuser experience is almost always betterfor for a real-time serving becausepeople just don't like to wait and ifyour API can respond quickly or thefaster it can really respond generallythe better your systemis and if it responds fast you stillhave a problem of serving the right datawhich brings me to my last topic offuture engineering any of the datascientists who actually engaged inmultiple new projects know this kind ofbeginning of a project where just startfrom a blank state of a data set tryingto figure out which features to usetrying to transform your data explore itI think there's multiple studies thatsay something like 80% of data scientisttime is spent on data cleanup dataprocessing and feature engineering somaybe this whole affair should not becalled machine learning but featureengineering um and the process actuallyhasn't changed over the time despiteseeing many like Innovations and youknow with deep learning and Transformerarchitectures feature engineering islargely the same that it was like 10years ago you still have to just takethe data manipulate itapply some transformation methods reviewthem train the model with themunderstand it and likely be disappointedand do all all over again until you findsome features that actually work there'smany algorithmic methods that help youdo this much faster like feature searchalgorithms automated feature engineeringalgorithms and they're still only in thenency of theirpopularity so this was just a subset ofsome problems and I know it's the end ofthe day and hopefully I got you a littlebit worried about well am I doing dataright with my machine learning stack andyou know do I need something to uh tohelp me there um so the good news isthat for the rest of my talk I'mhopefully going to be calming down theanxiety because feature stores are thesolution to the many of the problemsthat I've just described there and theyreally are there to create discovermanage and serve ml features atscale feature stores are kind of likeapis to your data you can think of it asan interface to the entirety of your datdata stack they abstract the dataengineering for your data scientists sothey can focus on creating the featuresand actually shipping those featuresinto production so I actually think thatlooking back at my diagram earlier todayfeature store should be at the centerbecause you don't usually interact withthe data you interact with the interfaceto that data or the tools that thattransform this data and that's exactlywhat feature stores are and they kind ofhad a bit of an Ascend because just 5years ago I don't think that the producteven existed and now um in 202almost every company not just AI leadersnot just the the big names in the EMLspace but just about every companyeither has a feature store or someconcept of a feature store or is veryactively evaluatingone and that is why I'm so excited totell you that vertex AI has a newfeaturestore we have completely rebuilt it fromscratch on top of bigquerythis is the best data warehousing anddata processing infrastructure in theworld and we're building rout on top ofit we also completely re-engineered ourserving stack and we now have some ofthe lowest latencies for realtimeserving of any features through outthere and those of you who areexperimenting or maybe even notexperimenting but productionizing yourgenerative AI apps you no longer need aseparate sta to manage data for Gen appsyou can do all of this inside of ourfeature store because we have built inNative methods for working withinstructure data and common tasks forgenerativeAIso the new feature store doesn't justintegrate with bigquery we actually haveworked closely with the bigquery team tomake sure that you can access the bigtables from the feature store withoutanyoverhead so you you don't have to copythe data and ingest it into featurestores like mini products request youcan literally just call them using thestandard bigquery apis it just worksthat also is brilliant for any kind ofsecurity setups you have with bqualready you don't have to replicate themin featur store theypropagate now our online serving latencynow is at 2 milliseconds this is reallyfast this is as fast as you can probablyget there for practical purposes and isdefinitely fast enough for you to nolonger worry that you have to optimizeyour features for performance your readlatency server side it's going to betopnotch and of course Google cloud isone of the leaders in generative AIapplications and research so we had tomake sure that feature St works well foryour generative AIworkloads so we went ahead and we builtin Native support for embeddings infeature storethat means that you can store theembeddings in bigquery feature storewould create the index for them it'llexpose the API method to get similaritems you don't have to set up aseparate Vector database you don't haveto copy your data you don't have toworry about a governance breach youdon't have to worry about syn conflictsall of this is going to work within thesame system that system being featurestore and I wish I had more time in mysession because the bottom of the slideis just as important as the top there'smany core feature store features thatare available that I'm only going totalk about briefly things likeversioning now change management is mucheasier it's much more resemblant of yoursoftware engineering practice thingslike lineage I mean that deserves itsown slide just all the different waysthat you can trace how the featureevolved before it enter machine learningany of you who ever debugged a or anykind of uh drift problem with machinelearning knows that this can easily be aweek-long procedure if you don't haveany lineage toolsand all the known methods that we havefor point in time lookups web uis sdksand so forth this is ourarchitecture you heard me mention thatwe're now built on bigquery and you mayhave been kind of questioning so whatdoes it mean so I'm going to illustrateit for you actually I'm going to startwith a quick story because the mostcommon feedback we heard on our Legacyfeature store was that it has to workreally well with bigquery so we actuallygot together with bigquery team andwe're like how do we integrate how do wemake sure that this is the bestintegration out there we started listingall the things that people expect out ofthe bigquery system out of the futurestore system like what are we overlapand very quickly occurred to us thatthey're almost the same things thatpeople ask for so instead of integratingwe decided to build on top of big queryand basically make bigquery the featurestore and it really helps that we areright next to the bigquery team and wecan actually do that I know that manycompanies tried but but couldn't get thesame level ofintegration and because it's built on Bit's right there in the middle of mydiagram showing you that you'll use thenative methods for data ingestion fordata reading you can use all the sameSQL you can use all the same processingall of it is powered by the coreinfrastructure that you're likelyalreadyusing as I like to say instead ofbringing your data into the featurestore we bring the feature store to yourdata and those of you hopefully very fewwho don't know how good bigquery is Ijust want to give you a quick summarybigquery is one of the best datawarehouses in terms of storing the dataretrieving the data and ingesting thedata it integrates with just about anysystem out there from jira to Googleanalytics to many many differentdatabases it has a very active Communityif you want to know more about theIntegrations or debug them um and it hasone of the most efficient storagesystems in theworld and feature store does not requireyou to do any kind of post processingwhen your data is in bigquery so as soonas it's available there it's availablefor uml workloads with noduplication SQL is what drives bigqueriesinterface and boy doesn't execute SQLwell just about any transformationyou're going to throw at it is likelygoing to complete inseconds and allow you infiniteflexibility in how you find yourfeatures and I don't have this in myslide but actually as of this conferencebit no longer supports just SQL we alsohave what we called um big frames whichis a pendas like python interface forbigquery so you can now Define yourTransformations not in just SQL I know alot of data scientists are not big fansbut you can actually Define them in yourfamiliar python you can have some testsagainst it so everything as if you'rewriting code in your notebook and thebest part about it is that this is justthe interface the execution is stillgoing to be on the big queryinfrastructure meaning that you'll getthe best performance and the highestscalability out there among any data wrewarehouses and no story about datawarehousing is complete without the talkon governance and bqu has it all columnlevel row level data set level tablelevel you can Define your governance andyour permissions however way you wantand it's always a challenge to make surethat you're protecting the data but alsonot making it a nightmare experience forsomebody who's trying to consume thisdata and big Cory has all the controlsfor you to make sure the policy isflexible yet effective and in featurestore we just use it as is you don'thave to rep at which is really reallynice because it really Powers yourgovernanceStory the one area where big query isnot that great is realtime serving it isreally meant to be a badge processingwarehouse and that's where the featurestore team came in and as I said we'veactually rebuilt our serving stack weused to be based on cloud bigtable andit's still an option for all of youthere but we also have a new low latencyserving option which deliversmindboggling 2 millisecond serving sidelatency which is really fast probablyfaster than most feature stores outthere and for Anto latency it's in about5 milliseconds and I know many peoplesay that latency measurement this kindof number throwing is really complicatedreally depends on the type of workloadon the size of data so we ran quite afew benchmarks to arrive at at ournumbers and we mostly compared with theworkloads we saw already so we encourageyou to do the same and many of ourbenchs actually showed a lower than 2millisecond late although I don't knowmany user experiences who would benefitlowering latency even more our goal wasto make sure that you have to not worryabout latency anymore you just buildgreat user experiences and you letfeature store do the rest and to helpyou with this we made sure that it'strivial to set up an online featurestore all you have to do is write createonline store set up your data syncpolicy and you're done C is going toshow it in just a minuteminute and not only it's easy to deploythe feature store it's easy to maintainit too as you workload Spike we'llprovision moreservers as they go down we'll turn thoseservers down make sure that you canaccommodate any workload you want 5,000QPS 10,000 QPS 50,000 QPS no problem atall we'll make sure that we scale ourinfrastructure accordingly so you don'thave to worry aboutit and for the those dealing withgenerative AI I'm sure you've heard thatthe embeddings is the new most populardata type at least fornow and you have to know how to workwith this the key operation onembeddings is similarity search gettingthe similar items back there's an entireindustry now in Vector databases tryingto power this use case there's all thedifferent problems that you can run intoas you try to introduce a new API intoyour stack a new storage system intoyour stack which is why we decided tobuild this inside of the feature storeinside of the same online servingendpoint you already to use for the restof your data that way there is nogovernance breach there is no new APIyou have to learn maintain Run Securitychecks on it's all built in in onesystem and our goal is to make sure thatfeature store works for all of your datastructured unstructured predictive orgenerative all of this can be powered byfeature storewe know that many machine learning usecases are trained offline Serve onlineso the methods for similarity retrievalare actually going to be available inbig query as well so you can play withyour system as you're training yourmodel and you can run those methodsoffline in distributed fashion and thenwith just a few commands you'll deployit online and it's going to be availablethere for all of your real timeretrieval we take care of building theindex we take care of maintaining theinstances and there is a reason whythere's high performance in the title oftoday's session because we can supportjust about any embedding sites 1Gigabyte 10 even hung gigabyteembeddings indexes are totally fineyou'll still have millisecond latencyresponses we're powered by one of thealgorithms that Google has used for manyyears even in some Production Serviceslike Google search and YouTube and wescale really well across all aspects ofonlinesurvey nowthere is much more to the feature storethat I can cover today our data op storyis not going to be complete with justone slide I hope that I'll have morefollow-ups but we have lineage where youcan see literally you can click throughand see all the tables involved increating the feature all the SQL thatruns in between them that's really coolas I mentioned when you're doingdebugging Lifesaver if you just tryingto run some broad queries and try tounderstand like what are the most kindof Choke points like what to some tablesthat power most of my ml workflows youcan do that too there is versioning nowif you ever done change management andyou've just overwritten the futurevalues I'm sure that your futureconsumers have been not very happy withthat practice even though you meant youhad the best intentions but just likesoftware you don't want your librarythat you've been using to just updaterandomly on you data is part of yoursoftware in machine learning so you haveto make sure that there's appropriate uhchange management process and that'susually done through versions you justpublish any you ask everybody to upgradeyou don't force themto online serving continues to delivernew challenges for us so there's manyedge cases that we're addressing one ofthem being uh hotspotting where youretrieve the same value over and overagain we created some cash mechanisms tomake sure that this is effective uhpoint in time lookups this is a stapleof any feature store these days uh we'vedone some performance optimizationsthere there's a web UI there's a pythonSDK we got you there for all of yourdata Ops with vertex featurestore and to show you here is Cass doinga[Applause]demo we we will take a look at how touse feature store to retrieve featurevalues instantly as Alex mentioned anduse them for similarity search forrealtime shopping experience let'simagine there is a spot Weare Shopintroducing a new AI Kiosk for theShoppers they can scan an item with thekiosk and find similar items instantlyto explore variousoptions to implement the AI kosk youneed a system like this the first thingyou need to do is the uh to have the bigquery table storing item features suchas ID name price and so on then apply anml model to extract embeddings from theitem features in this demo we used textedings generated from the itemnames lastly you create a new onlinestore instance that Imports the itemembeddings from the bigquery table byspecifying the sync setting the updateto the table will be automaticallycopied to the online online storeperiodically then you are ready to fetchfeatures from the online store when theShopper scans the item with the kioskyou can use the item ID to retri theitem m benix in a fewmilliseconds the ambix will be used tofind similar items shown to TheShopper from the feature retrievalsimilarity search and to show the resultto theuser it will it would only take a TENSof milliseconds this is the real time AIuser experience you can build with thenew featurestore so you may be wondering can youuse any arbitrary big query table orview as an online store the answer isyes as long as the table or view followsthe conditions here the table shouldhave an ID should have an ID column andthe other columns should have thesupported data types it is recommendedto limit the low size up to 10 megab tooptimize the performance and also thetable should have should be stored inthe same region or Mregion those are the conss now you candeploy the feature vales to the onlinestore in three steps create online storeinstance registered bigquery table orView and set up a sync betweenthem so let's get into thedemo okay but I have prepared TheNotebook but it's too busy and too smallso that I actually copy and take ascreenshot of the important part solet's look at those the screenshots ofthe notebook so this is this is thesport real shops bigquery table that hasthe item id name category price and soon then I use the ml. embed textfunction of bigquery ml to extract thetext eming from the item name and savethem as a new table we will let featurestore with this table as feature valuesto start working with feature store youneed to load the feature store SDK withthose inputs and get theclient with the client you can call thefunction to create an online storeinstance for the first time it can takeup to 10 minutes but after that you canget the instance in asecond now with the online storeinstance you need to register a bigquery table as a feature view featureview is a collection of features youwant to use on the onlineservice the last step is to set up thesync between bigquery table and onlinestore here you specify the feature viewto start syncing and check the syncprocess the syn time you will depend onthe amount of the data you are you arethinking in this example my table hasabout 30,000 rolls and the think tookabout 3minutes now the online store onlinestore is ready for serving let's take alook at how you can use it in thisservice in your online service you canget the client for the online store andcall fetch feature values function bypassing the ID and that's it you youwill receive the feature values in a fewmilliseconds you can use them forfurther search andrecommendation the new feature store hasa low laty service as Alex mentionswhich is ringly fast to test thelatencies I did a simple performancetest i l the full RP and measur the timeto F the values for 1,000 times theresult you are seeing is 2 millisecondsso online store works just like a localvariable in your python code that holdsall the feature values automaticallycopied from your big query table no needfor issuing SQL query and wait forseconds to retrieve each item fromdatabasesthere's one more things as as Alexmentioned earlier the feature store cansupport the retrieve uh can also supportretrieve items with similar emingswithout the need to set up additionalServices here I passed the item ID tofind other items that have similarembeddings notice that how I didn't haveto import any other Library set upanother service or copy data it's alldone by the feat store itself it returnsa list of the similar items and scoresthat I can use for the ranking that'sreally nice if I wanted to combine theretried features for the recommendationsor sear usecases lastly I'd like to share one morecases of the shop scenario that is an LMintegration suppose you are adding an AIchat Bard to the AI kiosk using the LMlike verx AI PalmAPI also the shop has the big quitytable to track each C customer shoppingactivities such as the Google analyticslog on the website or on-site itemsearch history so the feature store canfeed those activities in realtime now when the customer starts tostart the chatting with the AI Kos youcan argument LM prompt with the lastitem the customer checked and have thepersonalized chatexperience you can easily fetch the lastitem from the online store for eachcustomer customer and build a promptlike this in this case the customer hadinterest in the crowd ve women'sExcursion shot so we want to we want theLM to lead the conversations for closingthe deal with the item The Prompt goesright this you are a personal shoppingassistant in the sportware shop thiscustomer looked at the croud women'sExcursion show last time start theconversation to Closs the deal with thiswith thisitem an example of the conversation withthis prompt will be like this let'slisten toit hello hello welcome to our store canI help you find anything in particularnothing particular what do you have wehave a wide variety of sportswareincluding the cloudveil women'sExcursion short these shorts are greatfor hiking camping or any other outdooractivity they're made with a lightweightwater resistant fabric that will keepyou cool and dry even on the hottestdays would you like to take a closerlook so please note that I haven'tcreated any part of this chatbot answerall generated by the pal API with theprompt build with the feature store thisis an simple example but there is anendless possibilities of what you can dohere hello oh sorry hello welcome to ourstore can I help you find anything inparticular nothing particular what doyouhave so in summary the feature storeit's so easy to retrieve any features orembeding value of the items or customersor any other business entities in thereal time this enhances the ability ofof your machine running model forrealtime prediction similarity search orrecommendation also it empowers geni andits us experim experience with thedynamically built prompts so that wasthe demo back to[Applause]Alexso I've talked to you about thechallenges and making data in productionfor machine learning I've described thefeature store made a few announcementsCass showed you the demo we played theaudio segment twice what else can I doto convince you that feature stores arethere and that you should be using themwell one of the coolest things aboutworking in vertex AI is that I get towork with some of themost advanced companies in Ai and I getto advise them and also learn from themon how to buildproducts and a few of thosecompanies sometimes go on record anddescribe the experiences they had withus building thoseproducts one of those companies isWayfair they're one of the largestretailers they're one of the largeste-commerce stores for furniture in theworld and they say that that vertx afeature store presents a majoroperational win for their team they lovethe big queryoperations and they believe that featurestore has simplified their mlopsstory another company that you probablyknow isShopify the second largest retailer inNorthAmerica and is really well known inmachine learning and data engineeringspace and they said that the new vertexa feature store is a significantImprovement in the design of the featurstore they loved our low latency servingit helps them create the magicalexperiences that the company is wellknown for and we work very closely withthat team to learn about how to buildfeature stores what it means to have afeature store in a large company there'sa lot of fun conversations we had aboutthe contracts between the futureproducers and futureconsumers but what's best about both ofthose quotes is that both of thesecompanies are very well known in mlspace they have thousands of Engineersdefinitely hundreds maybe not thousandsof ml engineers and both of themchose to not build a feature store butbuy a feature store because this is thetype of product that may be best builtby a company likeGoogle so that about concludes my talk Ihave one last thing to say the futurestore is going to be a available inSeptember for everybody in this room andeverybody outside of this room we'regoing to enter public preview by the endof next month so you can start reachingout to your salesreps and getting on thelist I want to conclude by saying thatfeature store is the type of productthat really for a lot of teams marks thebefore and after era in their mlmaturity when implemented right andadopted fully it really elevates yourentire mlorganization so I hope to see you as oneof the future store customers and Ican't wait to hear what you get to buildwith it thankyou"
}