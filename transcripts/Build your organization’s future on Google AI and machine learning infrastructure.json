{
    "title": "Build your organization\u2019s future on Google AI and machine learning infrastructure",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML200"
    ],
    "video_id": "C0ssCOWFMSI",
    "time": "Aug 29 04:45 PM - 05:30 PM CDT",
    "transcript": "foreign[Music]Welcome to our session full room todaygreat to see my name is George Elise andI'm a senior director of productmanagement leading our AIMLinfrastructure portfolio here at gcptoday we're going to cover how GoogleCloud will help you build yourorganization's AI futureI will be joined today by threeincredible speakers Paris Korea whoseleading product for a number of our mlis productsOmar Hassan VP of operations atuploading that is building a powerful AIbackedad engine and finally mile odds foundingresearcher at character AI one of themost Innovative AEI companies in theworld todayso have a full session and a lot tocover so let's get startedit's really no news to anyone in thisroom that in the last two years we haveseen leaps of progress in the in thefield of machine learning perhaps bestportrayed by the revolution ingenerative AIprobably most of you also know thatGoogle's research and Technologyis at the heart of this revolution inmultiple waysit is really no exaggeration to say thatAI is in our DNAGoogle say idea also directly impactsand enables Google Cloud customers inreally multiple waysthrough industry defining air D thatpioneered products like Transformers andtpusor through leveraging our experience ofbuilding our own models training themand serving them and taking thoseexperiences to enable our own Googlecustomersto build and fine-tune their modelsand of course by continuing our longhistory of Open Source contributions toinfluential projects like kubernetesJacks xla and many more that trulycreate an open AIML ecosystemand we can't forget our learnings thatwe get from scaling apps like Gmail andGoogle search and YouTube and how GoogleCloud translates these learningsto customer benefit in order to cut downtheir AI journey and remove any scalingobstacles that they might faceand you can really see that AI DNAreflected in Google Cloud'sinfrastructure that will make availabledirectly to customersan efficient highly available performantand secure infrastructure at scalewhether that is with purpose-builtcomputebest-in-class networking or asustainable and efficient data centersthat run on green energywe really think that Google cloud isuniquein offering so many advantages to AIcustomersand it's therefore no wonder why AIcustomers overwhelmingly choose GoogleCloud for their workloadsin fact 70 percentof the most Innovative generative AIorganizations have already chosen GoogleCloud as their platform customers likeanthropic ai2 Runway ml character couldhear and so many more build and servetheir models on gcpand of course also build Innovativeproducts from AI chatbots to Advancedvideo editing capabilities to newmarketing tools and so much morebut it's not just organizations thatwere built to do AIthat make gcp their platform of choiceit is really customers of everyconceivable vertical or use casefor example LG uses tpusto train their own large language modelsor in the financial services industrycustomers that do fraud detection orrisk management like Goldman Sachs andWells Fargo run on gcpin retail our customers build some ofthe most advanced and accuraterecommendation engines out there likefor example Walmartand in manufacturing customers like Fordand GE with predictive maintenancecapabilities and with quality controlsystemsleverage gcps AI infrastructureand of course you have medianentertainment and so many otherverticals for example snap andmid-journey some of the most Innovativemedia entertainment customers out thereuse both video and imagesto generate content that is consumed bybillions of users dailyall build on Google Cloudand we on our sides are truly fortunateto be working with thousands of thesecustomers every day and helping themleverage AI to achieve their missionand of course we get a lot of learningsfrom understanding their needsand our advantage is really applyingthese learnings in order to build acomprehensive machine learning stackthe trend is all the way frominfrastructure through software and upto servicesallowing you to consume exactly where isbest for your applicationfor example let's say you're producing afoundational model from scratchGoogle cloud is unique in offering youBest in Class Choice among Best in ClassAI supercomputers based on both gpus andtpus that scale to tens of thousands oftipsor let's say you're fine-tuning modelsyou probably need to manage a large andcomplex data setsgcp platform services likeGoogle Cloud dataproc data flow andbigquery allow you to manage multipledifferent training sets for each one ofyour customers security and securely andefficientlyand of course you could be consuming andserving these modelsbaked into your applicationwell gcp enables everything from thelowest latency serving to the highestcost efficiency inferencing withHardware choices like the nvidil 4 GPUor TPU v5e both of which you will hearmore about later todayand of course you can move all the wayup the stack and consume at the vertexAI platform layer where we offer managedservices that completely abstract awayall of the complexity and offer you thefastest path to production AIbut for now let's focus a little bit onthe infrastructure layer which is theending of much of the Innovation thatwe're seeing in AI todaythere we can take a quick dive intoGoogle Cloud's accelerated offerings thegpus and the tpus let's start with gpushere Google has a deep and storiedpartnership within with Nvidia that isallowing us to build an industry-leadingCloud GPU platformthe approach we're taking is to provideGPU infrastructure that is bestoptimized for your workload and we dothatacross three three main actorsthe first one is speed to Marketwe were the first Cloud to bringnvidia's P4 T4 na100 instances to thecloud and we're still the first and onlyCloud to have Nvidia L4instances offered to our customersso Google customers can continue to restassured that they will get early accessto the most Cutting Edge Nvidia gpusgoing forward as welland in a fast changing commercialenvironmentthis can be a huge competitive Advantagefor our customersfor example it can allow you to serve tolower your serving cost compared tocompetitorsthe second axis is the breadth of ourofferingwith nine different Nvidia gpus withflexible VM tapes we provide optionalitythat allows you to pick the rightconfiguration for your exact workloadreducing costand increasing efficiencythis follows through whether you'redoing Graphics work work or you'retraining the largest large languagemodels out thereand finally the third axis to optimizethe cloud experiencefor performance and productivitywith software stack optimizations likeJax and open xla that we brought to gpusand by offering scale and elasticitythat many of our customers need to scaleto thousands of gpus that work in tandeminterconnected as a single supercomputerso let's take a quick look at two of ourlatest GPU offerings on Google Cloudand we'll start with A3our most powerful GPU instance poweredby the h100 Nvidia TPUthe a3vm is designed for your mostdemanding AI ML and HPC workloadsit combines eight h100 gpus connected byEnvy link nvidia's high-speed GPU to GPUinterconnectA3 achieves 10x the networking and 3Xthe training performance of itspredecessor the a100 which we madeavailable on Google Cloud through the A2VMare unique intelligent Jupiter datacenter networking fabric allows tens ofthousands of h100 to be interconnectedand work together as a single machinedelivering up to 26 exaflops of AIperformancethis level of performance is a gamechanger when it comes to training thelargest AI modelsA3 is going da in the next monthbut is already trusted by multiple genAI unicorns like anthropic and characterand mid-journeyearlier this year we also announced ourg2vmwhich is powered by the Nvidia L4 GPUG2 is our most versatile GPU offeringand is able to power a broad range ofapplications from generative AI toGraphics to Virtual workstations orgamingit offers significant performanceimprovements compared to its predecessorwhich was powered by the T4 Nvidia GPUlet's take a look at some numbers we'vealready seen customers achieve 2.5 xwith Gen AI inference performance forgaming 3.3 X and up to 4X for digitaltwin applicationsthis has fueled real rapidumadoption amongst a wide range ofcustomersfor example Volkswagenuses D2 to power their virtualshowroom experiencebending spoon bending spoons achieveslower latency in serving generative AImodels and you know bending spoonsearlier this yearto tens of thousands 32 000 gpus inorder to bring their application to thetop of the App Storeand then you have customers likeworkspot that optimize and change reallythe cloud PC workstation game byimproving frame rates by as much as 2Xto sum it upour strong partnership with Nvidiaallows usto be the first to bring the best gpusto Marketto optimize the software stack so youcan get the best out of the NvidiaHardwareto ensure our customers enjoyscalability and elasticity so somethingreally important for them today with ahuge Fleet of multiple GPU offeringsand of course to deliver optimized fullymanaged services like vertex on top ofgpusand you can see the value that gcpbrings to customers with products likeA3 and G2in the consistency of the testimonialsof some of the most influentialcompanies in the AI and Graphics spacewell by now you probably heard from meenough already why don't we heardirectly from a Google Cloud customerlet me bring on stageOmar Hassan VP of operations atuploading let's give him a workout[Applause]thank you George for the kind intro I'mOmar Hassan DP of operations at uplovenI've been working with Apple event overthe last decade heading out technicalinfrastructure overseeing devopsinfrastructure networking infosec and itteamsapp Lovin makes technologies that helpbusinesses of every size connect totheir ideal customerswe provide end-to-end software and AIsolutions for businesses to reachmonetizeand grow their Global audiencesapp Logan's mission is to createmeaningful connections between companiesand their ideal customersat op 11 we have been working hard toupgrade our advertising algorithm withthe latest AI techniqueswe're using AI to replace humanoperationslike targeting setting bids learningphases and more that either our teamor our partners used to have to makemanuallythese updates allow our partners toachieve their goalswith greater accuracy and speed on anexponentially larger scale while savingthem time and improving theiroperational efficiencywith these latest AI techniques we areseeing early results and impact they'realready making for our partnersour advertisers are able to benefit fromThe increased Automation and recognizebetter roas or return on ad spend at amuch much larger scaleour AI engine axon was first launched inlate 2020 and was part of our firststepping stone into Google Cloudsince then we've continued to build uponand expand itin our latest iteration we wanted toimprove the operational efficiency forour platform for both app loving and ourcustomers by moving towards a newaccelerator for axon 2.0however we weren't sure what to selectgiven the different number of productsavailable to usand after Consulting with the GoogleCloud teamwe decided based on our use case andperformance requirements that Nvidiacloud gpusis going to be the best choice for usthe three main criterias for us wereease of programmingorchestrationVisa programming and orchestrationperformance and latency requirements andavailability of Hardware across manymany different regionswe were already in the midst of alarge-scale gke migrationand as our Legacy infrastructure wasstarting to show its age we feltcomfortable launching our next iterationof the axon 2.0 platform on Nvidia L4gpusorchestrated by gkewith the help of Google we were able togo from an initial discussionto large-scale production rollout inweeksso how did we get thereeach of our research scientists are ableto run their jobs in unique developmentenvironments powered by vertex AIworkbench which improves our time todevelop and therefore our time to Marketwith training orchestrationwe chose composer and while this isairflow under the hoodhaving it being managed by Google led todurable operational efficiency whileleveraging and being able to leverage uhNvidia a100 gpusto power our new training workloadswe leverage bigquery to help with dataaggregation to simplifya transition from dataproc and Spark andinto a server solution that was easierto query and faster for uson the inference side we leverage dkeonce again toto scale worldwide across tens ofregions which consistently allowed us toramp upour business unimpeded by Hardwareconstraintson the axon platformled to Greater Automation andconsistency across our application stackAgain by leveraging gkeimproving improving our training time byleveraging the latest GPU Hardware likel4sand the agility by having theflexibility to launch into new newregionsbecause of Google Cloud's deepcommitment to AI Technologies theadvancements in our platform have beenpossiblethank you everyone and on to you paresh[Applause]thank you Omarwhat an amazing story of using a wholehost of Google Cloud Technologies tofuel recommendation systems and one ofthe Technologies Omar talked about wasGoogle's cloud gpusearlier George talked about how weworked very closely with our partnerNvidia to optimize gpus on Google Cloudin addition to Google cloud gpus inorder to enable even more choice for ourcustomerswe also offer on our Cloud our homegrowntensor processing units or tpusnow before before I deep dive into tpuslet's take a step backmodern generative AI is transformingcomputinglarger and more complex models arebasically creating a lot of capabilitiesacross a whole range of use cases andfueling some amazing applications but atthe same time this complexity and thescale of these models that creates amassive Computing demand in fact theComputing budgets have grownastronomicallyif you look at some of thestate-of-the-art models today trainingevery single one of the state-of-the-artmodels can take up to tens of millionsof dollars of computing budgetand when you deploy these models in yourapplications the cost can run intohundreds of thousands of dollars per dayat Google we are of course no strangersto this phenomenon we realized almost adecade ago that if our users surgedusing voice for just three minutes a dayGoogle would need to double its DataCenter capacityit's with this realization we createdour first TPU and since then the pace ofevolution the pace of innovation hasdramatically increased today tpus aresuper Computing scale AI systemsthey are powered by purpose-built chipscreated from grounds up by Googleand they are interconnected by some ofthe industry's fastest Opticalinterconnectsand they are deployedat extreme efficiency using liquidcooling in our data centers throughoutthe worldin fact some of the most popularapplications and services that you'refamiliar with including search YouTubeand Gmail as well as some of the mostcommonly known models at Google likepalm and Lambda they are powered byGoogle's cloud tpusso let me take you uh behind thecurtainssee how the Google tpus look like today[Music]today's AI applications are powered bysomething you almost never get to seepurpose-built AI infrastructure andwhile we're operating thisinfrastructure can scale it all startswith the chip Google's tensor processingunits or tpus are custom developedapplication specific integrated circuitsalso known as A6 designed to create atscale supercomputers that handle some ofthe world's largest AI workloadsin fact we believe in TPU so much thatwe use them to power some of Google'spopular services including photos andsearch the real magic happens when thesecheats are all interconnected using ourcustom intercore Optical interconnect tocreate a single super computer capableof utilizing thousands of these chips tohandle some of the world's largestinference and training weaponsutilizing the power of our data centernetworking we can create large-scaleversions of such super computersconsisting of hundreds of thousands ofinterconnected shipsGoogle has taken a step further andutilized the optical circuit switchtechnology that allows the supercomputers to reconfigure themselves andcreate efficient machine shapes andsizes for your workload scaling up ordown on the fly as needed without anyhuman interactionthis is all made possible by ourstate-of-the-art OCS softwareinfrastructure optimized for efficiencyand speed our vertical power deliveredto the super computers is cooled by ourlatest generation water cooling systemthat is designed with sustainability inmindall this technology has the attention ofour dedicated team of data centertechnicians and friendly securityPersonnel who are there to guide throughour various security protocols like theBiometrics verification through Irisscanall of this operating at the scale ofour entire planet because we believethat AI is for everyone so we'rebuilding the magic to make it happen andgiving you an opportunity to build evenmore on our Google Cloud TPU platforms[Music]wowand the tpus is stunningearlier today we announced the nextiteration of our Cloud tpus the TPU v5eTPU v5e is our most cost efficientversatile and scalable TPU to datefirst is cost efficiencyif you're training large language modelsand generative AI models TPU v5e wouldprovide up to two times more performancecompared to our last generation TPU V4and for serving these large models youcan expect up to 2.5 times higherperformance for every dollarsecond it's incredibly versatilein addition to doing both training andinference TPU v5e is available in eightdifferent VM shapes and these shapesallow you to have a combination ofcomputing capacity and memory capacityso you can fit a full range of models ifyou're looking at serving large languagemodels you can fit models up to 2trillion parameters on TPU v5eand finally it's a TPUso it surely can scalein combination with our multi-slicescaling software TPU v5e can scale totens of thousands of chipsto allow you to train the largest modelsincredibly fastand on top of that all of this scalingis available on top of the same out ofthe box excela programming model thatyou are already familiar in using on thetpusso what does that mean to you threethings reallyTPU v5e allows you to train and servemodels fastermore cost efficiently and also moreproductivelynow I know all you techno files outthere are craving to see the details ofthe architectureso here they areTPU v5e is not the largest TPU thatyou've ever seen but it's certainly themost efficientin a world where we are confronted withglobal AI chip shortageswe've decided to think differently aboutthe architecture and how we serve ourcustomerswe've chosen to make every chip countso TPU v5e while not the largest chipit surely packs a punch every singlechip carries almost 400 trillionoperations per second of AI performanceand in combination with its incrediblyfast interconnect of up to 1600 gigabitper second we can have up to 256 ofthese chips work together as oneand powering further powered by ourintelligent Jupiter data center Networkthat George talked about earlier at 6.4terabits per second per podwe can string together hundreds of thesepods to enable tens of thousands ofchips to work together to train a singlejobthis elegant architecture allows us tooffer Cloud TPU v5e at an incrediblestarting price of just 54 cents per chiphourand starting today TPU v5e is availablein our North American region withavailability in APAC and Emi are comingsoon later this yearin fact if you go to the hardware Warsyou can check out the full rack of TPUv5e that we are displaying thereto enable our customers to take fulladvantage of TPU architecture we've alsoinvested heavily in the complementarysoftware stackyou can use all the popular Frameworkslike tensorflow Jacks and pytorch toboth train and serve your models now ontpus this allows you to have oneplatform for your end-to-end training toserving Pipelinesand secondly we are bringing two newscaling Technologies first ismulti-slice training now training modelsacross thousands of chips is extremelycomplex you need to distribute themodels and do parallel training thereare different kinds of parallelismtechniques like data parallelism andmodel parallelism so our technologymulti-slice training simplifies all ofthese parallelism techniques and enablesyou to scale your jobs to tens ofthousands of chips the second technologyis multi-host survey because some of thelarger models and state-of-the-artmodels are so large they cannot fit thememory capacity of a single chip so theyneed to be distributed to multiple chipsand multi-host serving allowsDistributing models to multiple chipswhile enabling efficient survey and bothof these Technologies are available ontop of our familiar out of the box xlabased programming modeland last but not leastone of the very highly requestedfeatures and capabilities by ourcustomerswe are bringing native integration withGoogle kubernetes engine for tpusso you can now seamlessly orchestrateyour applications whether you're runningserving or you're running your trainingpipelines across tens of thousands ofchipsand if you want to do fully managedtraining and inference pipelines you canuse our vertex AI Integrationsthis combination of a very eleganthardware and out of the box softwarebasically means that you can have yourdevelopers be very productive and youcan deploy latest state-of-the-artmodels while also being cost efficientnow if you take the TPU v5e for a spinwhat do you expecta bunch of bar chartsbut I'll make it easy there are threebarsfor every application in this chart andwe are basically comparing TPU V4 withv5e and we've normalized to Performanceper dollarso if you're training using the common16-bit B float 16 Precision for a modelof size 32 billion parameter you canexpect up to two times more performanceper dollar compared to our lastgeneration TPU V4 and if you're traininga larger175 billion parameter model you canexpect up to 50 percent more performanceper dollarand using a new technique that we areintroducing called accurate quantizedtraining or aqt you can actuallyleverage eight bit integer Precision tofurther boost this training performanceby up to 35 percentthe second chart basically shows what toexpect when you scale these applicationsacross tens of thousands of chips thereare two lines on this chart and we haveto be very careful to choose the colorsof these two lines because otherwisethey would look indistinguishableone lineis the perfect scaly and another line ismeasuring what we expect you would getas the performance when you scale yourjobsin combination with the multi-slicetraining that I talked about earlier youcan expect nearly linear scaling ofperformance of your applications and sowhat does that really meanmodel training that used to take monthscan now be done in weeksand while you're accelerating the timeto train these models you are alsosaving moneynow here are some more bar charts youknow you take TPU for a spin and itspits out a lot of bar chartsuh this time this is inference andthankfully there are only two bars forevery application so we're justcomparing uh TPU V4 with v5e and doingperformance per dollar the chart on yourleft basically you're seeing uhperformance across the range of some ofthe most popular large language modelsmeta recently released llama 2 modelsand so you're seeing on the range ofthese model llama 2 based models you canexpect up to 2.5 times more performancefor every dollar when running inferenceusing TPU v5e also there is a stablediffusion in there which is used forgenerating images and this GPT 3 175bbut while you're getting this amazingperformance to price efficiency you'renot compromising on latency in fact TPUv5e enables all of these complex modelsto be also inferred incredibly fasterproviding up to 70 percent faster speedupso in a nutshell what does all of youknow the things that I talked about inthe TPU section trance how does thattranslate to you three takeaways firsttpus are a legitimate platform to serveyour generative AI applicationssecondly tpus are designed to deliververy high performance per dollarthird the learning curve on tpus is verysmall because you can use the same toolsthat you are used to using fordeveloping your application and get outof the box performance from tpusso it's no wonder that our customers arereally excited about tpusentropic is looking forward tobenefiting from the price performance tofuel their next wave of AIcrayon is able to train their modelsfaster and more cost effectivelygrid space is seeing a 6X higherperformanceImprovement compared to the priorgeneration they were using for speechrecognition and emotion predictionmodelsand to talk more about the experiencecharacter AI is having it's my honor andprivilege to welcome mile Ott foundingresearcher of character AIhi everyoneumI'm miles a founding researcher atcharacter AIum so character is a full stack directto Consumer social AI platform that letsanyone create and interact with lifelikeAI characterssince launching our product about a yearago we've seen tremendous growth andreally creativity from our community ofusers on how they use character with usecases ranging from creative writing helpto language learning to entertainmentand role playing and even sometimesemotional support and friendshipso today the character AI Community has20 million users and millions of thoseusers sign on daily and and use theapplication for on average more than twohours per dayand so that's really a testament to howmuch value our product is providing tousersuh under the hood our AI characters arepowered by large language models uh andso at character we pride ourselves onbeing full stack and so what that meansis we train our large language modelsourselves in-house from scratch and weserve them to our community at scale andso that introduces really interestingscaling challengesand so today I want to talk a little bitabout uh some of our recent experiencescaling up those workloads and why I'mso excited to be doing that work on gcpokay so historically characters trainedwe've trained our language models onlarge GPU clusters and these trainingruns will often consume thousands of GPUaccelerators for a month or longerand because of that we've spent a lot ofeffort optimizing this workloadincluding really low level detailseverything fromwriting custom Cuda kernels tooptimizing the distributive collectivesthat we're using during trainingummore recently we've explored you knowwe've asked basically can we train theselanguage models on cloud tpus uh andwe've been really just blown away withhow quickly we've been able to onboardon this new platform uh and start usingtpus productively so in under a month wewere able to Port our highly optimizedPi torch code base into Jax and startusing uh many smaller slices of tpus inconcert with a technology calledmulti-slice xla which allowed us toscale up much faster than waiting for asingle large contiguous block of computeto become availableum on top of that uh tpus integratenatively with gke and allowed us to kindof reuse a lot of our existingkubernetes based tooling and so that'sallowed us to really build a kind ofuniform platform across gpus and tpusfor our researchers to usemy favorite part of the cloudis the flexibility that it providesum because we can use either gpus ortpus for our workloads we can also startto map our workloads to the platformthat best uh you know is is that is bestfor that workloadum and it's still early days but we'refinding that you know gpus are great foruh you know High throughput applicationssettings where you really you know wantto squeeze every little bit ofperformance out of the chips and arewilling to do kind of these low-leveloptimizations things like writing customCuda kernels and like thattpus on the other hand with their veryfast interconnects have been great forapplications like low latency inferenceand use cases where you may not want toreally kind of prematurely optimize andyou just want kind of decent performanceout of the box so research anddevelopment kind of applicationsum we're also really excited aboutnext-gen accelerators and we heard alittle bit about those earlier today butthe h100s and v5eum these accelerators offer a lot of youknow technical improvements but one thatwe're really excited about is bothplatform support now native 8-bittraining recipes so on tpus we're goingto have aqt with in date and on h100swe're going to have fp8 with Transformerengineum all together we're finding an earlytest that these new chips are going toprovide you know roughly 2x betterperformance per dollar and that's reallyexciting for us because it meanstraining language models is going to beeasier and cheaper than it's ever beenbefore and especially as we look toGrowing character to hundreds ofmillions and billions of users in thefuture on Google Cloud we're reallyexcited about thisso with that I'll pass back to paresh toclose us out and thank you[Applause]you just heard from mile firsthandhow character is able to use both ourcloud gpus and Cloud tpus to power theirapplicationsthis is our superpowerwe are the only Cloud on the planetto offer two incredibly compellingplatforms for doing your AI workloadsand we offer a whole range of instancesso you can fit the right choice instancebased on your workload whether you arelooking to inference on the small modelsor you're looking to train the mostmassive ones Google cloud has just theright choice for youforeign"
}