{
    "title": "Planet-scale storage purpose-built for AI and cloud-first workloads",
    "presentation_type": "Breakout",
    "categories": [
        "Architects, IT Professionals",
        "ARC218"
    ],
    "video_id": "JuQqL-XnGPY",
    "time": "Aug 29 04:45 PM - 05:30 PM CDT",
    "transcript": "foreign[Music]welcome to architecture session 218.this is uh about the Google CloudStorage also commonly known as the GCSwhich is the planet scale storageOptimizer for AI workload and the cloudfirst workloadmy name is Jason Wu I lead the cloudstorage product team joining me today wealso have a SRI Devi who is the seniorengineering director for cloud storagewe're also very excited to have threedistinguished customers joining us todaysearching from PricelineJack from neuro and ugor from codewaywe have a very packed agenda today Iwill first give a little context aboutthe cloud storage why choose GoogleCloud Storagethen we'll move on to some of the newproduct announcements we haveafter that we'll have customers coveringsharing about how they're buildingInnovative AI Solutions on top of cloudstoragefrom large-scale AIML data pipelines togen AI model training and serving on topof the cloud storageto Enterprise AI with a delightfulcustomer experiencewe will share about our future productdivision for cloud storage with GeneralAIand at the end we'll share some timewe'll have some time for Q A's as wellall right so what is the cloud storage aquick primer cloud storage also known asa GCS it's it is uh highly availablecost effective durable and simple to useobject storagemany customers using cloud storage for astore and manage large amount ofunstructured datathere are few key differentiators forGoogle Cloud Storagethe first one I'd like to highlight issimplicitywe offer four different storage classesfrom standard all the way to Archiveall these storage classes they share thesame set of API and provide instanceaccess even for accessing archivestoragethis is very different from a lot of thearchive storage products out there inthe market which oftentimes requirescustomers to rehydrate the data and thenyou can access the data which normallytakes hours to do it and for us it's allinstant access with the same API so thisdramatically simplifies the applicationdevelopmenteven for some of the very Advancedcapabilities such as Geo redundancy andyou know business continuity type ofcapabilityGoogle Cloud Storage offers a dualregion and multi-region product which weoffer to store two copies of your dataover two or more of the differentregionswith that kind of a multiple copy of thedata we still provide a single Globalendpoint with a strong consistency aswell as with automatic request routingand failover failback so application caninteract with the Dual region andmulti-region product the same as theirinteracting with the original bucketreliability is another area which issuper critical to Enterprise customerswith a class of storage dual region andthe market region will offer the highestavailability SRA for both read and writewith the Dual region and the optionalturbo replication capability we alsooffers the best RPO and RTO in theindustry with a 15 minutes guaranteedRPO and zero minutes for RTOclosed storage is also optimized fordata intensive workloads such as Ai andanalytics which is a high throughput andQPSwe also offers incredible scalability wehave customers today running expabytesof data with the trillions of objects oreven from a single bucketand cloud storage is uhbuild with a secure by Design to beginwith and which offers a wide variety ofsecurity capability to allow ourcustomers to meet their security andcompliance requirementthere are many use cases cloud storageis well suited forparticularly for AI workload in the lastcouple of years we have seen thisdramatic increase of use of cloudstorage for AI workloadwith this uhhigh performance programmability andmanageability which make cloud storagewell suited for these gen AI workloadboth full model training as well as formodel servingrunning data intensive analyticsworkload is another common use case wehave customers using cloud storage forlarge-scale data processing and alarge-scale data analytics where theycan get the high throughput and the lowlatency out of their datawe have some world's largest contentproviders using cloud storage forstoring their content and serving theircontent from cloud storageand backup and data protection hasalways been a major use case for cloudstorage as well especially with the lowarchive storage price combined withinstant data access all the time make itideal for a lot of the disaster recoveryand the data backup purposeso next slide I'm going to cover a lotof the new capabilities we're launchingto make the cloud storage you know andyou love even betterwe have spent a lot of time to optimizestorage optimize the cloud storage forthe AI and analytics workloadin the areas of programmability we'relaunching cloud storage Fields withtransfer for hdfsas well as a pop sub to cloud storagesubscriptionscloud storage diffuse allows customersto mount and access their data from aGCS Bucket from a cloud storage bucketas a local file systemit's also fully integrated with gke sothat's with this it dramaticallysimplifies the AI workflow for trainingand inferencingin the areas of our performance we'reintroducinga newer cache with a as well as a grpcAPI and the performance Improvement forHadoop connectora newer cache is a zono SSD based cacheSSD based read cache which is designedfor data intensive workloadcombining anywhere cache with our uniqueoffering of matter region customer canget this ultimate data placementflexibility as well as the ability toco-locate the data with their computeresources in anywhere in the marketregionso this offers an ideal data storageinfrastructure for a lot of thecustomers building a single unified lakehouse on top of itin the areas of manageability wherelaunching manager folders as well asevent-driven transfermanaged folders allows the customer togroup a set of objects within a bucketand set permissions and access on top ofon that group so this simplifies a lotof the data management for AI andanalytics workloadin addition we continue to invest tomake cloud storage even better forEnterprise and Mission criticalworkloadsfor intelligent storage we areintroducing storage inside the data setwhich allows customer to query theirstorage object metadataget insight directly using bigquerywith the auto class which helps thecustomer to manage their storage anddoing cost optimization based on howfrequent the data get accessed whileexpanding Auto class to include all theyour data including all the existingbuckets for where you have your datastoredwe're also introducing a number ofsecurity and compliance controls to helpour customers to better meet theirsecurity and compliance requirementand the last but not the least wecontinue to make the already awesomereliability and scalability even betterwe're launching soft delete replicationmonitoring as well as the customer dualregions for both Canada and Australiainsoft delete is a key capability to helpcustomers protect themselves from thisaccidental operation errorsI think many of us probably had theexperience where we accidentally deletedsome of the objects and then onlyrealized that a couple of days laterwith soft delete you will be able torecover those deleted objects veryeasilyI know that I run through a large numberof new product announcements veryquickly a lot of these are new productsare available today and you can try itand you can look into that now and someof them will become available in themonths aheadin the next few days we'll share moredetails about each of those features andwe're very much looking forward toworking with you and having you try themoutwith that I would invite Jack to the tothe stage Jack is going to share howthey are buildingum Innovative Ai and autonomous drivingSolutions on top of cloud storage Jackthank you okay thank you thank you Jason[Applause]I'm head of autonomy infrastructure atneural so today I'm going to talk aboutwhat data and AI as scale and Euro lookslike what the challenges that weencounter and how we partner with GoogleCloud Storage to resolve those some ofthose challengesso a neural our mission is to bettereveryone's life with robotics and we'rebuilding autonomous driving technologyas well as the customer robots for goodsdeliverywe have been partnering with GoogleCloud for more than five years we're aheavy user of cloud storage computeengine and gke so my work for my Org thegoals is to accelerate the developmentof autonomous driving technology withthe ml first strategy with a focus ondeveloper productivity cost optimizationand operational efficiencyso what does data and AI s scale looklike so basically in long story shortwe're dealing with massive amount ofdata so there are a few sources of datalike one major source is the on-row datacollection we have a fleet of vehiclesand robots that's driving on the roadand collect a lot of data and we havehigh resolution sensors like cammultiple cameras like lidars and Radarscontinuously collecting data at aboutyou know a few hundred gigabytes perhour of driving so you can imagine theamount of data quickly accumulatesand we don't want to throw away any ofour data because you know we identifythe failure modes of the autonomousautonomous technology and we want to dosome data mining to mine someinteresting scenes from the corporativedata that we collect in order to usethem for training or evaluation purposethe second source of data coming fromsimulation so simulation is a digitaltwin of unreal testing so we usesimulation extensively to test ourautonomy software before we put them onRoadso we're talking about you know exabyteof scale of data here and when we runlarge-scale simulation we're oftendealing withterabits per seconds of bandwidthum so when you're dealing with massiveamount of data we encounter challengeslike the cost of storing all this dataand also like the bandwidth throughputfor some large workload like simulationand some security side of considerationslike security and governanceso how do we partner with Google CloudStorage to to solve these problems sobefore we we partner with Google Cloudwe have on-prem data center that westore our data and constantly we have tofight with ever growing the amount ofstorage and also reliability issues sowhy we migrate everything to GoogleCloud you know reliability is isextremely good and we don't need toworry about it and the the scalabilityis also also great at a good causticfactor in thisumuh the the other thing I think GoogleCloud did really well is a really gooddeveloper experience it provides a veryunified API it's very easy to work withso what are the features we're lookingforward for cloud storage so first oneis auto class as Jason just introducedso Auto class is a way to automaticmedically manage the lifecycle of dataand you know move them to cheaperstorage layers so we don't need to spenda ton of engineering effort to optimizethe life cycle of the data and actuallyspend more useful engine resources ondeveloping our autonomous drivingtechnology and the second feature isanywhere cache so we want to bring ourdata close to our the GPU compute sothat we are not bottleneck on the thedata I O and have a high GPU utilizationso the third feature is managed foldersso we want finer A fine grain control ofthe security uh fine grain AccessControl at below Bucket Level so superexcited and look forward to a lot offeatures that GCS offers for usso next I'm going to hand over to Uberat co-way to talk about jnai and carstorageforeign check thanks Jackhello everyone thanks for joining myname is and I'm working as a lead devopsengineer at codeway so I will be sharinguh our experiences with Google CloudStorage with the integration of GCSviews on top of GK todayso codeway is a startup company we werefounded back in 2020 so we are likethree years old company but we alreadyscaled our products across like 125million users all around the world andwe mainly focus on ai ai products thesole purpose of our company is providingthese models as much as as possibleglobally by productionizing it givingsome unique unique productsand Google cloud is really our maininfrastructure provider and since we aredealing with uh Big Data uh with acrossdifferent products uh also we aredealing with different AI models wereally don't want the data to be anotherchallenge for us so this is why from theday one we're always using the cloudstorage but today's example will be on aspecific product called voy which hasthe GCS fuse integration with itso this specific use case is for the foran application called voi and we alwaysproduce mobile applications and voi isan AI based portrait maker applicationbasically we ask some users from user weask for some headshots from differentangles different lighting conditions youknow the color conditions and we takethat from the application and then ourgke cluster basically create an uhtraining pipeline as soon as we havethese images and training pipeline ishandled by a specific pod in the clusterand this cluster is running of course ongke and we are using lots of spotinstances with different kind of GPUaccelerators on top of that so for thisspecific app we are mostly using a100 GPaccelerators from Nvidiaso the entire architecture is working asevent-based as soon as we have theimages we store them on GCS packet whichtriggers a trading Pipeline and as soonas training is finished we actuallycreate a unique model for each of oureach of our users so we don't have ageneric model but we are more likededicated more model for each of ourusersso where the GCS fuse at the value isactually on the step four as soon as theuser has the trained modelthey can ask some specific uh let's sayuhvisual let's say they want to seethemselves as a Greek god let's say sowe we take that as as prompt as soon asuser demands this and inference workloadstarts at this time so that means weneed to fetch this AI model and then putit on on a port for influence designbut the challenge is of course user armwants this multiple times not just onethat means we need to fetch this modelmultiple times as well so we need reallylike an abstract layer of storage sothat our stateless part can work with itif you think about the like this stepfour if you don't use the Google CloudStorage what will we do we would usepreferably acli to fetch the image likegsutol but then we we also need to knowuh we also need to Cache this data onspecific parts so this is where thechallenge startsif you want to implement this withoutthe GCS fuse we will probably use lotsof percentage volume claims that meanswe need to manage this storage as wellbut with the GCS fuse this is reallylike a distributed abstract layer and wejust ask the user's model so that we wecan run an inference and as soon as wehave this output image we publish itover the Google CD and to the end userso what is the results out of this weget uh as I said it really provides anabstract layer of storage alsoinstrumentation is very easy you justadd certain annotation to yourkubernetes manifestsand you control access on the port levelby creating an attached a serviceaccount on your pod this willimmediately inject the sidecar containerinside your application pod whichcontrols access to the Google CloudStorage itselfso you really have Port level securityuh this is also covering our securityconcerns and also this applicationactually reaches its first million userswithin six days so as a startup companyscalability is really one of thechallenges especially for the devopsteam but with the uh powerfulfunctionality with GCS views we onlyfocus on the improving the AI model wehaveand yeah with that being said I'mpassing section thank youthank youhello everyoneit's an honor to be here today speakingat Google nextmy name is Sachin Menonprivilege to serve as the vice presidentof data and marketing technology atPricelinePriceline as you may all know is theleader in online travel for over 25yearswith a mission to be one of the besttravel deal makers in the worldand this lets us fulfilla purposeof helping our customers experience themoments that matterin my capacity as head of datamy foremost aspiration is to optimizeour data platform for Speed andreliabilityconcurrently we also wanteda cutting-edge real-time data frameworkthat empowers well-informed decisionmakingand personalized used experiencefurthermoreit was critical to harness the fullpotential of our data Lake architectureenable large-scale machine learninginitiativesand drive innovationin our data platform modernizationJourneywe prioritized both proper data storagesolutionas well as efficient platform ofmanagement in terms of operations andcosta criteria focused predominantly onscalability durability and optimizationamong the options Google Cloud Storagestood out by embodying these qualitiesand seamlessly integrating with a rangeof services and processes that we wantedto accomplishas depicted in the architecture diagramon the leftGoogle Cloud Storage servesa central role as a data Lakefacilitating seamlessanalytics machine learning and crucialoperational functionsa parallel priority for us wasoptimizing costthrough integrated features like storageinsights and other classwe gain the ability to identify usagetrendsempowering informed decisions andenhanced cost efficiencylooking aheadwe are focusing on three important usecases or areas for using Enterprisestorage in Ai and Analyticswe are leveraging the integration ofvertex AIand GCF together to create apersonalized travel conciergeto harness the power of personalizationand recommendationand improve the customer experience inallwe're also using GCS and gen AIto search share and reuse internal dataassets which makes a team morecollaborative and productivethe last one the third oneGCS helps us with also the predictiveanalysiswhile maintaining the thesecurity features of certainly andproviding real-time insights giving usbetter visibility into the datathese effortshighlight that dedicationand commitment to fully leverage thecapabilities of our Enterprise storagelayer in the realm of AI analytics withthat I'll handle to 3jov for the nextEditionhi everybody I am SRI Devi I leadengineering for GCS we'll be shiftinggears a bit we have been talking aboutso far as how GCS can actually help ourgen AI customers or AIML customershere we want to talk about how maybe wecan use Janae ourselves to improvestorage sowe have been thinking about this forsome time and one of the big problemsthat we identified was storagemanagement as you know a number of ourcustomers stored billions if nottrillions of objects on GCS and storagemanagement is actually a huge problemwe haven't just announced the insightsdata sets which is like take all of ourmetadata put it in bigquery and enableyou to gather insights using SQL but canwe do more we have gener at our disposalcan we do more so one of the things thatwe have been thinking about is maybe wecan do something like national languagewhy is SQL why limit ourselves to SQLthe next one is why only metadata weshould also think about data metadatalogs your context your security and gcppoliciesthethen instead of just waiting for you toanswer ask questions we should probablybe thinking about the outcome what isthe user trying to do and kind of tryand do that instead of waiting onchecklist Etcthe other one is instead of like havingyou run all these commands can weautomate all the repetitive tasks sothat was the goal that we set ourselveswith and at this point I'd like to sharea prototype that we have built usingvertex AI model garden and the duet AIlike AI safety practices Etc again justcalling out this is proof of conceptunlike all the other launches we don'thave commitments yet but we aredefinitely looking for some feedbackum so you come into the GCS UI you seeall of your buckets and objects if Iwill try and give you a snapshot of yourproject which is relevant to you so herein this case it talks about howyou know you have your project and ithas so many objects of data Etc you sayokay tell me more now it can actuallyshow based on your data kind of hey thisis the right way to generate the chartright so kind of show where your bucketsare what are you where is your data ineach regionthen it suggests some prompts that maybe useful to you right or you can enteryour own prompt so one of the things isoh do you want to know which buckets aregrowing fastokay so you click on that then you knowit actually shows the buckets that aregrowing fast then it again suggests ohmaybe you want to know you know whichbuckets actually have no life cyclerules this is important right like ohyou're not managing your storage so youcan click on that and then it actuallysays I can actually these are thebuckets that don't have lifecycle rulesand you want to fix thatand you can suggest the command that canrun and for extra measure actually tellsyou that this has added cost right andit can actually run the command as wellif you give it the permissions so thisis kind of the experience that we areshooting forbut what was more interesting this is asyou can imagine most of the data thatDCS has GCS documentation knowledge baseEtcmost of our customers actually havecompliance rules that they have tofollow based on their own company rulesso what if you can actually upload yoursecurity requirements document and askin size gen AI how am I doing on mybucket right so then it can synthesizeyour document and look at the GCS dataand say okay based on your document thethat you have uploaded all your bucketsshould not be publicly accessible thatmeans no bucket has to be publiclyaccessible and you should have CMACprotection setup this is customermanaged encryption key Etc so itactually suggests what you should bedoing on your bucketso hopefully that kind of gets youexcited as we are excited about this andhope we can do more so we do believethat AI enabled storage is near and asyou can see one of the nice use cases isto combine all the data we have withwith us and create multi multi-term chatexperiences customized charts and codegeneration and execution this is justthe tip of the iceberg I think boardwill come as we go onall right I think we are at the end ofthe talk here hope all of you havelearned something about GCS[Music]"
}