{
    "title": "Use Looker to empower cloud financial decisions and FinOps",
    "presentation_type": "Breakout",
    "categories": [
        "Architects, IT Professionals",
        "ARC213"
    ],
    "video_id": "-hj7dLdx1CU",
    "time": "Aug 31 01:30 PM - 02:15 PM CDT",
    "transcript": "foreign[Music]welcome to using looker to empower Cloudfinancial decisionscompanies on average use four differentfin Ops tools according to a recentsurvey by the phenops foundation andthey could it could be third-partyproviders these could be homegrown ornative to the cloud providersnow when I heard this four number I wassurprised it seemed a bit low as somecompanies have told me they use 10 ormore tools to perform their fin Opsactivitiesnow a big reason behind this is thatmany tools are Point solutions theysolve for just one or two use caseslooker is a powerful platform thatsolves across numerous use cases inCloud cost management and that's whatwe're here to talk to you about todayso what we hope you'll take away fromthis session first is some exciting andpressing results from CME Group whoimplemented this solution and then we'llwalk you through the roadmap behind thatas well as building blocks of theproducts that comprise it and just anote to please save q a we have mics upfront for the endmy name is Sherry Cunningham I'm teamlead for finnops at Google Cloud I sitwithin Professional Services we workwith customers on for example developingfinops operating models implementingin-depth cost optimizations as well aslooker Cloud cost managementimplementations such as you'll see todayI'm also joined here by my colleagueRupa Patel who's a senior productmanager at Google as well as KevinKrantz director of finops who was deeplyinvolved in many of these results you'llsee todaythanks Sherryall right well welcome everybody andthank you for being here today as Sherrymentioned I'm Kevin Kranz I'm directorof finops at CME Group and I've been inthe I.T financial management space forabout 14 years and in the last year anda half I've been helping CME stand upwith finops operationand for those of you that don't know whoCME Group is we're the world's leadingderivatives Marketplace and we haveproducts like the S P 500 the Dow Jonesgold copper silver corn cattle soybeanswheat you guys get the picture we have alot of products but the reason we'rehere is to learn about the cloud and in2021 CME partnered with Googleuh to bring all of our applications tothe cloudand in 2022 we were fortunate enoughalso to partner with Google's PSO teamto help stand up our fin Ops operationso over the next 12 minutes or so I'mgoing to take you through three examplesthat CME has used looker to drive Cloudcost management and when people thinkabout bi tools like looker you tend tothink about graphics and data andmetrics and that's true that's all therebut we find at CME there's more thanthat to using a tool like looker we'rewe're finding that it helps Drive what Ilike to call the three C's of looker andthat's cost awareness collaboration andcultural changeall right the first scenario I have foryou is uh with bigquery storage so atCME one of the first things we did wasmigrate data from on-prem to bigquerystorage and when we did that the teamswere excited because you know they'regetting their hands dirty for the firsttime with big uh with gcp in general sothe uh the excitement was high and onthe fin Ops Team we were excited toobecause we're seeing data come throughthe billing file and we're also seeingreports light up in looker like you cansee on the screen herein this graphic you could see the linethe line graph that's representing thenumber of gigabytes that were beingstored in bigqueryand then the bar graph is what is thetotal cost of that storage and when welook at that number together we'reseeing the cost per gigabyte and for usthat happened to be a lot more expensivethan we originally anticipated andthat's when we knew there had to be ABetter Way Forward so from there weteamed up with Google and our internalEngineers to understand what a betterpath forward could be and that's when wefound there's two different ways tostore data in bigquery you have thelogical format in the physical formatand in the physical format you takeadvantage of Google's data compressiontechnology and lucky for us our datahappens to be highly compressible so itmade sense for us to make that changeso we simply did that and I say simplybecause I didn't do the work but the devteams were the ones who moved thebuckets from logical to physical and wesaw a 60 reduction from our initialdeployment cost and you can see thatresult clearly in this graphic where thegigabyte stored stay high and the costcomes pretty lowso I'm going to bring you back to thosethree C's that I mentioned because wesee two of them alreadywe see cost awareness based on thegraphic and we also see thecollaboration because at the end of theday the fin Ops Team can't do this workthemselves we need the partner with allof our technology staff to make surethis is you know the opportunities thatwe're seeing through these Graphics areactionable and that they won't impactperformance of our applications so thatcollaboration is extremely vital hereand the other thing that's interestinghere is the third seat cultural changeand if you are at CME what you wouldhave seen is people asking questions howdid they save the 60 what kind of dataare they seeing can we have access tolooker and see what they're seeing wewant to be a part of this movementso that was the cultural change likekind of taking place right before oureyes which was amazingthe second scenario I have for you isGCS storage tiers now I'll jump to thechase on this one we were originally inthe standard storage tier and we wentdown to the cold line tier and we saved66 percent probably no surprise toanybody here because you've probablybeen there done that but for CME it wasinteresting because as a synapsepractitioner we're watching the costrise for GCS at the same time we'reseeing cost rise in bigquery storage andwe were just curious to know why thatwould be and we just didn't know thefull architectural plan behind this sowhen we met with our Engineers we foundthat GCS was going to be a landing zonefor data being moved over to bigqueryand that at the end of the day the bigthe GCS storage would be backup storagefor for a bigqueryso once we realized it was going to be abackup storage solution we knew we couldsafely walk down those tiers so that wasa 66 reduction costbut the really cool thing here was thecultural change that took place becausewhen the engineers started feeling likethey were being pulled into the fold ofthin Ops they they started putting theirengineering hats on and started thinkingabout Automation and that's exactly whatthey did they automated our storage lifecycle policies which start automaticallywalking down those tiers of storage totake advantage of the lower cost pricesover time when they see no accesspatterns on that that data in GCSso again we could see those three C'stake place in this scenario as wellthe third scenario I have for you iscloud logging this one's a littledifferent than the first two becauseit's not a huge cost reductionhowever if you look at the first graphicon the left that's Cloud logging and youcan clearly see the spike thereand I was able to drill into that Spikeand looker and find the project ID thatwas causing the spike and I took thatinformation to the to the logs Explorerin the console and I found theapplication ID that was contributing tothat Spikeand from that I was able to find out whothe application owner was and then wehad a discussion to understand moreabout what was happening with hislogging and to all of our surprise thedevelopment team didn't even know thiskind of logging was happening and wewent when we looked in the console wesaw 1.4 billion logs being produced inan hour and that was a pretty big numberto me I didn't really have a referencefor logging but we were all surprised bythatso anyway the developer was able to gointo the system and make adjustments andyou could see the the logging Spike goawaybut we still needed to understand moreabout logging so we teamed up with ourSRE team because those those are theguys that are going to be responsiblefor kind of rolling out the loggingservice at scaleand right away the engineers beingEngineers they knew they or felt theywere being pulled into the spinops foldthey automated as well so they starteddoing anomaly detection at the log levelto hopefully prevent large loggingevents from making their way into thebilling filein showing up as real costso if you notice on the screen I havetwo graphicsone the one on the left is cloud loggingand the one on the right is bigquerystreaming inserts and if you notice thepattern on both of these is exactly thesameand that just made me curious and I hadto dig in further so I met with mydevelopment teams to understand why thismight be and that's when they informedme that were you taking all of our logsand streaming them over to bigquery sofrom a finops perspective I'm thinkingokay we're paying for logging fee we'repaying for big query streaming insertswe're paying to store the data inbigquery as well but they also told mewe're storing the same data in GCS andwhen we looked it's in the standardstorage tier so you can see we have aton of optimization on our hands and theteam is actively working on this now andactually this week they have releasedsome uh some really impressive resultson reducing all these costs for usum but again through this situation youcan see the three C's play out you havethe cost awareness through the graphicsyou have the call the collaborationbecause it takes a team of people tomake this stuff real and actionable andto be able to see the real cost savingsthat are in front of us and also youhave the cultural change which is highlyimportant so now all of our Dev teamsare being more cost aware more costconscious and they're putting theirskills to use by automating a lot ofthis for usso I'll end thereum and I just want to remind you of thethree C's because really what I'mfinding with any visualization toolreally but with looker that's what weuseum it's been impressive to see thesethree C's play out so I'm going to passit over to Sherry who's going to walk usthrough a little bit more detail on howCMU was able to stand up looker and usethe billing file to make all thispossiblethank you60 savings Kevinexcellent 60 savings kudos to the entireCME team for contributing to thatnow moving from on-prem to the cloud weknow this turns engineering andArchitectural decisions into Cloudpurchasing decisions this is what makesthat culture of finops so important andone of the biggest ways to drive thatculture is to have concrete results toshow people what the impact is of theirefforts and having these results thisdata requires a tool such as looker toget thereso starting on our crawl walk runJourney when we start with crawl werecommend starting just with one Cloudas that strong base of a platform we'lltalk about multi-cloud in a bit behindthis there's also the need to developpersonas so each Persona should viewdifferent types of data differentinsights Executives should not beviewing the same insights as costoptimization analystsit's also important to know that crawldoes not mean low impact I worked withthe company recently to implement crawlover the course of a few months and theytook their month-end billing invoiceprocess from working with various teamsto get inputs and approvals took thatfrom three weeks down to two daysalso behind the crawl phase it'scritical to have really strong metadatametadata can come in the form of tags orlabelsit's not a one-time effort it's more ofongoing automations we recommend upfront in the actual provisioning ofresources to automate with terraformtemplates or some other sort ofautomation to include those tags andlabels and then further down the streamimplementing automations for compliancechecks of that metadata within lookeryou could also create a compliancedashboard that shows teams where on theproject level on the resource levelthey're missing tagging and labelingcompliance this is just another way tomotivate teams to increase theirmetadata across the platform so that youhave better insightsnow getting into walking with lookerthis is where multi-cloud starts to comeinto play you can harmonize across theclouds to know what cloud storage is onone Cloud versus the other versus theother and later Rupert will talk aboutan exciting effort with the finopsfoundation called Focus to developbilling standards across clouds that'llmake these harmonization efforts eveneasier for youalso within walk you have more granularinsights and cost optimization such aswhat Kevin shared earlier todayand then finally you have showbacks andchargebacks many companies are doingthis manually at the end of the monthdeveloping invoices sending them out tovarious cost centers within looker youcould set up a one-time automation thatcan send monthly to however many costcenters you decide it can include bothdirect and shared service costsgetting into the Run phase of lookerthis is where you start to make the datamore opinionated you can developefficiency kpis such as what is yourcost per core or unit economics metricssuch as your cost per transaction andnot only can you have these moreopinionated metrics but you can alsodevelop benchmarks to start trackingyour progress over three months sixmonths 12 months Etc to see how you'reactually progressing as a cloud inefficiencyalso part of this run phase isforecasting we see many companies haveat least 10 percent of forecast variantsto actuals and with a tool such aslooker where you can tune models addyour own business logic you cansignificantly bring up that forecastingaccuracyand then there are Integrations withlooker you can integrate with slack jirawithin jira you can for every costoptimization recommendation create aticket that goes to the workload ownerand actually track when they're actingon cost optimization recommendations andover time see where you're having impactor notCME Group is embarking on implementingmany of these run phase items and we'rereally excited to see where they taketheir Journeyso if you're like me you've probablyspent a half a day or entire daystracking down anomalies trying todetermine why they occurred puttingtogether a plan promising why it'llnever happen againI'm excited to announce we now have acost anomaly with generative AI solutionso this is not only identifyinganomalies with our bigquery arima plusmodel but alsousing generative AI our Palm apis inorder to tell you both reasons for whythe anomaly occurred as well assuggested recommendations on how to tofix it now this is multi-cloud as wellit doesn't have to be implemented justwithin Google Cloud on Google's data youcan do it elsewhere as well so you nolonger need to be a data scientist totackle cost anomaliesnow imagine a day where there's a costanomaly and you or the workload ownergets an email saying there's been ananomaly but no fear here's likely why ithappened and how you can fix itthese are all meant to save you and yourteams time to resolution on anomaliesso now I'll hand it to Rupa to talkabout the building blocks behind thisjourneyawesome thank you so much Sherry so nowI'm going to talk a little bit about thebuilding blocks of a successfulimplementation specifically talkingabout what tools Google cloud has tooffer just a little bit of backgroundabout me I work in product managementspecifically I work on the billingconsole and our programmatic exports ofcost data to bigqueryso a little bit about nopeohsorry a little bit about our buildingblocks for a successful implementationhow do you actually build uhimplementation using Google Cloud wellthere's a couple building blocks firstand foremost bigquery bigquery is aserverless scalable solution for yourdata and I'll talk about that on thenext slide we also offer a ton ofexports so you can actually get yourCloud billing data and contractualpricing through our bigquery exports andthose that data is actually enriched anddemocratized for your organization I'lltalk a little bit about that and thenmetadata Sherry touched a little bitupon this but it's really important tomap your organizational structure to theGoogle Cloud constructs that alreadyexists so you can actually make the mostof understanding your Cloud cost dataso bigquery exports data automaticallythroughout the day so if you enablebigquery export and I'll show whatexports we actually have available in asecond you can actually see your costsautomatically throughout the day thebenefits of bigquery is it's a fullymanaged Enterprise data warehousesolution that actually lets you manageyour data and analyze it it's serverlessarchitecture makes you allows you tobasically SQL query on that cost dataand there's zero infrastructuremanagement so once you actually enableit it scales as you scale so withbigquery you can actually queryterabytes of data in seconds orpetabytes of data in minutes andbigquery is the is one of Google'sPremier Solutions for data querying andand storage so I'll talk a little bitabout how you can actually use GoogleCloud tools to analyze cloud and Cloudcosts and pricing data in a second butprimarily you can use a tool like lookerto visualize your data that sits on topof bigquery as Sherry had talked aboutpreviously so some of our exports weoffer three exports today we offer ourstandard billing export in this exportyou can get access to cost credits usageregion related data labels tags and thelist goes on projects folders Etcstandard billing export is a reallyimportant export to enable you candirectly enable it from your billingconsole in one click so prettystraightforward solution and it allowsyou automatically throughout the day tosee your costs detailed export has thesame data as Standard Export but it alsooffers granular cost details so if youthink about what is granular cost datathink about for kubernetes how would yoube able to see cluster namespace and podlevel data for compute engine how mightyou actually see VM cost level data sodetailed export actually gives youdetailed granular costsand lastly our pricing export ourpricing export allows you to getdetails about your contractual pricingso your list price your discounts Etcmore recently we just launched theability to join the detailed export andthe price export so you're able to seeprice in line with your costsso another thing is you can actuallyscan this QR code and get access to ourdocumentation which shows your schemasso you can see our schema is related toStandard Export detailed export pricingEtcone of the things that Sherry touched onpreviously was the concept of focus weare actually working with the finopsfoundation on focus and focus stands forfinops open cost and usage specificationin June we launched the V 0.5specification I helped actually corecontribute to this specification laterthis year we're going to launch theactual V1 standard so what actually is astandard well across Cloud providers AWSAzure and Google Cloud we all have ourown constructs of how weassign data right if you think aboutcost credits usage we all have differentlanguages for how you know how thesebasic con concepts are defined and sowith Focus we actually are standardizingall of these definitions across Cloudproviders so if you actually go aheadand take a look at the focus standardyou can see how we've aligned all of ourlanguages across the three major Cloudproviders Google is also a premiersponsor of the phenops foundation wework very close with the foundation onstandardizing this billing data wewe have worked on Focus for a prettylong time now so we're excited inNovember to release the the next versionof thisum one of the things I wanted to touchon related to our exports is granularcosts so I mentioned that detailedexport allows you to see resource levelcosts right sub service level costs asit relates to really really granularcostsum so later this year we're going tolaunch a couple big service services forgranular costs just this month weactually launched bigquerydata set and job level costs so you canactually go into bigquery detailedexport and see those costs and laterthis year we're going to be launching acouple new services like data flow Cloudlogging bigtable Cloud functions soyou're able to actually get granularsubservice level costs for thoseservices and in 2024 we're going tocontinue our journey in our roadmap todeliver more granular costs this isreally important as you think about theuse cases related to cost visibility andcost allocation and of course all of theother use cases that Sherry and Kevinhave mentioned related to cost anomalydetection forecasting Etc so reallyexcited to be releasing those thingsthis yearregardless of if you label it or notyou'll be able to see this data indetailed exportone thing I wanted to touch on istagging late last year in September wereleased tags tags is beneficial for acouple reasons tags first and foremosthas really strong governance so tagsallows you to actually create a tagsadmin viewer and user the reason whythat's really beneficial is if you thinkabout a developer labeling a resource adeveloper May delete that label byaccident right but if you have tags youcan actually ensure that you can onlyremove that tag off of a resource if youhave permissions to do so so thatcreates really reliable cost reportingadditionally you're able to actuallycreate tag bindings so you can only puta tag on a resource if you havepermissions to do so same thing withremovalthe another benefit of tags is you canit also has inheritance so if you assigna tag to a folder for example all thechildren resources underneath thatfolder will inherit that tag value andlastly tax has an enhanced character setso tags allows you to incorporatecharacters like the at sign or theperiod so that's really useful if youwant to put your developer's emailaddress as a tag value onto thatresource so if you want to read moreI've attached the white paper we'vewrittenabout tags and some of the benefits thatyou can actually gainforeign"
}