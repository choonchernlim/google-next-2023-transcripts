{
    "title": "What's new in Serverless Spark",
    "presentation_type": "Breakout",
    "categories": [
        "Data Analysts, Data Scientists, Data Engineers",
        "ANA204"
    ],
    "video_id": "s4fZXqj-cYs",
    "time": "Aug 30 06:45 PM - 07:30 PM CDT",
    "transcript": "foreign[Music]today we'll talk about serverless SparkI'm Abhishek kashyap I lead productmanagement for open source dataanalytics at Google cloud and I'm veryexcited to have Richard Williamson andEthan Keel from Walmart joining me todaybefore we talk about anythingI would really like to thank them fortaking the time the use case they willtalk about is very high impact and weall deal with it every dayso in terms ofand talk about many many new things thatwe are announcing today for dataproc andserverless Parkthen Richard and Ethan will follow upthey'll talk about the scale they dealwith at Walmart the use cases and howthey are using serverless part to solvethemso let's start with data blockdata clock is Google cloud servicefor open source data analyticsat scaleproviding you with the best priceperformance for your workloadswe are heavily focused on security andreliability for your open sourceworkloads so your customers who are thebusiness teams get a rock solid platformthat you have to do minimum workmanaging and is at the price performancethat fits your budgetwewe provide you a single end-to-endplatform through seamless Integrationswith vertex AI for AIML as well as withother data analytics products likedataplex and bigquery so you don't haveto do a lot of integration workin terms of what we focus on the focusfor data proc is four key areasonewe want to continue providing you thebest price performance in the industryfor spark workloadssecondWe Believe spark should be Cloud nativewith serverless execution just like dataanalysts can use SQL in a data warehousewithout worrying about VMSspark should be the same waythirdwe are going to bring spark to your datacenter because a lot of customers we seecan move 90 95 percent of their data andprocessing to the cloud but a smallsliver continues to stay in the datacenter for very very sensitive dataand finally we are continuouslyinvesting in Leading Edge security andcompliancebecause the compliance landscape isevolving very very rapidlyso let's start with price performancewell an analyst TST did a study andfound thatHadoop clusters when migrated to datablock lead to a 54 lower TCO then theHadoop clusters on-prem so this was thestarting point they compared us withother Cloud Solutions as well and foundthe number is not 54 percent but it'sanywhere between 30 to 50 percentbecause of our agent price performancethe study is about an year oldover the last year we have madesignificant improvements in priceperformanceif you think about data proc these areopen source analytics workloads runningon data in either Google Cloud Storageor on bigquery storage for both of themwe have made significant improvements in4k areasfirst one is in the spark run type weare making a lot of improvements to theexecution engine and over timecontributing them back to the opensource these are not right waysecondfor bigquery storage and GCS we havemade a lot of improvements to thosebigquery and GCS are also makingsignificant improvements to betterbetter these Spark workloadsand finally gcp is continuously evolvinghardware and we are leveraging Hardwareto get the best price performance foryounow the second area of focus which isserverless Sparkif you look at spark today thetraditional model where you create acluster and then you run jobs in it IDCfound that this requiresabout 60 percent of our developers timeto think about the infrastructure whatVM type do I need what shape do I needwill I interfere with others how muchcapacity you need etc etcifif someone thinks that you're spending alot less than 60 percent would wouldlove to know more I've actually talkedwith data scientist teams who say my jobwas supposed to be data science but hereI am thinking about VMS I need versusthe model I need to build so withserverless sparkwe want to make this a hundred percenttime on your business logic we don'tbelieve data engineers and datascientists should be spending so muchtime on infrastructure management andsizing we want you to focus on yourspark applicationand provide you with a serverlessservice where you submit yourapplication it runs you don't worryabout it and you only pay for the timethe job is going you are not buyinginfrastructure anymore you are notpaying for startup time shutdown timesyou're just having long underutilizedclusters you treat spark as an API thatwill run your applicationand architecture-wise that's how it isbuilt serverless power is an API yousubmit your code to itit will start off auto scale using smartDynamic allocation there is no yawnit will scale back down all theresources will be cleaned up after yourjob is done executingand we have also madesignificant improvements tools by Autoscaling and contributed it back to thecommunity andthis is now much much better thanyarn-based Auto scalesince this is an API you can connectfrom your interface of choice be itbigquery be it a notebook be it anorchestration platform like airflow orcomposer and it will connect to anyauxiliary service you can have datablock metastore you could have a hivemeta store in your own data center youcan connect to any of those and thisdoes workon different file formats table formatsto a optimized GCS connectors or withbigquery storage as wellforeignslide has an issueall the logos went off okaythat's right sowhat are we announcing todaythe key announcement today is for sparkfor interactive development for datasciencethe first one is what you might havealready heard in the keynote serverlessfile now supports Nvidia gpuswe are starting with L4 and a100 gpus sofor a subsidy of your workloads you canget up to a 10x speed upwe are the first serverless service toadd support for this for spark and we'llcontinue making it betterif development is that just like forbatch workload you could just submit toan API you can now start an on-demandspark development session thetraditional way where you keep a clusterup and running you access its notebookeven if it's 20 utilized at night youcannot shut it down you no longer needto do that your your data scientist dataengineer can log in through a notebookclick a button start an on-demandsession it will auto scale with an idletime to live it will be cleaned up afterthe user is done so you don't have to docost management you don't have all theseresources sitting idle and users getfully isolated environments for theirnotebooks so you don't even run intoissues of one user's codemaking the cluster out of memory so youget a much higher reliability as wellinteractive sessions are available to afullkey interfaces one we have open sourceda jupyter plugin so you can go to themarketplace and install a jupyter pluginto any jupyter Notebook on your laptopin the cloud and start working withserverless spark from right there secondvertex AI has managed notebooks you canaccess them from there and very soon weare going to be on two partner notebooksas well one being hacks and the secondbeing deep knot so you can go there andyou serve on the spark as wellokay so here's a quick demo this is thisis a user on their laptop they're justlaunching Jupiter labyou get to Jupiter lab you can see alist of data clusters you can connect toor you can start an on-demand serverlessspark session you can Define templatesfor the users to use it will createinfrastructure in the back end on demandyou can start a notebook start codingagainst it you are on your local Jupiterbut each cell is being executed on thishighly parallel infrastructure that autoscales based on spark Dynamic allocationin this example we are analyzing someweather dataonce the user is donethe this is all cleaned up with thetitle as I mentioned and you can workwith python SQL scholar and R throughthese sessionsso this is available in public previewtoday will be GA in about a month so youcan go try this outforeignplatform the most common one we see isairflow we are managed air flow Servicescomposer today we are announcing a stepchange in how composer is used we areannouncing serverless composeryouso far you had to worry about a lot ofpieces a lot of IP addresses to usecomposer with serverless composer welaunch everything in attendant projectspin up all the services that composerneeds behind the scenes you don't haveto worry about it so your users can justfocus on that ads we take care ofmanaging airflow for you managing allthe services needed for you to run thisreliable orchestration serviceand you get features like this willconnect to your VPC with a single clickthere is no need to use a range of ipsanymore it will use a maximum of twointernal IPS so with serverless composerthis will make your orchestrationpipelines a lot easier to manageforeign[Applause]some of you may be aware we have ananthos based Appliance called Googledistributed cloudthe idea is that if you have certainworkloads that need to stay in your datacenter then this gives you uhan environment so you can standardizeServices across gcp and your data centerso you don't have to maintain twoseparate stacks and operate them nowthis has been available generallyavailable what we are announcing todayis the availability of spark throughdata proc on GDC Edgeso let's say you have sensitive datathat cannot leave your data centeryou start using Cloud for 95 percent ofyour data processing you can use thesame spark applications deploy it on GDCEdge in your own Data Center and havevery low operational costs in managingspark across two environments such asgcp and your data center now this thisis a full stack it comes with all theauxiliary Services you need like gcpimor logging monitoring so you can justinstall it and start using spark thesame way as you do on cloud this isavailable in private preview and you cantalk to your sales rep if you areinterested[Applause]a quick view on security and reliabilityI know this is not something that soundsvery cool but this is very veryimportant for a lot of our customers wehave very very Advanced securityfeatures likeencryption using your keys but using anexternal key manager and requiringaccess justification for key accesssimilarly you can control who supportsyou who sees your logs you can enforceour policies that encryption Keys mustbe used and you can connect to your ownID like active directory versus usinggcpin and finally for data residency wenow support it at West in transit and inuse so data block is now available to beused in a lot of places where compliancerequirements are very highthe next announcement we have is thatdataplex which is our governanceplatform now supports table levellineage for spark as wellso if you use multiple gcp Services youcan go to dataplex and see lineage forbigquery for data flow and now Spark soyou have a single place where you cansee all lineage and this works not justfor SQL but for spark in other languagesas wellandso yeah I have to go really fast to giveall the time to Walmart so we are happyto talk about these offlineif you deal with stockouts we have a newfeature for data block where you canDefine multiple VM shapesand when we create a data cluster orAuto scale it we'll pick the best onebased on availabilityso in peak times when sometimes theseoperations may have caused reliabilityissues with flexible VMS you will beable to get a much will more reliablefirms to give you an example let's sayyou wanted High memory machines but Ramwas in shortage you also provide us witha shape with smaller Ram we will usethat shape and make it happen for youthis this is available in public previewyou can start using it today as welland the last onelast one metastore which weare announcing to be availablefor active active disaster recovery soyou can now deploy date of birth metastore in multiple regions in an activeactive configurationit will sustain a regional outagebecause metadata is critical yourapplications will continue running itcomes with horizontal scaling so withoutany downtime you can scale it up anddown depending on your workload and itsupports fine grained access control sorather than having many many meta storesyou can have a single one and you cancontrol access among your users andbusiness unitsand with that I would like to inviteRichard to kick it off for Walmart thankyou Richard[Applause]thank you Abby it's been a pleasureworking with you and your team over thelast couple of years to scale out Apachespark for Walmart's use casesand great to see everyone here I leadthe applied AI end to end machinelearning engineering team for Walmartwe support over 100 data scientistsrunning lots of models on Apache sparkfor usI started with Walmart in 96 as part ofthe first AI team within Walmart so I'veseen lots of growth in the AI space overthe yearsthe first half of my career was spent asa data scientist writing optimizationand forecasting modelsand then in 2009 I shifted Focus tobuild out the first Hadoop clusters atWalmart and since then I've been helpingdata scientists scale their machinelearning problems ever sinceSo today we're going to talk aboutserverless architecture for API drivenworkflowsbut first just a bit about Walmartas our customers shopping patterns havechanged we've been quickly adapting tohelp serve our customers betterwe understand now that our customerstime is more valuable than ever so we'veintroduced things like online pickup anddelivery within our stores and scanningat Sam's so you can walk right out withthe without going to a registerso these are just a few of the wayswe've been enabling our customers tosave timewith the tremendous business scale thatwe have at Walmart which you can seefrom this slide we obviously have tobuild uh technology platforms at massivescaleour team currently focuses on Walmart uswhich has over 5000 stores with a broaditem selection so you can quickly seehow the scale matters[Music]open source platforms like Apache sparkand Apache Hadoop have helped usmitigate vendor lock-in for over adecade nowthat's allowed our business to scale forthese needswe now run concurrent models usinghundreds of thousands of course and wework with tables up to 100 trillion rowsthat's 18 petabytes in a single tableour area is platform of choice for thismassive scale is Apache spark and wehave leveraged gcp spark serverless overthe last two years since it was inpreview modeI've been very thankful to obby's teamwe actually went to production right asit went to GA with with our first usecase and we've had no downtime from itour architecture Team Works across Fivemlor Pillars supporting various businessdomains from supply chain to Merchantmerchandisingand we built this end-to-end modelserving solutionwe've been teaching our data scientiststo start using pi Spark versus python uhright from the get-go so they don't haveto translate laterwe use the flexible architecturetemplate which you can see here ourinfrastructure supports now nearreal-time or models which do not followthe typical model training and inferencestructureso each model is computed dynamicallyand results are obtained at the end ofThe Runso if you look at the diagram hereyou'll see the application API we'llcall our integration API Hub and thenfrom there we'll pick the specificplug-in those plugins will kick off aspark serverless job and return theresults back to the businessthe serverless architecture has allowedus instead of relying on the one clustermini jobs problem of having contentionacross those jobsto now just have a single scalableplatform in two Dimensions so basicallywe can scale the number of concurrentjobs easily and for each job we canscale out to thousands of course per job[Music]and then here's how we think about ourmle development life cycle so as you sawfrom from Bobby's slide on theinteractive sessions we've been usingsome of those in preview so some of ourdata scientists will go use thosesessions others will be working on alocal Jupiter notebook the key obviouslyis to give them a clean slate where theycan startdeveloping without having lots ofconstraints from from Techum and then within there they'll buildout their data frames from either csvsor bigquery and then once they're readyto move to production we will help themwithin our environment to scale thosealgorithms to Pi smart using things likethe pandas UDFand I think this architecture will makea lot more sense if I turn it over toour team member Ethan Keel to show a usecase[Music]hello everyone my name is Ethan Keel andI'm a senior data scientist on Richard'sapplied AI architecture team I've beenwith Walmart for the past two years andin that time we've seen our ml Ops andmle pipelines grow exponentiallyand today I want to help show howserverless spark was able to drive thatgrowthI have the great privilege to talk aboutone of our Flagship algorithms that Ihave worked on for the past two yearscalled store assortment optimization itrecently won the informs prize for uhoperations excellenceand from a business perspective it is atool that delivers a locally optimizedspace aware assortment recommendationand what that means is we are actuallyoptimizing the number and type of itemsin a store based on merchandisinginformation and forecast datayou can imagine how this is critical toWalmart's mission of serving ourcustomers the items they wantat the price they deserve so they canlive betterspecifically where store Summitassortment optimization is used is withour merchants and they usually use threeadjectives to describe his algorithmfast transparent and strategicit is fast it has to be able to deliveran assortment recommendation in lessthan an hour transparent we have to beable to show how our current storesOptima our our current stores assortmentcompares to our optimized assortmentand strategicon top of all this it has to be able toadhere to business and Supply supplychain constraintson the diagram on the right you can seehow Merchants think about implementing anew assortment with this toolgoing from inputting your data and thenjust seeing an optimized assortment andthen comparing afterwards and under thehood you can see we are using forecastsand calculating incrementality which isthe uniqueness of an item applyingconstraints and then running our modelnow here comes the overwhelming partto highlight just how overwhelming someof these slas were I decided to just puta ton of words on this slide and try toenforce you to read them in a minute 30or lessumso from our perspective a storeassortment optimization is a batchoperations research Optimizer what thatmeans is we are using a batch of storesand we're trying to calculate theassortment for every store individuallyit has to be able to scale to 4 600storesand a hundred thousand items per storerun under one hour support up to 100simultaneous requestsand all the while minimize our costsyou can imagine how much my colleagueson ours were sweating when we uh when weread thisum butreally serverless spark saved us quite abit here it enabled us to stick to ourprinciples of everyday low pricesdeliver that time optimized resultdeliver persistent results with thebigquery connectorand the GCS connector and then it isscalable in two Dimensions one from amodel level where we are able toscale one run to All 4 600 stores andthen two enable a high throughput ofrequests at the same timeforeignso as Richard was talking aboutintegration Hubum you know we really framed that ideaaround a clean architecture plug-inadapter type architecturebut it also adheres to uh MichaelScott's keep it simple stupidmethodology and I'll show you how itdoes thatstarting at the bottom left corner weare getting a request from our businesspartners for an optimizationthis request contains details of what wewant optimized and all of themerchandising data we may needthat request then goes to our servicemesh layer called integration Hub thatis built in-house andwhich routes the request to the correctpluginthat plug-in logic then uses ourMerchant information to dynamically setthe pi spark configurationsrequired and able to meet all of ourslasonce we finish once that plug-indynamically sets those configurations itposts that information up to our firstfeature store which contains ourMerchant metadata our PI sparkconfigurations and the third thing Ihaven't talked about yet are modelrepository integration Hub does notactually post the repository the datascience team in our CI CD pipelines areresponsible for that but it isnonetheless included in our inputfeature storeonce our metadata is pushed up to thatGCS bucketintegration Hub then sends a restfulrequest to the serverless dataprocbatches API with the metadata URLrequired for that runand I'll go a little bit more intodetail about what's actually happeningin this component in the next slide butfor now you can kind of think about itas both an ETL Pipeline and our modelserver in pipelinefinally when our model finishes it postsall of our pre-processed data our outputdata and then enables our end users tolook at dashboard views of our resultsanddecide if they want to use thatassortmentdata scientists often also use thisinformation for that cicd Pipeline andto identify anything that may have gonewrong in the Runso now I'm digging a little deeper intothat serverless dataproc componentuh serverless the the badge itself canbe thought of as an individual componentof a model serving pipelineor as the entire end-to-end servinginfrastructurethe diagram on the right illustrates howthis is possiblein the topmost layer we have ourpre-processing data ETLso this is a pipeline that leverages PiSparks scalability and distribution tomost efficientlyperform our ETL functionsnow the second layer is where it gets alittle bit more interesting that secondlayer can go one of two ways we caneither use the right pathway which uhleverages the exact same Pi sparkconfigurations as that first ETLpipeline or on the left side we canactually trigger a new batch that hasnew Pi spark configurations and enablesus to maybe build bigger models thatcould not use the same Pi sparkconfigurations as a typical ETL pipelineone example of this is going from havinga CPU handle one taskto having 16 CPUs handle one task forone modeland then that third layer is exactly thesame as that pre-processing layer theonly thing different again here iswhether or not it is using the pi sparkconfigurations from that previous layerthis architecture has enabled us to hosta wide variety of operations researchmodels scaling from 100 000 variablesall the way up to 13.5 million which isa huge feat considering this all needsto be done under an hourone amazing component about serverlessspark is its horizontal scalabilitythat enables us to get that timeefficiency and that cost efficiencycomponent that Walmart really needs inorder to operate efficientlyfor one job we can see a range of two tosix thousand CPUs being dynamicallyscaled based on the various requirementsthat integration Hub has provided forthat jobso now I would like to go into you knowsome of the benefits that we havereceived from transitioning to thismodel serving infrastructure someimmediate benefits cost savings huge wewere able to radically change our costmodel for serving operations researchmodelsthe second one infrastructurereliability we with a static approach weoften faced lots of downtime having tomaintain our clusters and just reallywe're not able to keep a 24 hourreliability that we that our Merchantsthought[Applause]and then fourthan increased request throughput in ourprevious architecture the staticdataproc clustered architecture we wereonly able to serve about 12 models at atimewe have now increased that to about 100models simultaneouslysome benefits that were realized overtime one of my favorite reduced carbonoutput the ephemeral nature of theseclusters really has enabled us to bemore conscious of how we are using ourserversthe second one supercharged Sparkagain that ephemeral nature is huge inbeing able to transition our PI sparkconfigurations for to use more disk andless RAM for super heavy uh memoryoperations for example we once had agreedy algorithm that had you know 50window operations 50 Group by statementsand we were running out of memory everysingle job and as soon as we were ableto start using that checkpoint withserverless spark everything startedworking and that's really difficult touse with the static clusterthird eliminated on-call infrastructuresupport hugeI can't tell you how many more hours ofsleep I've gainedand finally the scalability of ourcontinuous uh integration andImprovement pipelinewith the development of our interactivesessions uh thanks to ABI we have beenable to radically change how we thinkabout actually developing large-scalemodels and ETL pipelines and itintegrates seamlessly with the actualserverless data proc batchesso what's next for dataproc at Walmartwe really are excited to Leverage TheGPU accelerators on serverless dataprocthis will help a ton both in our PIspark operations but also if we evermigrated our models to using some of thepi spark native languagessecond standardizing this serverlessarchitecture as we have built outintegration Hub and store assortmentoptimization was really our first usecase we've onboarded you know about adozen new models onto the samearchitecture but we've got dozens moreto goand then third applying spark lensinsights intohow we think aboutour PI spark configurations it's reallyuseful for setting a correct Dynamicallocation ratio or for determining themaximum number of executors for your SLAat the end of the day like I said it'sall like from an ml Ops perspective it'sall about the slas and so having thatinformation is always helpfuland finally our applied architectureteam is hiring please uh yeah pleasereach out if you're interestedum and I would like to invite my myfellow speakers back on stage uh yourfeedback is greatly appreciated thankyou[Applause][Music]"
}