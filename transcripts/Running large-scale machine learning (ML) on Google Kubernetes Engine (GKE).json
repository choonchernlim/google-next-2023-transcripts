{
    "title": "Running large-scale machine learning (ML) on Google Kubernetes Engine (GKE)",
    "presentation_type": "Breakout",
    "categories": [
        "Application Developers",
        "DEV304"
    ],
    "video_id": "CxemKHmRtsc",
    "time": "Aug 30 01:45 PM - 02:30 PM CDT",
    "transcript": "foreign[Music]large-scale training and inference on GQand I'm Alex zachanov I'm a director ofengineering for Google kubernetes engineteamand with me is Nathan who works on aproduct site and we'll be talking bothoverview of why you run large-scaletraining coincidenceas well is a concrete new developmentsthat we providing in the product toenable this functionality firstmentioned gke is a key part of Google AIml infrastructure stackand that reflected in growth that we seein the space and in our customerexperience so just over the last12 months we saw over 200 percent growthin workload running inference andworkloads running training on gke sowhich is a tremendous growth and thatgrows on like really big base it's allkind of fun can I share the exact basebutindustry breaking type of growthand if we look on the usage usage comein from all segments from Oggy customersfrom small to the largest onesbut the key thing that we see prettymuch all our largest customersusing gke for large model trainingand as well as a running large modelinferenceso what's unique challenges that we areexperiencing the industrywe see tremendous growth in the modelsizesgraph represents growthgpt3 this Jeep T4 Visa Palm this llamamodels and that growth doesn't stopwhy that growth happening because as weincrease the model sizewe seefundamental shifts in the modelcapabilitiesso we're going to be on the trend for awhile like what exactly will happenwhere exactly the trend will stop andwe'll go to a more plateaustill to be seen But at current momentwinds that rapid growth phaseandeach next layer unlocks your businesspossibilities for our customersand why our customers use gkey likewhat's behind thatfirst and most important as we're goingthrough that fast scaling phaseyou need to have a platform that bothcan scale dynamically with you to reallyhigh limits but also do it in a costefficient waybecausefor the large models training extremelyexpensive influences also extremelyexpensive so unless you have a solidplatform that provides all thefoundation for performance and priceperformanceit can be really dangerous from ahealthy business standpointso secondkey thing is we're in a rapidly changingMarketso in a rapidly changing Market yourtime to Market time to value for yourcustomers is super critical so one ofthe most important things is how fastyou can bring new capabilities to yourcustomers and how fast you can validatethose new capabilitiessothis G key we provide a standardplatform that spans across all yourapplication types that spans across yourtraditional web apps stateful apps batchapplicationsandnow bringing EML training and inferenceof topology key what it means they canreuse all the best practices you use allthe policies that you have within theorganization for what it takes toprepare application for production solet's see your one standard platformthat provides fast time to Market with afor Enterprise radio applicationsthe last key point is EA email is stillextremely new spaceand what it means that we'll seesignificant term in Tooling in ecosystemin the next couple years like you seenew Frameworks new offerings new OSStools coming up almost weeklyandjiki is based on kubernetes kubernetesis a open and standard platformand what we are doing we are making surethat in that highly and fast evolvementspace we provide the open foundationfor you that you are not locked in in aspecific toolingso how that translates to concrete andInvestments that we do in uh on gke teamto support youand I'll start from the bottom of thediagramfirst is G key foundational EML platformso in the foundation one of the keythings that we see is for large modelshigh-speed networkingis one of the key things to enablelarge-scale training so we invest a lotin integration with gcp networking bothon a GPU site on a TPU siteto unlock training scenarios second bothfor training and inference storageperformance is super criticalpretty much as we think about forinference latency load time for themodels is a or one of the keycharacteristics so we partner veryclosely with gcp storage teamsto enable differentiated storageperformance as part of gke clusters sothat's a foundationand in that Foundation we ensure that wehave we cover all platform computeplatform options across TPU that wasannounced yesterdayGPU and CPUso you get consistency of compute as yourely on gkeas we look on specialization betweentraining and insurance workloads the keyenvious means in training is onhyperscale clustersGTE today provides larger scalekubernetes clusters so we support asingle cluster up to 15 000 nodesand we support multi-classerarchitecture with gkey Enterpriseso training is really pushing theboundaries of training both withinsingle region multi-regionenablingthe skills that you need on incidentsthe balance is different on inferencethe key things is latency of yourinference modelsprice performancebecause especially for the large modelsserving is super super expensiveand so is a high throughout put formodels that are successful for yourbusinessso there's three key areas ofInvestments that you will see happeningin gkey as weintegrate trade Insurance offerings inthe G keythe last Point going to the top of thediagram is ecosystem as I mentioned eamland GNA is a highly highly Dynamic spaceand in that highly Dynamic environmentvariety of tools that we'll all be usingis gigantic yeah and the key Investmentsthat we're doing on a gke both provideout of the box integration for allpopular OSS Frameworksand we also partnered with earlystartups early with a stream works thatstill on the riseproviding open platform for that withthat I will pass to Nason to cover indetails Investments that we do in bothon a training inference and ecosystemsite[Applause]thanks Alexso I'll start by discussing how gkeenables large scale trainingand to structure our discussion togetherwe can think of a typical architecturalcake diagrama successful ml training experiencedepends upon amazing foundationalinfrastructurekubernetes then provides the criticalorchestration layerand above that an open source kubernetesnative system called Q provides many ofthe features you need to manage jobsand on top of gke you are running yourml Frameworks tools and librariesI will start at the bottom of the sackand work upwards so first gke providesdiverse and industry-leading acceleratedcompute optionsnow most managed kubernetes offeringssupport gpusbut today I am thrilled that we haveannounced that gke is the first managedkubernetes offering to support tensorprocessing unitsGoogle's custom designed machinelearning accelerators are now generallyavailable in gkewe've brought together the remarkableprice performance offered by tpus withthe industry-leading scalability the gkeprovidescustomers like character AI light tricksweights and biases and grammarly arealready experiencing some of thebenefitsof tpus in gkeif you're already using Cloud tpus gkecan save you money and simplify youroperations by scaling up when demandRises and scaling down when demand Fallslight tricks an AI first company thatdevelops photo and video editing appsfor creators found that TPU support ngkehas replaced many of their customscripts that they built to operate theirlarge model trainingsweights and biases which provides adeveloper first ml Ops platform hasworked with Google to add support forcloud tpus to their launch productand their TPU support is built on gkein grammarly's research on largelanguage model alignmentgrammarly found that Google Cloudtpus and Jacks outperformed many similarofferings that they evaluated alongdimensions of performance reliabilityand scaleand in his keynote yesterdayThomas introduced the cloud TPU v5e ourmost efficientversatile and scalable AI acceleratorand today I am thrilled to announce thatthe TPU v5e is available as a preview ingkev5e offers up to 2x more performance perdollar for training workloads comparedto our previous generation of tpusv5e is designed to offer highly flexibleVM shapes to perfectly fit your diverseworkloadsand our multi-slice technologyenables v5e to power massive mlworkloads that scale to tens ofthousands of nodesI'm thrilled as well that in the comingweeks the a3vm powered by the Nvidiah100 GPU will be generally available ingketraining very large ml models requirestremendous Network throughput betweennodes so we designed the A3 to have anorder of magnitude more Networkbandwidth compared to the previousgenerationwith eight h100 gpus in a single VMthe A3 provides remarkable 26 exoflopsof AI performance shrinking the time ittakes to train modelsthe A3 is revolutionary and customerslike tab 9 depend upon it to power theirbusinesstab 9 offers an AI powered codecompletion trained on permissive opensource repositoriesto compete tab 9 depends upon the speedand price performance of A3 VMS runningin gkethe power and scalability of the A3 ingke allows tab 9 to rapidly buildtrainand serve each of their customers in away that's personalized to their privatecodeI've spoken several times about thediverse options gke provides forAccelerated computeour A2 Mega VM is unique among allhyperscale public cloudsit's the only VM offering 16a100 gpus on a single VMwith 16 a100 gpus on a single VM manycustomers find the A2 Mega's uniqueshape can offer Superior priceperformance for training large workloadsAI 21 Labs is one of the leaders indeveloping Foundation modelsthey have found that the A2 Mega helpsthem cost efficiently run their productsand r d workloads because of the A2omega's unique configuration of 16 a100gpusSalesforce AI research found that A2Mega makes it easier to fine-tune largemodels and saves time during modeltrainingwe've seen the gke's diverseflexible and Powerful acceleratedcompute options our one key way gkeenables you to train massive ml modelsanother essential aspect of traininglarge models is getting fast access tolots of dataand that's why I am thrilled that todayGoogle Cloud Storage fuse is generallyavailable on gkefuse is short for file system in userspace and it allows applicationdevelopers to use common file i ooperations to access GCS rather thanusing GCS specific apisif your workloads fetch unstructureddatanow you can move those workloads to gkewithout requiring any change to how youaccess filesstandard python file semantics work outof the box to fetch data from GoogleCloud Storage making your applicationsmore portable than ever beforeGCS views can also speed up the time ofyour applicationby allowing it to load data and startprocessing that data before all data hasfinished loadingcustomers like openx are already usingGCS fuse with gke and have found itvastly simplified configuration of theirapplications and accelerated pod startupTime by up to 40 percenton top of a diverse powerful and easy touse foundational platformgke adds the very best of our managedkubernetes offeringgke supports the training of massive mlmodels by scaling to as many as 15 000kubernetes nodes in a single clusteroutscaling other public clouds by up to3xbecause the scale of ml models isincreasing far faster than the size ofHardware accelerators a multi-nodetraining architecture that supportsmassive scale is critically importantand gke uniquely offers thatas the number of nodes in your trainingcluster increases the probability thatany one of them will fail naturallyincreases as wellsince one node failing means all nodesmust wait we've made extensiveInvestments to help you maximizeworkload uptimegke's proactive notifications give youadvanced warning about potentialbreaking changes in future kubernetesreleases and our maintenance Windowsgive you control over when upgradeshappenif any unplanned node failures occur gkecan detect and auto repair the nodewhile allowing training to resumequicklywhen training ml models you need theentire job to get done with all podsworking together as a unitand with many different teams sharing aaccess to limited compute resourcesso to make this easier we've invested inqan open source kubernetes native jobqueuing systemif you have many teams within yourorganization all sharing limited computeresourcesQ makes it easy to share those resourcestogetherwith Q each team can get their fairshare of resourceswhile bursting to take advantage ofunused resources elsewherewith Q the most important jobs getpriorityand lower priority jobs get preemptedand all of this is fully open source andintegrated with a diverse ecosystem ofml tools Frameworks and librarieswe know gke is just one piece of youroverall AI Journeyso that's why we support a diverseecosystem of tools Frameworks andlibrariesso we've seen gke provides pay-as-you-goaccessto a world-class supercomputer wheneveryou need it for training ml workloadslet's see how gke enables highthroughput low latency servingso just like training I'm going to startby looking at the foundationalinfrastructure that gke provides andwe'll work up the stacka few months ago we announced thegeneral availability of the G2 VMpowered by Nvidia L4 gpusG2 is the industry's first Cloud VMpowered by the Nvidia L4 GPU and itspurpose built for AI inference workloadswith up to 4X better performance thanthe T4by switching from the Nvidia A10GPU to G2 instances organizations canlower their production infrastructurecosts by up to 40 percentunlike the a 10g or the T4 The L4 GPUincludes FPA support which can driveeven better price performance forserving workloadsI spoke earlier about how easy it is tochain together many TPUv5e chips into a massively scaledsupercomputer for training workloadsthe v5e has very flexible VM shapesmaking it Stellar for inference as wellwith up to 2.5 x more performance perdollar for inference workloads and up to1.7 x lower latency for serving llmscompared to the prior generationv5e can provide an affordablelow latency serving for your ml modelson top of our inference optimizedHardwaregke adds amazing kubernetes goodness tomake serving models fastreliable and affordabletraditional kubernetes offerings evenmany managed kubernetes offerings stillrequire that you manage your kubernetesnodesto help you focus more on your businessand less on infrastructure management webuilt gke autopilotautopilot is a mode of operation inwhich Google manages your clusterconfiguration including your nodesautopilot is perfect for ML servingworkloadsallowing you to build and deploy fasterwithout the need for deep kubernetesexpertisethe resources requested by yourkubernetes pods are backed by Googlesresreducing the burden of day twooperationswe found customers love autopilot for MLserving workloadsso I've talked about how gke is an idealhome for your large-scale training andserving workloadsbut as workloads scale costs tend togrow as wellso I want to spend a final few minutesdiscussing a ton of ways that gke helpsyou optimize your spend whilemaintaining your customer experienceone of the best ways I've seen customerssave money doing inference is to removefrom their container image any unusedNvidia librariesand to serve using Nvidia Triton andfaster Transformerone customer that I spoke with was ableto achieve a 75 reduction in the size oftheir containers by doing thisI've seen other customers move inferenceworkloads from a single a100 GPUto a multi-host inference architecturein which the model is split across manysmaller gpus such as t4sand in exchange for slightly increasingyour serving latency this can lower yourGPU costs by up to 50 percentand gke auto scaling and node Autoprovisioning makes it very easy to serveprimarily using spot gpus which are upto 90 percent cheaper but to fall backto on-demand instances when spot becomesunavailablewhenever you're serving user-facingtraffic you'll generally want toprovision slightly more gpus thanimmediately necessary in order to absorba sudden spike in demandif it takes your pods a while to getscheduled on new gpusthen you'll need a large GPU bufferwhich raises costsso we've invested extensively to ensurethat new gpus can spin up quickly andyour workloads can get started rightawaygke image streaming provides you with upto 3x faster application startup byallowing your workloads to initializewithout waiting for the entire containerto downloadand as I mentioned earlier GCS fusewe've seen customers using GCS fuse ongke reduced by 40 percent their workloadstartup time for data intensiveworkloadsmany organizations also shareinfrastructure across numerous teams gkeprovides built-in metricsdashboards and alerts to measure andattribute per tenant costs as well aspowerful tools to increase your computeutilization for each team within yourorganizationone of the ways that you can save moneyis by using many containers on a singleGPU to then increase your GPUutilizationand gke's GPU time sharing allowsmultiple containers to share a singlephysical GPU allowing you to moreefficiently use gpusthis can be perfect for bursty workloadslike notebooks or low volume inferencegke's multi-instance GPU support allowsyou to partition a single larger GPUinto several smaller onesperfectly fitting each of your mlworkloadsand in the coming weeks gke will supportNvidia multi-process service whichprovides an additional way to letapplications share a GPU concurrentlyhelping you to increase the utilizationof your gpus and lower costswe've seen how gke is a stellar platformfor your AI workloads enablingcost-effective training and serving andthat's certainly truebut what makes gke even more powerful isthat it can do this not just for your AIworkloads but for all your containerizedworkloadscustomers like light tricks rely ongke's diverse Hardware acceleratorsupport by mixing and matching tpus andgpusand lightrix uses tpus for efficientlyprocessing text prompts and batchesusing a large text encoder and thenemploying gpus to run an in-housediffusion modelthere's no other orchestration platformthat offers such a diverse mix a tpusand gpuscustomers like lightrix and Salesforcegrammarly and AI 21 labstab 9 and openx and so many othersdepend upon gke as part of their AIJourneyand I am excited to see all the ways gkecan partner with you in the comingmonths to play an essential role inaccelerating your organization's AIJourney"
}