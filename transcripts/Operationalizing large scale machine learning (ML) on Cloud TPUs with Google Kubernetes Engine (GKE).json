{
    "title": "Operationalizing large scale machine learning (ML) on Cloud TPUs with Google Kubernetes Engine (GKE)",
    "presentation_type": "Breakout",
    "categories": [
        "Application Developers",
        "DEV212"
    ],
    "video_id": "wtKhG1aTgtY",
    "time": "Aug 29 06:00 PM - 06:45 PM CDT",
    "transcript": "foreign[Music]welcome ladies and gentlemennow we are going to talk aboutlarge models and how to operationalizethem using cloud tpus in gkeI know it's end of the dayand it may beat the point of getting tiring but Ipromise you we are also going to talkabout magiccan I get some enthusiasm in the air now[Applause]okaymy name is vaibhav Singh I'm a productmanager for cloud TPU and joining metoday for this talk isa generator where I lead from lighttricks and Matt Hoffman who is a machinelearning engineer from weights andbiasesin the next 45 minutes here is what weare going to discusswe're going to discuss why large modelswhat are the complexities ofoperationalizing large models and howcloud tpus in GK help you address thesecomplexitieswe're going to hear from you off abouthow lightrix is using Cloud tpu's NGKfor their production workloadsand we're going to look into how CloudGPU in gke is a foundational layer thatlends itself for developing with otherthird-party tools the more robust ml Opsfor your large modelsand finally we're going to hear fromMatt what weights and biases is buildingwith cloud tpus in gke so let's getstartedso first of all why large models I thinkit's by by this time of the day it'spretty obvious to everybody or I shouldsay by this time of the year it's prettyobvious to everybody but stilllarge models are generating anddelivering valueacross the use cases such as questionansweringsuch as image generationcoding assistancethe ability to use a model Supply itwith the text prompt and get a usefulwork done out of itis borderline magicalwould you agree it is borderline magicalso so that was the magic promiseeverybodyso this tweet from Andre karapati who isone of the co-founders of open AI someset up the best English is the hardestnew programming language so in case youdid not pay attention to your computerscience classes now is your chance toprogramlarge models unlock even newercapabilities with scale some of you mayhave seen this chart this is from ourPalm launch blog where it shows that atdifferent scale we have models acquiringunique capabilities but thesecapabilities also require more modelparameters and data sizes and hence morecomputeandthis is why Google Cloud brings the bestof cloud tpus and gpus so that you canbuild your large-scale ml infrastructurethat is cost efficient high performanceand sustainablehoweverinfrastructure itself is not importantbecause those of you who have tried toput a simple model in productionknow that operationalization is hard andoperationalizing large models is harderand why so because at the large scalethere is a complex interplay of manymoving pieces we need a orchestrator afoundational orchestrator which freesyou up from developing foundational worksuch as automatic logging monitoring sothat you are not developing code for itsuch as efficient utilization ofresource something that helps you do itout of the box so that you're notwriting your own queuing systemsuch as data Providence and modelprovenance using either the nativecapability of the orchestrator orsomething third party that easily plugsinto that orchestratorand in addition to that an orchestrationwhich can support a unique computationalcapabilities or unique interactions thatwe get to see when we are working withuse cases such as reinforcement learningwhere there is an actor learner and setof environments and their data flowingback and forth between all these movingpieces and finally we need anorchestratorfor larger scale that can handle thatskill reliably that can run yourtrainings recover from failures recoverfrom maintenance events gracefullyand if you're usingyour production systems which is servingmillions or even thousands of users youare already probably relyingfor Mission critical workloads on gkefor your machine learning workloadswhich is using gpus also probably youare relying on gke but for your machinelearning workloads using tpus until nowyou have probably something like thisthis is copyrights all all rightsreserved to memy auto scale Auto resumelarge scalebash dot sh and if you have such ascript in place probably your devopsfriends are not coming to your partiesso instead imagine a world whereoperationalizationorchestration was simplerimagine a simple answer to all theorchestration question that is gkeand this is why we have introduced thefirst party integration for cloud tpusin GK that means now you can create TPUresources in GK in the same way you cana GPU or a CPU resource and you can usekubernetes API to then interact withthese resourcesand for those of you even though I wouldask them why did you sign up for thissession if you did not know what is gkebut here is my favorite definition of GKGK is a cheat codethat helps you focus on using kubernetesrather than managing it and moreformally it it it is a Google managedflavor of kubernetes that abstracts awaythe control plane so that you are notmanaging your cluster and you'refocusing on your operationsback to tpus now I said that there is afirst party integration of tpus it'sfirst class supported you're asking okayso what can I do with itmy answer is you can do pretty muchanything you can do with Cloud tpusotherwise but here are three things thatI want you to walk away with today youcan run training on cloud tpus addhundreds of or even thousands of Chipscale that's that's what we call asingle slice Cloud TPU training you canrun training at tens of thousands ofscale for those of you who are in roomnumber 154 next door we just had asession that talked about how with CloudTPU v5e and our unique multi-slicetechnology now we can scale to tens ofthousands of chips beyond the boundariesof a single ICI domainso now you can run those training easilymanage them gracefully using gke andthird last but not the least you can doinference generating value from allthese large models that you are tryingto operationalize okay let's dive intothe training itself so if you are ifyou're training with that my favoriteCustom Auto Bashscript today then likely yourpainstakingly set setting up a trainingenvironment on every worker of a TPU VMand then you're writing your script andlaunching via SSH onto all these workersimagine what this setup looks like ifyou're working with a slice with 1000workerspretty brittle huh tpus in GK is here tosave the day and that's why thisdeveloper on the right hand side isactually happy now the previous one wasvery bored because she was not motivatedwriting that my custom script so now shecan write their Jacks writer jackstensorflow or pie torch training jobusing a simple manifest that uses theindex job API which easily encapsulatesthe training environment easilyspecifies what is the shape of computeit needs and finally mentions what is ascript that needs to be runand gke will take care of the restso enough about talking that how it issimple and easy how about seeing this inaction so let's see a quick demoall right I think thisneeds to be extended hereall right I did not do my offering tothe demo of gods as they saidso uh can everyone see my screenokay so here I'm creating a GK clusterand adding a TPU node pool to it this isa V5 e TPU node pool this is a partslice with uh 128 35e chips now I'mshowing you the workload manifest here Iam showing to this distinct sections ofthis manifest one is this headlessservice another is the index job API weare using headless Service Such thateach of the parts that is created by theindex jobcan have a unique DNS entry which isused for cluster Discovery mechanism andthen I'm simply mentioning mytraining container image and specifyingmy training script here this trainingrun is going to train a 16 billionparameter language model using 128 chipsand as you can see within a single lineof cube CTL my training is alreadyworking within a few secondsand now I can see the logsand within a few seconds you will seethe training will get started as you cansee here we have 128 TPU chips in thisclusterand the training has started now and itis already producing a hundred and seventea flops or teraflops per chip that isdelivering close to 55 percent of modelflop utilization and you may have hearda claim that we are delivering 2xperformance per dollar with V5 e withrespect to V4 here is this claim nowjumping to our monitoring UI you seethat to TPU specific Matrix which havebeen added these monitoring dashboardsare generated automatically without youwriting a single line of code automaticlogging I I again did not have to writeany code for that and I can buildsophisticated dashboard profile for mytraining performance here is a briefview of the profiler itself beginningwith the overview page and jumping tooperational profiling page where you cansee what are the categories of differentoperation and what is the fraction ofthe step time consumed by theseoperations jumping to the graph viewerpage where you can double click into thefusions or the kernels generated by xlajumping to the trace viewer page whereyou can see or zoom into the step timeitself and see that what are theoperations which are the bottlenecks foryour training run so that you can go andtry to optimize these operations andfinally the memory viewer page whichgives you the view of the hbm allocationfor the lifetime of your program in asingle training step so this was a quickview of our training run and by thistimemy 15-step training has finishedoh yesthank you soso far we talked about128 chip scale how about tens ofthousands of Chip scalethis is what I refer to as what isenabled by Cloud TPU multi-slice so itis a technology that extends the sameprogramming abstraction that xla bringsfor a single slice and extends it totens of thousands of chips which canspan across hundreds of these slices asyou can see in this performance chartwhich is showing you the scaling curvemeasured on training a 175 billion gpt3modelmeasured across ranging from 256 shipall the way to 32768 of V5 e chips andthis is the linear scaling that is thecore value proposition of cloud tpus andmind you with the same programmingabstraction that you have used so thatas a developer you continue to writeyour code as if it is going to run on asingle host or a single chip and it canautomatically scale to these tens ofthousands of chips that being said Ithink the core of this talk isorchestration and operationalization sohow about operationalizing it 32 000chip cluster that translates to 8 000VMS now for those of you who have triedto manage a cluster or run a large-scaletraining job know that it is not trivialmanaging your training and run it in areliable way at this scale or the skillBeyond it is not simple and this iswhere cloud tpus in gke will bring youThe Experience here once again followingthe same concept as XLE expands the sameprogramming model from one chip tothousands of or tens of thousands ofChip with gke we are expanding the sameabstraction you saw how the index jobAPI was used to basically Express a partsliced training job that runs on to allthe VMS inside that part size now thisjob set API isgoing to create a manifest that isgoing to automatically generate theunderlying set of index job which willthen run onto these hundreds of Partsslices and not only that this is goingto execute this in a way that is fairlyresilient that is going to automaticallyrecover from a multiple chip failure orset of Maintenance events and so on andat the same time giving you all the outof the box benefits that GK offershere's a quick view of our manifest asyou can see that you think the job setAPI we are also able to specify theworkload almost in exactly the same waythat we were in case of the index jobAPI that you saw earlierso finally in summary Cloud TPUmulti-slice gives you the unique abilityto scale to tens of thousands of Chipscale and Cloud TPU multi-slice in gkeallows you to orchestrate it in theeasiest possible Manner and gracefullyfinally to the third workload remember Italked about three things the singleslice training the multi-slice trainingand finally the inference here I amshowing you an example of runninginference on cloud tpv5e what you'reseeing in the bottom of the screen is alittle interesting demo you can also seethis live at our Hardware Wars Booth butmore crucially with TPU v5e we arebringing for the first time a TPU VMshape that can range from one chip toeight chips and if you have gke nodepools of this kind of TPU VM it can alsoAuto Scale based on your workload basedon your traffic and hence it it lendsitself to building production workloadsfor serving these amazing models thatyou're building with tpus in GKwith this I will now call upon you havefrom lightrix who is going to share withus what is light tricks and how lightrixis using Cloud TPU of NGK in productionthank you yeah thank you[Applause]my name is joab I'm a researcher and Ilead the core generative AI team atMatrixwith light tricks we bridge the gapbetween imagination and creation we haveseveral Market leading appsfacetune photo leap video leap andothers were our users generate more than200 million images per month usinggenerative AII want to tell you about our journeywith personalized generative AIhow we learned to prioritize ease of usein high quality resultsover quick response when this trade-offis necessaryand how we eventually got the best ofTwo Worldssince the early days of stick to imageour goal has been to empower users toincorporate their own faces and contentinto their creationslook at this photo of yours trulyI made it using an old version of ourpersonalized text to image featureI'm pretty happy with how it turned outbut I'll be honest it required someserious prompt engineering skillswhile a skilled prompt engineer couldwork their magic to achieve convincingresultsmore often than not the outcome wasn'texactly flatteringto address this issue we came up with asolutiontraining a custom model for each userthis approach helped us preserve theirunique identitywhile it was more costlyand significantly increased latency itproved to be necessaryshortly after resolving thepersonalization challengewe teamed up with Paramount for an epicDungeons and Dragons campaign centeredaround this very featurethe campaign had a tremendous impact onour app not only did it double thenumber of exports but it alsosignificantly increased user retentioneven after the conclusion of thecampaignlater this year we achieved sometechnological breakthroughs that allowedus to offer a lightweight alternative totraining a custom model for each userthis alternative is not only way fasterbut also better preserves the user'sidentitythe idea was to guide the output imageusing an image provided by the userrather than generating it entirely fromscratchand we didn't stop there we took theconcept further and expanded it Beyondselfie images this time we allowedcustomization for various types ofimages including rooms buildings andlandscapesprovided by the user we use these imagesas a guide to generate amazing scenessuch as Lego and Barbie inspiredcreationsfinally users are able to incorporatetheir content into generative AI withoutcompromising on user experiencepreserving user identity and thestructure of their content emerged intoan exciting Tick Tock trend showcasingthe creative potential of this featureusers love to see AI generated images ofthemselvesthe following month the new featuressparked a trend in the United Statesparticularly among iOS users includingpaying subscribersthe significant news is that photo isthe photo labor app made its way intothe top five free apps in the App Storetop 5 surpassing even Tick Tock andInstagramthe graph indicates the number ofsubscribers who've been with us for over30 daysand we can observe we can observe thatthey also join the trend elevating theoverall user baseso that's how we optimized the productand enabled users to use their contentwith generative AInow let me tell you about theinfrastructure behind this magicwe built our training infrastructure ontop of tpuv4we store our data in small shards onGoogle cloud storage and we stream theseshards to the TPU VMS in training timethe TPU VMS upload checkpointsperiodically to then to another GCSBucket from which a lightweight GPUmachine downloads them and generatesresults for evaluationTPU parts are treated atomically inother words either all CPU cores areworking or they all diet once this helpsus to recover from Hardware failurefailures and resume the training whenneededrecently we've started to use TPU in gkerather than setting up the TPU machinesdirectly in GCEnow we're able to use Docker images inother kubernetes features that save usmany custom scripts and add moreflexibility to our workflowwhile the overhead of creating a clusterand then a separate pool for eachtopology is high using TPU and gke opensup opportunities for us that weren'tpossible before with GCEone important advantage of the TPU Partsis the linear scalinghere you see that the number of imagesper second goes up linearly with thenumber of TPU coresthis is very important for us whentraining our text to image Foundationmodelsthese advantages of TPU as well as ourcareful data curation process in somearchitectural improvements enable us totrain our foundation text to image modelmuch faster and for an order ofmagnitude less money than it took totrain stable diffusion on a cluster ofgpusour in-house models are tailored to ourneeds enabling us to efficientlygenerate hundreds of millions of imagesper month thanks to higher throughputand better controlif you are interested in hearing moredetails about generative Ai and lighttricks you're welcome to come to the TPUHardware verse booth and will be therethank you back to you if I have[Applause]10x cost reduction these are the kind ofnumbers that brings me out of the betterevery morning because someone somewhereis using tpus and now tpus in gke todrive home these kind of savings andachieving this in this level ofefficiencymoving on coming back to the samepicture that we saw earlier in terms ofcomplexity of large-scale orchestrationI hope that you have seen in actionthrough our demo and heard from you offas well on how cloud tpus in gke ishelping to address some of thesecomplexities as a foundationalorchestrator as a base layer that lensesitself so that you can use yourresources in the most efficient possiblewayand as a out of the boxlogging monitoring profilingand all these toolings which can helpyou then debug and solve any problemsthat you run intonow that being saidwe also realize that all your problemswould not be solved at the at the nativeGK solution level itselfyou would need to also leverage otherecosystem tools in order to havefeatures such as model provenance dataprovenance and much moreand this is where the the role of cloudtpus in GK as a foundationalorchestrator comes into play and now I'mgoing to call upon Matt from recentbiases who is going to tell us whatweights and vices is building with cloudtpus in GK Matt[Applause]EXPerience and biases so I'm going to talkto you today about how we're making itreally easy for all of our customers andml Engineers to access the power of tpuseven if they don't know how to usecubectl so first off what's weights andbiases so our mission at w and B isbuilding the best ml tools out there forML practitioners and so we have sort ofa wide-ranging platform covering a lotof features for you know tracking all ofthe the results from your modeltrainings like yoab was was just talkingabout visualizing collaborating on thatdata with your colleagues and some sortof newer featuresum around sort of llm Ops as welland so we have you know customersranging from you know the small scalegenerative AI startups all the way tosome of the largest players in the spaceusing us for a few common workflows youknow training your experiments managingand viewing sort of the lineage of datasets to models supercharging orcollaboration and sort of you knowgetting a path to production readymodelsso what does that look like in practiceso over here on the left you can see ascreenshot where you know all of yourtraining runs or evaluation jobs whetherthey're happening in gke individual GCinstances sort of local notebooks allthat is going to be gathered togetherum and will also sort of generate somesome default charts for thatand then there's additional data thereas well so for a particular model overthere on the left that dag can take youall the way back to which data sets whenwhich versions of data sets were usedespecially for folks that have new datacoming online being able to answer thatsenior executive to say this is what wasin our pipeline when it was trained veryvery important and on the right we'recapturing a whole host of additionaldata so the the code Docker images thatwere used to to run that system metricsGP utilization all of thatand so you know we had this feature setand over the past year so we're speakingwith customers and sort of heard thisrecurring pain point right so you knowwe need a reliable way to spin up lotsof training runsum folks doing sort of big hyperparameter sweeps you know scaling thingsout we're having sort of blockages inthere in their pipelineyou know folks complaining abouttraining runs taking uh up to two weeksmultiple weeksum to sort of finish uh sort of slowingdown the the pace of innovation thereand I think you know this engineer putit very well you know those even whenyou have that solved there's a lot ofmanual developer workum to do those things we've been talkingabout today get all those scripts readyum and just sort of asking us you knowcould we just get a quick easy button toaccess the the power of cloud computeand so we launched this feature calledlaunch and have been making lots of punsabout launching launch uh earlier thisyearum really sort of focused on that painpoint right so connecting mlpractitioners with that Highum High scale specialized Hardware liketpusum and making it you know really easyfor the ml Engineers we work with notjust folks that are you know veryfamiliar with ML Ops you know inpractice working with folks what we'vereally found is you know you can bringthem together the ml Ops person can runthose fancy commands and get all thetpusum available to use and then connectthem to a place where the ml Engineersare already working so with sort of thethe gke support for tpus now we're ableto launch the you know Docker imagescode requirements all of thatinformation straight into gke so you cansort of from one simple easy buttonconnect so you can see here one run in Wand Bfrom the interface we can see all of themodel configs you can change your datasets change you know your training speedand then you can see here the ml Opsengineer already has this connected totpus and boom you know that is now offrunning in gke with no need for the mlEngineers to have to do anything otherthan log code to W and B and then relyon that you know config setup 13 linesof code you know one time setup and thenyou're good to goso with thatum bring vibhab back up to tie it alltogetherthank youthank you MattCloud tpus NGK with a click of a buttonwith 13 lines of code how wonderful thatis thank you for the work that weightsand vices is doing and congratulationson the launch of launch sorry I couldnot resist that jokeso coming back to our complexity pictureI hope that now you have seen that wehave a platform that can addressorchestration operationalization foryour large-scale ml workloadscomprehensively right from out of thebox capabilities and also leveragingother ecosystems of which you saw oneexampleso in summary what are the threetakeaways number one Cloud tpus are herein gke you heard three primary workloadsthe single slice training themulti-slice training and also runninginference is now possible and not onlypossible but you can do it easily andefficientlysecond it is already delivering on thepromise uh lightrix is already usingcloud tpus in production and there aremany such customers who are leveragingthe power of gke with Cloud tpus tosolve their large model orchestrationand finally as a substrate or the baselayer or the foundational orchestratorcloud tpus in GK is already lendingitselffor you to leverage other ecosystemtools such as weights and biases so thatyou can create your own flavor ofml apps for your large-scale modelorchestration thank you very muchforeign"
}