{
    "title": "From transformer to Terraform: Cohere\u2019s large language model (LLM) takes flight",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML219"
    ],
    "video_id": "k0QBgQJ-78A",
    "time": "Aug 30 05:30 PM - 06:15 PM CDT",
    "transcript": "foreign[Music]all right hello everybodythank you all for being here todayappreciate you making the time to joinus this afternoon for what I think willbe a really interesting conversationwith kohir we have sudeep here fromcohere and what we're going to betalking about is kind of exploring theirJourney from large language models asthis sort of uh Theory and researchthrough to where we are today andspecifically where cohere is say fromoperationalizing those large languagemodels in a way that allows developersEnterprises researchers to buildincredible AI driven applications usingthat technologyum so to begin my name is Craig Oliva Iam the director of customer engineeringfor Google Cloud Canada and it'sprobably a term you haven't heard ofbefore but uh in essence I work withcustomers in Canada specifically to helpthem adopt technology and modernizeusing Google Cloudum joining us today is cohere so cohereis an AI company out of Toronto and whatthey do is they work on foundationallarge language models and they use themto help Enterprises solve really complexproblems and as you know this is a topicof the day for the last year or so forsure in the mainstream spacewhat's unique about cohere in particularand I'll let sadeep kind of provide anoverview in a moment but what's uniqueabout qhere is that if you look at theDNA of the companythey are actually found their Foundersare ex-googlers who worked on the thetechnology the underlying foundationaltechnology that comprises large languagemodels today the Transformerarchitecture so Aiden Gomez Nick Frostthese folks are pioneers in that spaceand so they they have the depth and theknowledge Beyond most companies to adoptthis technology and provide value withitum so the links between Google andcohere are very strong and sudeep is thedirector of engineering at cohere andmaybe if you don't mind just kind ofkicking off maybe in your own words Iguess talk to us a bit about umhere talk to us a bit about sort ofcohere in your own words and then so whydid you join cohere in particular andthen how is the engineering team sort ofchange from where you are when youstarted to where you are today sureum yeah firstly I guess thank you forthe opportunityum it's reallyum nice to have the opportunity toactually talk about cohere and ourjourney over the past four years or soso uh cohere was founded in 2019um our CEO Aiden Gomez he was one of theoriginal authors of the attention is allyou need paper which as many of you knowis basically the foundational paper thatled to the field of generative AIum he did that work while at Google uhin Google brainum and what he saw was the Revolutionaryimpact that the Transformer models werehaving inside Google how it was gettingintegrated into a number of Googleproducts and he wanted to democratizethat and make it accessible to a broaderaudience and that basically was what ledto the foundation of cohere back in 2019and obviously like Aiden had thatforesight to see the potential impacteven before all of the generative AIBuzz that we are currently facingum so yeah uh I joined cohereum two years later around 2021um I was like Aiden also in Google I wasin Google brain and I was a researcherthere I kind of saw the potential impactthat large language models could have Ishared Aiden's Visionum to make it happen and that's why Ibasically left Google and joined uhcohere even though we obviously stillactivelywork and partner with Google in a numberof different directionssince then cohere has obviously growntremendously when I joined cohere was uhI guess if somewhere between a 40 and a50 person company now we are more than200 and 30 employees spread across threelocations we have a modeling like amachine learning expertise Hub which isbased out of London our primaryheadquarters is in Toronto and then wedo have a Bay Area office as well wheresome of my teams um currently history[Music]umyeah I think that's probably like a veryquick summary of the last four yearsbetter than mine yeahum so where do we start with youmentioned a bunch of growth cohere hasgone through tremendous levels of hypergrowth in the last couple yearsum how have you gone from where you werebefore to where you are now in terms ofenabling developers and Enterprises toadopt this technology I think a lot ofthem have wanted to adopt it they're notsure how how have you have you met thatchallenge yeah uh I think one of thethings that has happened especially inthe last six months to a year is peopledon't necessarily ask the question ofwhy I should use generative AI in myproducts I think now the question hasshifted to how I should use it how Ishould integrate it into my products andwhat we have seen is there's still aquite a big gap in just being aware ofthe power of natural language uhprocessing and large language models andbeing able to actually make itintegrated in a seamless manner into uhinto a product while leveraging theproprietary data sets that you mayactually already have so here has beenat the Forefront in terms of justeducating our not just developers butalso Enterprises through variousmechanisms like we have a llm universitywhich is a course structure that triesto go over many of the fundamentals aswell as the potential use cases that canbe driven by some of these largelanguage models we also have forwarddeployed engineering teams who work withlarge Enterprises in a very Hands-Onmanner we uh to solve their specificproblems we have Partners as well ascustomers with ranging from smbs toFortune 500 companies including McKenziesap and live person who are using ourmodels and have dedicated customersupport from usumyeah in terms of just the modelingcapabilities itself what I have seenover the last four years isum even at the beginning of the lastfour years we had generative modelswhich were basically truly foundationalmodels and uh in the sense that theywere mostly continuation style modelswhere uh the model you give it a partialsentence and you ask the model tocomplete that sentence the nextEvolution that I saw was instructionstyle models where the models were nowcapable of following instructions sowrite me a blog post instead of givingyou the first sentence of the blog postyou ask the model to write your blogpost and the model writes it for you thenext Evolution that we saw isconversational models which is what wemost what what created the most buswhich is natural multi-turn instructionfollowing uh the things that we areworking on actively today are retrievalaugmented generation which is combiningthe power of large language models withexisting search and retrieval systemsandum we are also working mostly in theresearch phase on action orientedlanguage models which are capable ofactually performing actions in the realworldcool fantastic so I know as I've beenworking with cool here now since 2021andum you lose you use a lot of advancedtechnology alone in in sort ofsupporting the business you do massivetraining on a very large model usingtpus you use gpus for serving a lot ofother things can you can you talk a bitabout sort of like how do you accomplishthose tasks I think a lot of folks hearabout all these different Technologiesall these different tool sets they canuse to operationalize AI at scale how doyou approach statically or how do youuse tpus how are you using gpus todayspecifically uh through our partnershipwith Google we basically got EarlyAccess to TPU v3s initially and that'swhat we were usingum uh to do our pre-training of largelanguage models uh and thenum back in I guess 2021 we moved to TPUv-force as one of the early users of TPUv-force and we got more than a 2XImprovement in performanceum and when I say performance what wereally care about is price perperformance because we want to drivedown costs and we want to maximize theefficiency and tpus really gave us agood uh good balance uh when it came toprice performance uh we also had likereally uh good uh dedicated supportthrough our partnership with Googleespecially the Jacks team as well as theTPU team and it was very much asymbiotic relationship in the sense thatwe were early adopters we were kind ofanticipating running into a few issueswhich we did and we were fortunate tohave like Hands-On support from both thejacks and the dpu team to resolve theseand hopefully that led to like a betterand more reliable product which is uhyou know done helpful for all all Googlecustomersum yeah so that's our TP that's how weuse tpusum and then on the inference side wewant our models to be deployed inwhatever environment we uh our customerschoose to deploy them in which is why wemade a choice to go with gpus for ourinfluence uh because they are slightlymore widely available across multipleplatforms uh we do use high-end gpus forentranceum uh on gcp and we use a number ofperipheral services including Googlekubernetes engine ummonitoring gcp logging for basicallybuilding up our platform to to servethese models efficiently fantastic soone of the thingsumI'm curious about his his modelperformance so as things scale how doyou maintain model performance todayum how do you handle tuning them overtime and then I think the other part isyou've worked with a lot of differentEnterprises now and they all havedifferent needs right so they all havedifferent needs from a performanceperspective but also scaling to securityas well how do you accomplish thosetasks how have you approached that todayyeahum as probably like many of you alreadyknow you cannot necessarily improve Asystem that you cannot measure so uh wehave invested heavily in uhcomprehensive evaluation framework thatallows us to measure the performance ofour models our evaluation framework isbiased more towards Enterprise use casesthat our customers are seeing as opposedto academic benchmarks we see a skewbetween academic Benchmark performanceand what use cases our customers haveand we definitely skew towards solvingum solving for our customers which ishow why our evaluation Frameworks arealso bias towards thatumyeahum in terms of the model uh performanceitself we have uh one of the strategiesor like one of the I guess designdecisions that we made is uh notnecessarily going for really uh bigmodels in terms of the parameters butrather trying to uh take a more holisticapproach we try to invest in highquality and high volume data to trainour models we try to invest in a lot oftooling that enables us to do rapidexperimentation and iteration for newergeneration of models and then we alsotake into account the cost of inferenceand latency with which we can serverequests while deciding the size of themodels that we want to put in front ofour customers so we have a couple ofdifferent sizes that we currently offerbut yeah we we don't necessarily want tochase just large models we want tochoose the right right size models forour customersum and finally in terms of model qualitywhat we have seen is that a lot of largelanguage models obviously suffer fromhallucinations so we have it currentlyinvesting heavily in retrieval augmentedgeneration which allows thesegenerations to be grounded on concretedata sets as well as have citations sothat the generation uh you know why themodel actually produced a particularphrase or a piece of text because it islinked back to the source document fromwhich the model derived that answeramazing awesome so so as you've had toscale I'm assuming you've had to makesome interesting technical decisionssome architectural decisions they allhave trade-offs I'm sure are there anythat stand out in your mind as as verykey architectural decisions that youmade that either had a a positive impacton performance or even a negative impacton performance and what did you learnalong the way in doing that yeah yeahum uh let me Focus first on the trainingside and then I'll focus on theinference side uh on the training sidethe fact that tpus uh have largeinterconnect domains allowed us tobasically not use pipeline parallelismor not want there was no incentive foryou us to use pipeline parallelism forinstance right so we were mostlyrestricted to data parallelism andtensor parallelism and because of thehigh bandwidth interconnects that tpusoffer we are able to scale it to thesizes of the to potentially like half apod half a TPU pod or even threequarters of a TPU pod which is by theway a TPU pod is like 4096 chipsum some of the issues that we ran intothough is are more around reliability inthe sense thatum you know you can imagine that at somepoint in the fast past when we wererunning data centers they had tens ofthousands of machines now if you have aTPU pod that TPU pod itself hasyou know a few thousand nodes soum and if your single training job isnow using all of these nodes a singlechip fails then that can actually stallyour training run so how do you mitigatethe impact how do you make your systemsmore reliable was one of the challengesthat we need to overcome and we did thatwith Googleum the uh on the inference side uh wehave invested heavily in uh smartbatching solutions that allows us toachieve High utilization when we runinferenceum on our large language models so oneof the key challenges in runninginference for large language models isthe fact that your requests can havevarious different context lengths andthat eventually translates into raggedtensors which are non-uniform and thoseare usually not very optimal when youwant to actually run deep learningentrance so we have Solutions which tryto basically do smart batching so thatwe can have more even shaped tensors andwe can maximize you utilization sothat's like another again like designdecision of how you can build potentialperipheral systems that are sit sitoutside the core model itself that canstill significantly improve theutilization of your models so and areyou thinking aboutum as you mentioned using the you knowthe V4 pods the 496 chips the thechallenge of you know if there's anissue on that one you know in the modelwe might have to restart some of thetraining how are you looking atum things like multi-slice sort oftraining we talked about new V5 chipsthat are coming I know this is veryearly but um any thoughts on that howyou're how you're thinking about it yeahI just saw the announcement about thenew V5 chips I'm pretty excited toactually give it a shot both forinference on DPO v5s but potentiallyalso multi-part trainingum one of the things that I amparticularly excited about is as forinstance go here tries to train uhmultimodal models which uh more likelyare going to try toum basically require models of largerparameters slices that do notnecessarily fit a single podor if we explore alternate architectureslike mixture of experts models whichum which can be more easily parallelizedacross multiple pods uh would definitelybe very interested in exploring uh thethe newer hardware specs like TPU v5sokay and and so one of the things thatwe hear oftenum is is this you know customersstruggle with I think you've been ableto overcome this but there's a balancein the trade-off that comes down withum we want to have the most highestperformant AI capabilities but doing soon a cost-effective way and so and someof that sometimes I've heard it fromcustomers it's hard to even estimatewhat you know how much how much capacitythey might need how should they bethinking about GPU something you know umwhat's that balance and trade-offbetween training something really reallyquick but making sure it's costeffective how how has kohir thoughtthrough that to make sure at the end ofthe day you're able to offer a servicethat is performant but also costeffective for both yourselves and yourcustomers yeahum as I mentioned earlier we have beenvery cognizant of thisum latency and cost trade-offs when itcomes to inference which is why we didnot necessarily just pursue large modelswe did train large models of the orderof like 300 billion parameter models butthe one we actually put in front of ourcustomers is a 52 billion parametermodels we rather invest significantlymore in higher quality data that allowsus to actually get a good amount of goodlevel of performance even at modestscales and that allows that offersbetterum price performance trade-off pointsfor our customers uh one of the thingsthat cohere uh activelyum is pursuing and differentiates itselfis we want to bring our models to ourcustomers so we want our models to bedeployed in Enterprise environmentsinstead of requiring Enterprises to sendtheir data through an API to usand as a quarterly of that uh when wewhen Enterprises do want to deploy ourmodels in their environments then theyalso need to take into account what kindof gpus they need to acquire how muchthat would cost and we actively as Imentioned like we work very closely withmost of our Enterprise customers so weactively work with them to educate themon the various knobs that are availableso that they can tweak the latencyversus throughput for some use cases youmay be okay with a higher latency aslong as you get a higher throughput andhigher utilization for other cases youmay be very latency sensitive and youwant a response within like a fewhundred milliseconds so we engage withcustomers to advise them on all of thesevarious factors and how they can achievethe best price performance trade-offs soyou mentioned about bringing thosethings for example for probably aregulated environment or somethingbringing it on premise to handle some ofthe security aspects what other thingshave you have you incorporated into intocoherent or offering to address securityconcerns that exist obviously that's athat's a really good one in terms ofallowing the portability and to bring itin what other aspects of security haveyou had to tackle in this space becauseI assume a lot of people are concernedabout yeah absolutely yeah what we wantis basically to offer a wide variety ofoptions for our customers so that theycan choose the right solution for themwhen it comes to security versus privacytrade-offs we obviously have a managedSAS API similar to many other companieswhere you can just send your data to anAPI and get inference through our largelanguage models that potentially isunacceptable for many Enterprises whohave sensitive sensitive datawe have a intermediate solution whereour models are hosted in managedenvironments for example in vertex andthe customers can then spin up ourmodels through that managed platform themanaged platform takes care of scalinginfrastructure all of that aspects andyou still get to use our models but in acompletely private environment so thatcohere doesn't see any of your datauh beyond that we have um an offeringthat basically allows Enterprises tocompletely replicate coherence platformsso whatever we use to serve ourcustomers and the managed SAS it isreplicated entirely uh for you on akubernetes cluster in a completelyisolated manner where none of the dataagain flows back to our um to go hereand finally and that we can do that inany of the public clouds and uh ifyou're particularly sensitive like ifyou're a government agency that requiresto be on a a specific fed cloud orum if you have your own data center thenwe also enable you to deploy thatsolution and that completely on-premenvironment uh one of the particularchallenges that we are facing uh asprobably many of you are already awareof is like the scarcity of gpus so oneof our focuses has also been todiversify the hardware that our modelscan efficiently run on and that enablesus to be deployed in many differentregions as well as in many differenton-prem settings which doesn'tnecessarily have a specific type ofHardware yeah I was going to say becausethat is obviously a problem everyone'sexperiencing with GP receptive what goesinto you know how do you make thatdecision then from a hardware if you'readvising a customer who is going to lookat adopting cohere and maybe there is anissue getting certain gpushow do you advise them on which ones aregoing to make sense and what that willmean for their performance perspectiveyeah often customers themselves actuallyhave requirements for regional localitybecause they are in a particular regionthey have other compliance requirementsthat require you to be in that regionum andum they basically maybe like only aspecific set of Hardware is available inthat region in which case we try ourbest to make our modelsum run on that specific Hardware uh ofcourse it's notalways possible we cannot run downarbitrary Hardware but we try our bestwe have been able to run on CPUs we havebeen able to run on gpus we have doneearly prototyping of running inferenceon tpus as well as well as a couple ofother uh proprietary chipsumand uh often what happens is that youcan always run inference of largelanguage models on other Hardware aslong as you are willing to take aperformance hit in terms of either lowerlatency or lower throughput and in manycases that's perfectly fine because whatwe have seen is that the value that thecustomers get by using generative AI intheir products typically far exceeds thecost of running entrance soum even if it's a marginal increase onthe cost it still might make sense formany of these use cases yeah that makessense so so I'm curious about you knowyou've been here now for uh two yearsright two years he's obviously part of alot of the change there when you look atwhere coheres come from you know thissort of looking at it from an earlyscience research receptor from thatpaper through to developingum you know serving capabilities throughsecurity and operations through to hypergrowth in the scale you're right nowthat whole life cycle that Journey whatalong the way what's the largestchallenge you think that sticks out ingoing from that idea to where you arenowum I think one of the challenges that Ihave seen isum we are obviously in a very rapidlyevolving field in which new papers getpublished new ideas get bounced aroundandum it's it's hard to uh be focused onyour vision and continue to executetowards that without necessarily gettingdistracted with all of these randomideas at the same time you always havethe form or that you don't necessarilywant to miss out on the next big idea soyou do want to keep tabs on it but youstill want to actually keep your teamfocused on executing what we what youactually intended toum so it's been a challenge but I thinkwe have done fairly well so far so we'llsee yeah in the time you answer thatquestion 50 papers were releasedpotentially 100 companies just spun upthere you know until interested so umagain so when I look at portableSolutions I just want to make sure I gotthis for a moment from the openness andperspective that can walk me through andyou said other clouds can run otherclouds on premise Edge I assume thingslike this or you want to hear that as anoption as well yeahum yeah oh we do want to be Cloudagnostic so our customers may have datathat is sticky and they wantum to use a particular cloud provider ora particular region so we want we we arebasically Cloud agnostic we obviously uhpartner with Google and we are availableon gcp we are also available on some ofthe other public cloudsum so yeah yeah and then and so lastly Iwas thinking when you look towards sortof the future what can you talk a bitabout from uh whatever you're willing toshare I guess from a strategyperspective around how you plan to scaleAI new capabilities features thingsyou're exploringum it could be related to openness itcould be related to security portabilitydata controls things like that what is2024 and Beyond look like for coherefrom a strategy perspective yeah I thinkone of the things that I mentionedearlier is the fact that you know a lotof the customers are now thinking abouthow should we integrate AI or use llmsinto our products and as they do itthey're seeing that they actually needto address a lot lot of these challengeswhether it's security whether it's asmundane thinks as versioning right sohow do I ensures backwards compatibilityas newer versions of the models getreleased so we are actively trying toum make sure that we have solutions forall of these more mundane aspects ofthings which are necessary forproductionizing llms in Big Enterprisessecondly I see that we will havecontinued emphasis on collecting andcurating higher quality data becausewhat we have seen is that data canbasically be the biggestit gives us the biggest bank for puck interms of improving model performanceum and the third is as I mentionedearlier you know I have seen theevolution of models and right now we arefocusing on retrieval augmentedGenerations but we are also researchingon action on having llms be able to takeactionsum so um I would expect us to continuefocusing on that space in 2024 uh we dohave uh uh intelligent Enterprise agentthat we announced recently it's calledCoralum that can basically be deployed inEnterprises it's if you can give itaccess it has connectors so that you canconnect it to your proprietary knowledgesources the data doesn't necessarily beex doesn't need to be extracted out thedata can remain in whatever databases orknowledge repositories you haveum the model the agent will be able toextractum that Knowledge from thoserepositories and use that to producegrounded Generations soum that's something which we arewe will do our initial launch soon andthat's something which will continue tobe an emphasisum over 2024 exciting yeah coolum so there's the in this in this roomat this conference and others who watchthese stream there's a lot oforganizations that are as you knowlooking at how to adopt this technologyrightum very new there's probably they'refacing a lot of pressure to answer tothe boards around how are we going to dothis they're looking for use casesthey're trying to figure out how best toto adopt this technologyum how should they be looking at thishow should if you were to advise one ofthese customers right now about how do Ifind the right use cases how should Iget startedum how would you advise them yeah soum I thinkum I wouldyeah so we can probably do like aone-hour session which is that specificquestion we have three minutesum so I think one of the things that uhI would advise people to do is startsimple identify one particular use caseand try to figure out how you canpotentially use llms for that oneparticular use case and then try likeyou you learn something in that processand then you can try to expand out fromthere the second thing that um I wouldadvise uh a lot of smbs and companies todo is that you know data is somethingwhich you haveum because you have a product you havecollected you know what your users likewhat your users dislike and uh whentrying toum integrate llms into your product youneed to think about how can you use thatproprietary data that is exclusivelyavailable to you because thatproprietary data is what is going toenable you to build a defensible productright llms are widely available they areavailable by multiple providers so justby using llm it's you won't be able tobuild a defensible product what buildsyou what the only thing that isavailable to you that allows you tobuild a defensible product is theproprietary data so how can you leveragethat proprietary data whether it'sthrough retrieval augmentation whetherit's through uh customization where youfine-tune the modelsum so think about how you can use yourproprietary data excellent I love thatand then I'll ask one more specificallyon maybe use cases you talked about itwas called Coral right okay so if youwere to look at if you were to suggestuse cases that you've seen verysuccessfullyum that you might have done with fromcareer perspective and helping customerswhat are two or three that kind of standout to you that tend to be prettyhorizontal in nature can drive a lot ofvalue for peopleum I think one of the use cases isbasically just customer support we workvery closely with live person liveperson is a company that basicallyprovides intelligent conversationalagents for many platformsum they have been in business for awhile so they had like massive volumesof conversational dataum and as I mentioned like one of the isthat they could actually leverage thatdata is that they were able to fine-tuneour models on that proprietary data andthen they could so put it and put thosemodel Generations in front of theircustomers get feedback from theircustomers about how they feel aboutthose Generations incorporate thatfeedback to improve the modelperformance and have this positivefeedback loop soum that's like one of the examples of uhhow how our models have been widely usedforeign"
}