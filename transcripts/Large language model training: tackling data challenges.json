{
    "title": "Large language model training: tackling data challenges",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML119"
    ],
    "video_id": "s45kxeRiYPo",
    "time": "Aug 29 02:15 PM - 03:00 PM CDT",
    "transcript": "foreign[Music]Welcome to our session our panel onlarge language models and the role ofdata in training and tuning largelanguage models my name is Ali arsenjaniand uhwe have a very illustrious panel todayof Partners I lead Google's AIML partnerengineering and very happy that you canjoin us here todayso I'm going to go around and have ourpanelists introduce themselves startingwith yourselfhi everyoneI'm peroma one of the co-founders atsnorkel Ai and really excited for thediscussion todaymy name is Ben blast I'm a productmanager at mongodb focused on AI and MLand vectors hello everyone this is Chiefproduct officer for neo4jof companies like neo4j mongodb andsnorkel AI here with us our partners inour expanding ecosystem of developinggenerative AI applications and AIMLapplications in general so um one of thethings I wanted to ask you all today iswhat are the use cases that you'reseeing that's resonating with yourcustomers and what what is the uptickwhat are the key use cases maybe we'llstart with yusu dear okay thankfully forthe question and uh so uh I'm part ofneo4j neo4j is a graph analytics anddatabase companyorganization use us for organizing theirknowledge into knowledge graphs in formof entities and relationships and how tobring all these Technologies togetherand as we see generative AI capabilitiesmature one of the things organizationswant to do is one democratize access toall of the data that is available therelationships and the information that'sthere in the Enterprise so for exampleuh one of the things that our customersuses for is uh you know customer Journeyon their websites like how does acustomer come to the website how do theyTraverse through what they've done and alot of times there are a lot ofquestions that need to be asked aboutwhy did this person come and did notcomplete the transaction in the cart orsomething like that and today whathappens is the access to that kind oftechnology is limited to experts who canwork with the data who can write somecomplex queries and all and so thisthere is a whole move to can wedemocratize access to that using largelanguage models more natural languageexperience on it another great exampleis we're working with largepharmaceutical company they have acomplex supply chain and so theyrepresent their whole supply chain as adigital twin in form of a graph and thenlike in Florida what happens ifhurricane comes what would be the impactof that if like let's say you are unableto go and fulfill from that distributioncenter for two to three days what shouldwe do so those are the kinds ofquestions you can ask now there are 300people who can actually go Traverse agraph and actually use graph tools butwith large language models and like youknow natural language we can extend itto thousands of people in theorganization those are a couple ofexamples of what uh what we are seeingin the in the industry those use casesthat's awesomeum that's for neo4j knowledge crafts andits relationship to large languagemodelssure so um I work at mongodb and we haveAtlas the developer data platform andyou know our whole purpose is to make itas easy as possible for developers tobuild applications and there areobviously a lot of new uh generativetools in the space and we're looking tomake it easier and easier to consumethosethe kind of key use cases that we'reseeing right now are focused around ragarchitectures right kind of bringing incontext learning so that you can buildchat Bots and support utilities we'realso seeing a lot of natural languagegeneration that relies on some Corpus ofprivate or sensitive data and then wesee you know generative Technologies andkind of advanced embedding modelsbringing about kind of improvements andrecommendation systems and anomalydetection and so we're seeing a widerange of of those use cases for you knowrag architectures we see a lot of chatBots you know large insurance companieswanting to vectorize their knowledgeCorpus and have either their supportsystems or you know something that acustomer can interact with directly usethem we also see a lot of kind ofspecialized content generation sosomething you would in the past have tohave an expert fill out a form now youcan have an expert use a tool topre-fill a form based on some Corpus ofinformation that gets fed into a largelarge language model I think some reallyinteresting things around anomalydetection uh like detecting if a car ishaving a problem based on you knowmatching the signal of its engine toothers then just personally uh you knowthe headshot up there was generated uhwith uh degenerative technology becauseI didn't have one so that was mypersonal use caseit's a good wife manI was about to mention that because youtold me if you didn't mention ityourselfum uh so a little bit about snorkelsnorkel is a data Centric AI developmentplatform what that means is we help youdevelop your data label your dataprogrammatically allowing you to do thisyou know hundred to thousand timesfaster than manually labeling your datatrain a model analyze where it's goingright where it's going wrong and theniterate and correct the data that youhave to get the best performance out ofyour model so in terms of the use casesthat we're seeing I'll actually split myanswer into two large bucketsone really interesting one that we'vebeen seeing and working with customerson is this idea of building your ownlarge language model kind of making itcustomized given all the data thatvarious Enterprises haveum you know something that's becomereally popular now because you have allthese large language models that are soextremely powerful the kind of Major Waythat enterprises are differentiatingthemselves and kind of getting an edgeover each other is the data it's thedata that they own that's what lets youreally own your model you know customizeit based on what you have access to sopeople are building out you knowsomething I've heard this term gptumaking a large language model that theycan then use as a basis for all thevarious Downstream tasks that you'reinterested in in terms of the downstreamtasks you know we've worked on a varietyof use cases across a range of verticalsa couple of the financial institutionswe work with you know five of the top 10U.S banks they're using snorkel andagain focusing on that data piece rightthe part that they really own and cancustomize to do analyzing reports detectfraud in the Life Sciences base we'veseen things like doing clinical trialanalysis and you know using that toimprove demographics for trials that arebeing run which has become a reallyimportant topic recently and thenfinally in the insurance space we'veseen people be able to again focus onthat data Centric way of developingtheir data to create a model and youknow cut down on claims analysis Time byup to 50so this combination of using your datato build models for a very specificDownstream as well as these generalizedcustom large language models forEnterprises super exciting to see what'shappening in that space excellent thankyou yeah just one more thing to addright I think one of the otheradvantages that we are seeing is thiscombination of structured andunstructured data we've been talkingabout it for a very long time but how doyou blend these things into a singleinformation set and do Discovery acrossit we call it implicit and explicitimplicit is where you know the structurewe know the relationships andunstructured is I have parts manualsitting across the whole company in PDFform how do you take those and also likeyou know store it as vectors and havethe relationships mentioned so now youcan do a discovery of hey tell meeverything about a part that looks likethis and this has this kind of adescription and you can do a discoveryacross the vectorized search plus therelationships that are maintained in thedatabase I think those combinations ofthat we're seeing that in oil and gas wesee that in some of the largemanufacturer for federal government forMissions and stuff like that where youhave massive amounts of parts that gointo building uh like you know singleproduct and stuff like that that makes alot of sense you know combining theunstructured and structured the implicitand explicit as you termed it makes alot of sense to combine them togetherbecause the information is spread acrossthe Enterprise so I want to uh Benmention one thing how many people haveheard of ragraise your handokay fantastic that's excellent so uh Ididn't hear it I thought you might haveheard it incorrectly it's not rad it'srag I'm kiddingand so with retrieval augmentedgeneration how many people are using areyou using retrieval augmented generationraise your handvery good uh it's it's it's increasingeverybody's aware of it that's the firststep but you know maybe uh 15 areactually using it that's that's reallygood now across the life cycle for abuilding or leveraging generative AIwhether you want to customize a model uhor leverage the data that you have onGoogle Cloud your data is your data it'snot going to go anywhere else it's notgoing to get leaked outdata models and embeddings and code arepreserved within the secure Enclave ofyour Cloud environment and when weinteract across the life cycle you willhave very specific needs to interactwith Google's partners potentially forvarious types of things you may want tocreate a knowledge graph that go toneo4j you may want to vectorize the datayou have to combine the structurenon-structured go to mongodb for exampleand then if you want to prepare yourdata for tuning and creatingdomain-specific models we have snorkelAI so but along this journey there arechallenges and I want to tap into theexpertise I have here on this panel totell us what your experiences are interms of the challenges and difficultiesthat you're seeing your customers areexperiencing this time I'm going tostart with Barbara go ahead yeahum all my answers are going to startwith this two-part split but you knowagain being in a space where we'reseeing people really focus on the datato build out their models there's twobroad questions that we see come up fromour customers that we talk to one iswhat data do I use right you have thesegeneralized models there's they've beentrained on some amount of data that'sout there is that the correct type ofdata you want to use for your kind ofEnterprise production level use case doyou need to edit it do you need to checkyou know how it's performing on yourspecific tasks do you want to customizeit again on the data that belongs toyour Enterprise another place where dataactually has a big role to play isevaluation I think something we've seena lot with these generative AI models isthese really nice demos right you seethese demos you upload one PDF or twoPDF and it looks great it's amazing itdoes perfect now scale that to a milliondocuments scale that to 10 milliondocuments how do you know it's going todo well on each and every single one ofthose one option is to you know you goahead and label 10 million documentsthat's not feasible so you have to besmart about how you're using your datafor this evaluation piece as wellanother thing that's come up is thisthis idea of bias if you don't have fullinsight into the data that these modelsare getting trained on you don't have asense of you know whether your modelmight be biased or not you know thereare Parts there's things that the modelhas learned that you might not want tokind of pass on to your end users andyou your own customers so that questionof what is the data whether it's forevaluation purposes for training purposeand how Knowledge from that data isaffecting your use cases where you'reusing generative AI That's a really bigopen question and I think a challengethat that we'll see as people startusing generative for production usecases one more is how do you leverageand you know especially with a sudiraand Ben were saying how do you leverageall these different techniques that areavailable to kind of push your knowledgeinto these models there's raggingstructured data unstructured dataprompting fine tuning how do you knowwhich kind of method to use when you'reworking with your data and when you'reworking with your models and how do youdo that efficiently you don't want to dothis in a whack-a-mole manner right likepick one okay let me try it for twohours didn't work let me try another onelet me go back so what we're doing atsnorkel is really making that processmuch more guided much more efficient toto make it a lot more structured for ourdata scientists and our domain expert tobe able to build these modelsexcellent on the data side on thebeginning of the life cycle when you'recurating and preparing data there's somany concerns right you know youmentioned bias in the data you mentionedskew in the data potentially what's thedistribution of data that you need andultimatelyis the data credible right is itgrounded in factuality so factualgrounding is another key thinguh sure yeah so I think you know fromspeaking to a ton of my customers Iwould say the problems that I hear themost break into into really two groupsone is is knowledge and the other istools and so the first is knowledge alot of people don't know you know what'spossible with you know thesefoundational models that have come outalmost you know every engineer is beingasked to build something inside of adomain that you know they weren't in youknow just six months ago and noweveryone you know has to have kind of AIcapabilities and so knowing what whatcan be done and what should be done isdefinitely a challenge the the other bitabout that is then all of the techniquesandum kind of Concepts that you need tounderstand in order to do a lot of thiswork so for retrieval augmentedgeneration you know how do you chunkyour dataumyou know what is going to give you avery good uh response back to theninclude in the context that you provideto your large language model these areyou know really challengingum you know obstacles to get through inorder to deliver an excellent userexperience and kind of just alongsidethat is the tools bit and so toolsthere's all different types of securityand compliance concerns and you knowthat's one of the things that we're kindof eagerly interested in from a mongodbperspective which is how can we makesome of those things just go away andlet developers build applications moreseamlessly and so that kind of very muchevolves out of the developer dataplatform that we have but the reality isthat there are new tools and you need toknow about those new tools you need toknow how to use themum you know things for chunking uh youknow there are the new Frameworks andlibraries that are very popular such asthe Lang chains and llama indexes of theworld that people are learning a lot ofthese Concepts out of then in some casesputting to use then needing to againkind of operationalize and and use andsothose are kind of like the two groups ofchallenges that we see most frequentlyand what you know my customers aresaying to meI agree with both of you and I will justadd a couple of other points I thinknumber one is it's it's about groundingor hallucinations right so everybody isworried about what happens if the answerthat is coming back from large languagemodel inaccurate and what do we do howdo we ground it how do we make sure it'saccurate and this is where separation ofconcerns kicks in about what goes inlarge language model what's going tostay in the knowledge base within yourorganization are you going to ask llmfor a questions answer and then validateit are you going to go ahead and justbasically use it for vectorization andthen ask get the actual facts directlyfrom the knowledge base and then uselarge language model for more naturalexperiences so that is the whole areaand there's a lot of back and forth alot of open questions for for customerspeople are trying both different ways wesee in the phase one of this journey I'mpretty sure innovation in this space isgoing to only grow faster and faster butin this phase I see the separation ofconcerns and how you got into groundingbetter so that's one area I think isthere the second area from a dataperspective is the day data inEnterprises changes all the time andtraining large language model on the Flyis not possible in the context of yourprompt and the session you can go aheadand have some level of information butthe facts are changing all the time andso how do we make sure where you shoulddo which decision making and this iswhere we need better guidance foreverybody and from our perspective it ishow do you make like you know factualinformation versus natural language sothat is one area I think is is coming upagain and again and the third one I willtalk about is there are a lot oforganizations and we work with some ofthe largest federal governmentdepartments and all that and they havethis air gapped kind of an environmentsand so in that scenario then what modeldo I run how do I run how do I developsomewhere but not use the data that isthere and that you mentioned about datasecure to your data is data that is abig concern for everybody and then howdo we solve for that together I think isgoing to be the third one I will mentionawesome one one area that I think uhdon't forget what you're going to saythat I was going to ask sudiron is sothis this Confluence of knowledge graphsand llms how does that look like whatlanguage models together with knowledgegraphs I think there are two aspects tothat right one is if you believe allyour Enterprise knowledge has beencaptured in a Knowledge Graph and whereyou know what the facts are and all andyou know large language models are greatat generative languages like you know II always talk about when you went toschool it was like English class and amath class and math I learned about hereis the formula applied this wayotherwise you're going to fail andEnglish was like be more creative and Ihave tough time being creative so so Ithink the two different ways of thinkingand I think I think about large languagemodels a great way to continue teachingit getting it better and better but itis built for generating new informationand new things out of the existinginformation and then knowledge graphs orknowledge bases are a great way offactual information and the way they cancome together is one you can trainmodels better when you know what yourentities and relationships and whatthings are there but also grounding as Iwas saying like making sure that you usethe actual Enterprise no knowledge whichcan be updated in real time and you cankeep everything current and that becomesyour factual responses and all that andI'm looking forward to more innovationin this place with Lang chain aroundtooling and can you go ahead andintegrate both of them more seamlesslyand all that but that's I think uhbetter coming time that's great andspoiler alert we have a Blog on that youcan you can look uptwo things I wanted to double click intoI think on the topic of knowledge graphsand groundingum Ben actually called these modelsfoundational models right and they'refoundational for a reason it's becausethey provide a really good basis but youhave to build on top of that you knowpull in Knowledge from these knowledgebases knowledge graphs something thatwe've seen a lot of our customers do isuse ontologies especially in the medicalfield right so you have ontologies youhave these knowledge graphs then youhave just domain expertise you knowwe've had clinical researchers use ourplatform to build end-to-end AIapplications how do you combine all ofthat while taking advantage ofeverything that these large languagefoundational models give you is reallyimportant theI wanted to double click into wasadaptability right the amount of datathat these models have been trained onyou cannot go back and change that everytime you have something new come upum one of the banks that we worked withduring covid you know certain policieschanged and they had to adapt theirmodel if they had gone back and justchanged the data to fix their model itwould have been six months in today'sday with the type of models we havethat's you're talking years right it'sjust not feasible so having thoseefficient ways like we've seen withsnorkel to be able to adapt to changingbusiness needs to changing datadistributions SKU like you'd mentionedearlier that becomes really reallycritical in this timeone thing maybe just to add to this isum you know something you just mentionedBrahma about kind of adaptability iswhat we see as being really interestingwith regards to some of the largelanguage models is the fact that youknow us as developers and Builders wantto expose new services and Suites to allof our users rightum but you're not going to take all ofyour users data and train one model andturn it loose on all of your users rightso so it can tell you know me that Ihave blonde hair that would be unhelpfulum and so what we see you know happeningin the future is the use of databaseswith Vector search to allow you to kindof personalize the responses from modelsin a very secure way such that you knowyour session with this model can havecontext about you but that model is onlyaccessing that data within the contextof the session within within whichyou're interacting and never elsewhereright and so that also gets to the pointof like where you want to think aboutkind of maybe find tuning a model ormaking it better at handling some tasksbut never you know releasing anindividual's information inside of thatand kind of you know kind of pollutingthose two different needs which isreally kind of one security and privacyand the other you know making somethingmore performant for a certain set oftasks or characteristics yeah Iabsolutely I think to kind of draw asummary if I were a long large languagemodel and you would prompt me for asummary of what you all said I would saythat as an expert in knowledge graphsand vectorization and data preparationthere's this spectrum where you can doprompt engineering it'll take you so faryou can do retrieval augmentedgeneration or it's variations like flarefetch you know look aheadum by by having a factual grounding in aknowledge graph that you look up atruntime so to speak before you give itto the model you can have vectorizedcapabilities as Ben was saying whereyou've already created the embeddingtoward the embedding possibly combinethe structure and unstructured data sothat when you're doing the retrieval youcan have access to that data but all ofthese would be possible if the next stepwould be if you fine-tune a model if yousee this recurring over and over againon the same domain it might make senseto find two of the model as apartmentwas mentioning which is the ability totake that model and just teach it enoughin that domain so that you're notcompletely doing a full fine tuningyou're either doing an adapter tuningspeaking of adapting or doing somethinglike a low rank adaptation capabilitywhere you're creating a smaller modelthat can satisfy your needs in the shortterm and you don't have to you knowshoulder the burden of a large languagemodel that's where a cloud provider likeGoogle can support you but give you theability to create those adapter modelsfor your specific domains and then letleverage the capabilities of ourpartners here at each stage across thatSpectrum did I capture that okay yeahokay I'm just a large language man veryaccurateso as you as you take this journeyforward in your products and yourjourney with your customers what are youwhat are your plans or how are you todaybuilding on top of the Google AIplatform in order to accomplish yourfutureuh kind of needs I'm going to start withBen now and then I I'll it's a 50 50slit who goes afterwardsvery just of youum yeah so we're thrilled witheverything that's coming out of vertex IAI we're you know super excited aboutthe model Garden for us what's kind ofyou know closest is the new embeddingmodels like Gekko which are Superexciting for us to use inside of ourVector search offering and so uh we'regoing to plan you know deeper uh andmore easy to use Integrations and kindof further enable our developers to takeadvantage of these new technologies thenlarge language models broadlyum you know we want customers buildingapplications we see them kind of drivinga lot of new applications being built onthe platform and so making it easier toconsume those Services utilize you knowGoogle's various Services inside ofvertex AI is kind of directly alignedwith what we're trying to do and so wesee a journey you know starting withembeddings uh and getting you know muchdeeper there and then moving into largelanguage models and and furtherawesome thank younow this is going to be a surprise toeveryone I'm going to give the audiencea quiz a very short quiz what is the newEST window size that you can input intoa palm modelif it's is it 4K raise your hands is it8K raise your hands is it 32k raise yourhands you guys are awesome and fantasticit's 32k awesome very good so are youguys by the way uh now when something isoutput of the of a large language modelwhat about that token size how many howmany K's of tokens is it 1K tokens is it2K 4K or is it 8K tokens yes it's 8Kdon't don't hesitate we have 8K tokenscoming outand that's sold to the gentleman fromkidding yeahyeah excellent so we have 32k contextWindows 8K outputs and hugely enhancedmodels that are being constantly updatednow with that I'm going to go toParmaum I one of the things I was going tohighlight was the inference piece thatyou would actually mentioned you knowlarge language models are really greatwhen you want to build a large number ofkind of specialized uh task specificmodels on top some of the customerswe've worked with they just want youknow one very specific use case youdon't need all the knowledge that's inthese large language models you don'twant to bear the cost of kind of theinference and everything else that comeswith it so they've started distillingthese models for just the knowledge thatyou need and having you know Google AIand the Integrations that we have hasmade that much simpler right it's it's aone-click kind of process to go fromtuning one of the largest models to thendistilling it down and having thatflexibility in the platform due to theIntegrations with Google has beenamazingum one very personal story I have toshare is when we started with snorkelone of our first real use cases was withGoogle ads so this partnership has hasbeen there for decades now and somethingthat's very very close to our heart youknow being able to see how real usecases were being built you know what thekind of pain alongside data the datachallenges back in those days whereasand kind of really learning from thathas been great the last two things thatI'll mention is again similar to whatBen says the integration with vertex AIwith bigquery that's been really greatand we also have someone from our teamactually integrating Palm as well youknow and trying to see if you can takesome of the prompting from gen AI so Iknow I'm going down the laundry list butthat's that's really to say you knowthere's been so many products fromwithin the Google space that have beenso wonderful to integrate with and workwith and something that our customershave you know repeatedly been reallyreally excited about awesome excellentso before so there goes my second quizof the day so one of the problems thatwe have in large language models is theyhallucinate so the antid or at least themitigating strategy for that is factualgrounding how do we do factual groundingon large language models so youtypically do some kind of a you accessit via Knowledge Graph you access avector database you have a tune modelyou go out and do retrieval augmentedgeneration but that factual groundingneeds to be there so today we announcedhow many types of new factual groundingis it one type of factual grounding ortwo types of factual grounding raiseyour hands one or twoI feel like I'm in the eye doctor is itone or two it's two types thank you yougot it two types of factual groundingone for Enterprise search and anotherfor a different capability that we haveso factual grounding is something thatwe need to leverage in terms ofknowledge graphs in terms of vectordatabases in terms of the retrievalaugmented generation that we have sothat we can ensure or at least try toincrease the probability that the datais that coming back from the largelanguage model is credible data so letme answer it in two or three phasesfirst onesuper excited about the core dataplatform that Google has the securitythe governance the VPC boundary the axtall of that because every Enterprise Italk to they're all about how do yousecure my information that's the biggestthing the second is on the vertex sidesuper excited about model Garden becauseno one model is going to be perfect forevery use case having this ability andit was interesting to see the Llamamodel available it was a great demo thatnunchar did there but I think that onehaving optionality is super importantbecause we all have different needs andall and then the the app builder wassuper interesting you can build aconversational uh like an applicationpretty quickly I think that's superhelpful for us our customers so thoseare the three capabilities the twodifferent things we think about how wewill use uh like the vertex AIcapabilities for our own uh productenhancements and so in that the biggestthing I'm excited about is we arelooking at building our own uh like youknow fine-tuned model because we haveour own language in in neo4j Cipher manyof the graph databases actually supportCipher as a primary language it'ssimilar to SQL but more for graphs butit is a different language so havinglike a customized model that is actuallyoptimized for that which we could openup to the whole industry is a superexciting thing for us so building on topof one of the other code models that isavailable Palm two maybe or one of theother ones by llama code one that'savailable so so that is one area superexcited and then that augments intoassisted development how we can go aheadand do simplifying learning Journeys forour developers and stuff like this sothat's one area and then the second areafor for the Enterprises where this isreally relevant is how can we give morenatural language chat experience moreeasily on on their knowledge that theyhave created so without them having tobuild the whole environment train andthen fine tune and all that so I thinkone of the things we are excited topartner with Google is can we go aheadand take the knowledge graphs and likeyou know automatically create finetuning on top of that so we can do thatand then customize the models for themtoo so those are a couple of areas superexcited on the where these things goesnext that's a very Cutting Edge set ofinnovations that is awesome[Music]"
}