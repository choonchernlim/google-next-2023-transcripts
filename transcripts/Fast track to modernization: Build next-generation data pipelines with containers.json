{
    "title": "Fast track to modernization: Build next-generation data pipelines with containers",
    "presentation_type": "Breakout",
    "categories": [
        "Application Developers",
        "DEV303"
    ],
    "video_id": "uMkjxpB0Q80",
    "time": "Aug 30 03:00 PM - 03:45 PM CDT",
    "transcript": "foreign[Music]hi everyone thank you for coming to mytalk on how to build Next Generationdata pipelines with containersmy name is Carolinaand I'm a product manager at GoogleCloud I work on cloud runso we've all heard that data is at theheart of modern applicationsit's used to make business decisionsimprove products and servicesand create new opportunities and this isbecoming even more true with the rise ofAI driven applicationsbecause your AI powered applicationsonly ever going to be as goodas your dataSo today we're going to talk about threetypes of data processingon-demand data processingoh well look at thatslides that's not my slidesbut those are my slidesthank youokay ah there we golet's go a little bit backwardsso we're going to talk about three typesof data processingon-demand data processing batch dataprocessing and streaming data processingand I'll show you how to build thesepipelines using containers as our basicbuilding blockcontainers of course are thestandardized immutable basic buildingblock of modern Cloud nativeapplicationsand there are the next Evolution step inthe move from on-prem Hardwareto Virtual machines to now containerizedapplications each step progressivelyabstracting away more and moreinfrastructureand containers give you two reallyimportant thingsthe first is flexibilityflexibility allows you to use anyframework and any language so thatyou're not limited to what a particulardata processing platform offers but canchoose what's right for your applicationand your organizationand portability lets you move yourcontainer to the best compute platformfor your use caseand what I mean by that this slide showsdifferent ways that you can run acontainer in Google Cloudsay you start with Cloud run but thenyou find that you actually need accessto local disk no problem you can takethat same container without rebuildingit and move it to GK autopilotnow you can access local disk and youstill have a pretty fully managed dataplatformcontainer platform or perhaps you decidethat you need to use a processingalgorithm that requires fast internodecommunicationfor that Google batch is a great choiceso you take your container and you moveit to Google batch which is our ourproduct that supports high performancecompute use cases and you can keepmoving to the rightaccessing more and more layers ofinfrastructure but of course at the costof having to do more and moreoperational workso I'm going to State a principal Ithink you should always start with theproduct at the highest level ofabstraction that works for you and onlymove towards less managed products asneededso for container platforms that highestlevel of abstraction product is cloudrun and I'm going to talk about Cloudrun quite a bit in this talk so I'mgoing to give a really brief overviewCloud run is our fully managed containerplatformit's a great fit for a variety ofserving and data processing workloadsand we designed Cloud run withSimplicity and Automation in mindto get you as fast as possible fromprototype to production applicationand because Cloud run runs containersyou get that flexibility and portabilitythat we talked aboutso without further Ado let's talk abouton-demand data processingso on-demand data processing is when youhave some data and you need to processit on an ad hoc basisperhaps you need to generate a businessreportor perhaps you need to migrate yourdatabase to a new schemaand you probably don't want to run thison your laptop in fact hopefully youdon't actually have production accessfrom your laptopso you also don't really want to have tocreate a VM and install software and allof that you want an easy way to run yourscript in the cloudand so Cloud run jobs are a really goodfit for thisthis is how you run usin quadrant bash screen Cloud runassuming you've already built yourcontainerum using either Docker build or Cloudbuildyou run gcloud run jobs deploy you giveit a nameand you tell it where your image isthis is going to create a cloud run jobwhich is essentially an object thatencapsulates your container your code aswell as all the configuration around itand then you run that job with a reallysimple commandCloud run jobs just like Cloud run arefully managedyou deploy your container and Cloud runtakes care of scheduling your instancesand stopping all instances when they'redone so that you only pay when the jobis actually runningand Cloud run jobs work with all commondata storage solutions so that no matterwhere your data isas long as it's stored either in aGoogle Cloudmanaged storage solution or isaccessible from your VPC Network you'llbe able to process it and access itusing Cloud run jobsforeignSo eventually you might end up with jobsthat are pretty large you need toprocess a large amount of dataand sometimes you're in a situationwhere you want to get to results fastperhaps because that business report isdue you know in an hourum one way you can make this you canmake your jobs run faster is byparallelizing themwhat this means is you break up your jobinto multiple taskswhere each task is a running copy ofyour containerand each task is aware of its task indexas well as the total number of tasksand you can use that information tostructure your code in a way that eachtask does a simple computation to figureout is this input for me or is this forone of the other tasks so each taskprocess is only a subset of the inputand so I'm going to show you what thislooks like in Cloud run jobsso I'm going to create a jobmy job's being createdI pick a container and I tell it howmany tasksso we're going to run this with ahundred tasks so nothing's running yetand now I click executeand let's take a look at it so whatcloud runs doing right now is it'sscheduling 100 instances of thiscontainerand starting them and this is areal-time video so we're all going tosit here for a little bit and watch thisspin because I wanted to show you howfast this isjust like that we have started andfinished running 100 containersso of course these contain this thesample container doesn't do a whole lotbut still running this many containersthis quicklyis I think pretty uniquenow you might say Carolina that's allnicebut I don't really want to change mycode to tell each task which input toprocess and deal with parallelizing myjobs I just want to take my existinglong-running joband run it in Cloud run without havingto do anything extraand if that's the case I have good newsfor you because Cloud run jobs nowsupport up to 24 hour timeouts so thatmeans that each task of your job can runfor up to 24 hours so as long as yourjob takes less than 24 hours you canjust deploy to Cloud run and it willrent a completionso that was on demand data processinglet's talk about batch data processingnow which is much the same you have abunch of data and you want to process itall at once but the big difference tendsto be that you want to orchestrate it insome way rather than like click executeum here are some common times when youneed to do batch data processing forexample if you are maintaining even avery simple web appyou'll find that you need to umdo have various pipelines that handledeletions so that if one of your usersdeleted their datayou want to make sure you've actuallydeleted from everywhere in your systemyou want to probably periodically audityour Access Control list so that theright people have the access to theright informationyou might have some database managementscripts going on as well as businessanalytics scripts because you want tomake sure that your logs and user datais exported to a format where you canactually use it to make businessdecisionsand soone thing we did this one thing we didin Claude Ryan jobs to facilitatebatch pipelines is that we clearlyseparated management of jobs fromrunning of jobsso uh you saw earlier that you create ajob and then you run a job this lets youmanage your jobs with an infrastructureas code approach for example usingterraform or config connectorthat's how you would manage you knowupdating the container for exampleand then you can you can manage theactual running you can treat the runningof jobs as a data plane operationand so you can easily run your jobsprogrammatically using either an APIcall or a client Libraryor orchestration products like Google'sCloud scheduler or workflows so let'stake a look at thatso I want to show you how you would setup a scheduled job in Cloud run jobsso we go to the cloud run jobs page andclick triggersadd scheduler triggerand I'm going to tell it when it shouldrun the job this is cron notation so I'mgoing to ask you to run it at 2 am everymorningclick createand just like that my job's now going torun at 2 am every morning so it's reallyeasy to go from I have a cloud run jobto Let's schedule it to run regularlyas your jobs get more complex you'llprobably get to a point where you mighthave multi-jet Parts jobs that you wantto chainwhere each successive job uses theoutput of the previous oneperhaps with some conditional logic inbetweenCloud workflows are a really good fitfor thisthey're also great for error handlingbecause a cloud workflow will with thejobs work close connector will run yourjob wait for it to finishsee if it succeeded or failed and thenyou can do further handling based onwhether it's succeeded or failed so thisis great for error handling kind ofoutside the cloud run jobs retriesand you can even set up this workload aworkflow to run on a schedule so if youwant to run your cloud workflowsI don't know every every morning at 2A.M you can set that up using Cloudscheduler as wellCloud run jobs can also be really usefulfor background processing where a useraction results in some larger set ofoperations that you want to do laterfor example that deletion pipeline Italked about imagine a user deletestheir data you don't want to kind ofblock that operation you want to returnsuccess to the user and then in thebackground later you go through yoursystems and all your deletion so Cloudrun jobs are a great place to offloadsuch processes to get them out of thecritical user interaction pathand here Cloud workflows can help againso you wrap your job with a workflowand then you set up event triggering foryour workflownow you might think how do I get theevent payload into the cloud run joband so I'm excited to announce that wehave added execution overrides toclutter and jobsand so thanks to this feature you canparse out the event payload from theeventand then have your workflowpass that event payload to your jobeither as an environment variable or asargumentsso execution overrides lets you pass insome information every time you actuallyexecute the job so they overrideexisting settings in the job withoutchanging the underlying configurationyou can overwrite environment variablesand arguments and this is useful to tellthe job for example this is where theinput is located this timeor perhaps you know to pass along thatevent payloadyou can also override the number oftasksand this is useful if your job iswritten in a way that each taskprocesses exactly one piece of input soyou tell your job this time please run12 instances because I have 12 pieces ofinputand lastly you can override the tasktimeout this is useful when um so Cloudrun jobs only charge you when the job isactually running but as we all knowsometimes we make mistakes and getinfinite Loops so it can be useful tohave a task timeout set kind of close tohow long you think the job ought to runif there's no issueand here's how you actually use cloudrun jobs with execution overrides sowe've got an execute button up there butinstead of clicking that we'll click thelittle arrowexecute with overrides and now here Itell it uh what my what new values Iwant to set so I filled in here a newvalue for the bucket environmentvariable and so when I click executethis job would runso to sum this all up you want to usecloud scheduler with Cloud run forscheduled jobscloud workflows with Cloud run jobs forcomplex workflow orchestrationand event Arc with cloud workflows andCloud run jobs for either event drivenjobsor entire event driven data Pipelinesso finally let's talk about streamingdata process streaming data processingstreaming data processing is greatbecause it's conceptually simple anevent happens and you process itum in general there are two majorparadigms when doing streaming dataprocessing we've got push-based and pullbased push-based is when your queue ispushing eventsto your compute endpointand pull based is when your compute isactually workers that are running allthe time and pulling work from yourqueueso traditionally for serverlesspipelines people have used a push-basedmodel so what this looks like is youtend to have a input source for examplecloud storage or Cloud loggingthen you have a queue system that couldbe a cloud Pub sub event Arc maybe Kafkaand then you have some sort of endpointserverless endpoint automatically scaledlike Cloud functions or Cloud runthat does the actual data processing andthen finally you write out the output tosome place like for example bigquery ora databaseand this works really well there are alot of customers using Cloud run andCloud functions for this type of usecase uh it's easy to set upCloud functions for example even hasbuilt-in event Arc integrationand it automatically changed scales withtraffic and you don't pay anything whenyou don't have any eventsso you might ask should I use cloudfunctions or Cloud runand the answer in my opinion is againuse the simplest thing that worksso if you're processing code is really afunctionuse cloud functions it's the easiest toget started with you don't have to writea Docker file so you don't have tomaster build systems and it's gotbuilt-in support for eventsyou might want to use cloud run on theother hand if Cloud functions doesn'tsupport the language that you want touse or if you want that container levelportabilityor if you want control over the buildsystem for example for compliancereasonsso now let's talk about pull-based dataprocessingumyou want to use pull-based processing ifyou have a really high throughput if youhave a really high volume of messagesand need to achieve High throughputor if you need to achieve low latencyand the advantages of this kind ofParadigm is that a twofold one is thatwhen you're doing push-based processingyou have really an HTTP request forevery piece of input data right sothat's overall less efficientand so it's hard to get the same amountof performanceandum push-based processing also gives youbetter control over smoothing outtraffic spikesbecause in your streaming processingpipeline if you're using a push-basedmodel if you have a huge amount ofevents show up at a time your Q yourcompute is going to scale way uphopefully fast enough and then if you'reusing a database for example as yourbacking resource you might end up in asituation where you have too manyworkers trying to write to your databasewith pull-based processing you cancontrol this by controlling the numberof instances a little more yourselfso uh that's let's let's bring this backto how would you use this with Cloud runagain you have an input source you havea cube at this time you have a pullsubscription to Pub suband you set up your Cloud run servicethis time you want to use cloud run notCloud functions you set up your Cloudrun service with CPU always onwhat this meansby default your Cloud run Services onlyget CPU allocated when they're actuallyprocessing requestsbecause this lets Cloud run give youthat great billing model where we onlycharge you when your service is actuallyprocessing a requestbut when you're doing streaming dataprocessing when you when you have pullbased workers you don't have anyrequests coming in so you want CPU allthe time so you set CPU to alwaysallocated and then you set your Min andMax instancesnow until recently I would have told youyou want to set Min and Max instances tothe same numberbecause Cloud run scaling was builtaround request driven workloads and wecannot scale this howeverI'm happy to announce that in the comingweeks we'll be rolling out a new featurewhere I will Scale based on CPUutilizationeven when your service isn't processingrequestsso you'll be able to set Min instancesand Max instances to different valuesand as long as your workload is CPUboundwe'll be able to scale up your serviceUp and Down based on the volume ofincoming messagesso that was a lot of content aboutserverless productsbut I wanted to make sure you got to seean end-to-end example of how an actualcustomer uses this for their system inproduction to generate business insightsand so I spoke to Antoine custex fromL'Oreal unfortunately Antoine couldn'tbe here today but he sent me this greatarchitecture diagramso the team at L'Oreal loves bigquery soone of the design goalsof over here was to get their data intobigquery as quickly as possible asefficiently as possible and then use SQLand bigquery's advanced features togenerate insights and process it fromthereso starting on the left we see all thedata sources there's a lot of differentkinds of data here sales data productdata external data sources logs and soforth and they're processing 150petabytes per month these daysso the moment data arrives it triggerseither apogee or gets written to cloudstorageand that generates a trigger thattriggers a cloud run service and thatcloud run service does two things firstit writes the data to bigquery that'sthe whole we want to get it intobigquery as quickly as possible stepand then it generates an eventthat triggers a cloud workflow and thatcloud workflow takes the data through aseries of transformationswith various clutter and servicesactually doing the compute work oftransforming the data and writing theresults into that shared bigquery dataset on the rightand then to actually use this datathey've defined a few separate bigqueryviews so that different custom differentinternal stakeholders can view the rightamount of data based on you know whatpermissions they ought to haveand then finallythey've built several different ways toactually consume that data for end usersend users being you know businessstakeholders inside the company so theyhave a web front end built-in app enginethey also use looker as well asthird-party dashboardsso using all serverless componentsL'Oreal was able to build an end-to-enddata warehouse pipelinethat's increased their efficiency andlowered their maintenance costsso putting it all togetherumin this talk we showed you how to buildserverless pipelines for data processingyou want to use cloud run jobs foron-demand and batch processingCloud run services for streaming dataprocessing both push and pull basedand Cloud functions for event-drivendata processingand this is all powered by ourconstellation of orchestration productsand our great Suite of serverless datastorage productsso I hope you give this a tryon your next project[Music]"
}