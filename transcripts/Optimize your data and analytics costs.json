{
    "title": "Optimize your data and analytics costs",
    "presentation_type": "Breakout",
    "categories": [
        "Data Analysts, Data Scientists, Data Engineers",
        "ANA101"
    ],
    "video_id": "lNxIKDtPYqE",
    "time": "Aug 30 06:45 PM - 07:30 PM CDT",
    "transcript": "foreign[Music]welcome thank you so much for stickingaround for this last session of the dayI believe it is we are here to optimizeyour data and analytics costs who'sexcited for this topicthere's some energy in the room supersuper happy it is it's a pleasure tohave you all here today and actuallytoday we are going to invite a panel ofexperts also to join us in a moment onstage who will be here for a panelsession as we talk through the importanttopic of Finn Opsso before we do that I wanted myself andEric we'll do a little bit of setup andthen we'll have the panelists join usso Jenny Iwe couldn't start a session withouttalking about Jenny I it is the theme ofthe momentJenny I is everywhere and actually wasreally interesting about the topic ofJourney I and finops isso many of the use cases that I amhearing about from customers every dayaround Jenny I are around costoptimization productivity efficiencymight be your data developer who iswriting code who is now assisted by jniit might be that marketing creativeperson who is now being assisted withjourney I to create content be it imagesSEO Search terms email copyJenny I in the call center helping yourcall center individuals route throughcallsjennyi is everywhereand it is delivering on thatproductivity efficiency costoptimization those are some of the mostpopular use casesand in fact what we see from the statsfrom various companies IDC hbrEtcis that those leaders who are leaninginto Jenny Ithe data AI leaderstypically they're outperformingother companies on all the metrics thatmatter be it Revenue market sharealso operational efficiencyso as we enter this world of Jenny Iwe see tension because as we're goinginto the world of Jenny I it requiresinvestmentso as especially being in a year wherecost optimization a lot of scrutiny overbudgetsthis wave of Jenny eye has come and hitus and it requires investment yet at thesame time we're also seeing a lot ofcomplexity in our data Landscapesbecause with Jenny I comes new types ofdata unstructured datain fact it's estimated 80 percent ofcompany data is unstructured and now toreally leverage Jenny I we must be usingall of that dataand finally governance of that data thesecurity of that datawhen we speak to customers we hear youthere's a lot of drive to say hey I wantto innovate with Jenny I I want to bedoing experimentation I want to bebuilding the next gen EI appI have those experimental on-demandneedsunpredictable needs at the same time I'malso running Mission critical workloadsin fact I moved to the cloud because thecloud offered agility and flexibilityhow do I take advantage of flexibilityleaving behind the on-prem world where Ioften had to over provision capacityI want visibility I want transparencyand in fact these are all the topicsthat lend themselves into the world offinopsso as we move forward with Google's dataand AI Cloud we're very much thinkingabout all of these topicsand this is why you see in the way thatyou buy the data in AI Cloud we offerdifferent pricing models pay as you goversus editions commitments we offertiers of capabilitieswe offer different different uh types ofstorage to to manage that unstructuredlarge voluminous data versus yourstructured data so we're thinking a lotevery day about how do we deliver themost cost optimal the most performantprice performant data and AI Cloud forall of your needs today and those futureneeds with Jenny Inow I'm going to hand over to mycolleague Eric our head of finops atGoogle Cloud to say a few wordsthanks for asked me after we have the uhso as yesterday mentioned both theexplosion of data engine AIwe need a unify data open platformand we need to manage in the most costefficient mannerand what we need is really a disciplinedapproach on how we manage the Gen AIdata costSo within Google we introduced you knowand been working with many customersaround the concept and the discipline ofcloudfinops so what is cloudfinops is anoperational framework that bringstechnology business and finance teamtogetherdriving that Financial accountabilityreally accelerate the business value ofthe overall cost transformation Journeyto that extent we've been working withover 100 plus customers just in the pastyears alonein their transfer transformation Journeyto really help understand and guide themthroughwhat are the key building blocks forsuccessful adoption of Financethe first start with accountability andenablementit's really about bringing that fastfunctional team togetherto drive the standards the bestpractices on how you're managing yourdata analysts costs policies andcontrolssecond how are you going to measuresuccess regarding your gni use casesyour data workloads and so for certainlydefining those cost kpis the valuemetrics become very very importantthe third aspect is cost atomizationsunderstanding how you optimize your yourgeneral data the bigquery analytics costfrom the resourcing optimizationperspective pricing optimizations andperhaps you might need to re-architectyour entire application stack to reallymaximize the business valuethe fourth aspect of the building blocksof fin Ops is planning forecastinghow are you going to able to identifywhat is the actual cost you need as yourdata explosion increases you know timeover time so Financial forecastingbetter planning forecasting becomingequally important aspect of yourcritical pillar in a phenomousdiscipline finally is about a set oftwos accelerator bring near real-timeinsights into your day and day costenabling Automation and controlbecome an essential part of your FinanceJourneynow let us zoom into one of the keypillars which is what I mentionedcustomizationsand one of the things that with bigqueryadditions that introduced was Autoscaling and with all this Gallery youessentially can't specify your Top Linecapacity budget and bigquery what'sreally cool about this is actually itcanAuto skills based on your workloadrequirements so and it will adjust yourcompute capacity needs as your demandincreases and what's really awesome aswell is for your queries that are awritten process it can actuallyaccelerate those queries as wellthe other piece that for Bitcoin alwaysgoing it can scale down to zero so youset them in and a Max threshold and yourmean should be zero as well so that's areally cool feature that we introducewell in a finite perspective what allthe scaling does isis essentially eliminate the costof managing your entire cell dataNetwork loads from capacity in in othercases we see over a customer able todeliver 40 compute efficiencies uh overthe flat rate offerings todayas yesterday mentioned this is justanother example of how we are listeningto our customers needs and buildinggreat features to help our customersmaximize their business value on GoogleCloudso talking about listen to our customersand meeting their needswith the release of required additionswe're doing just that rightspecifically we're addressing customerneeds in three key areas the first oneis flexibilityonly paying for what you useyou can actually mix and match betweenthe chairing of the pricing structurefrom standard to Enterprise and so forththe second aspect of this ispredictabilityprovide one year and three-year commitsone key example is low-cost storageusing compressed or physical storagewe've seen many customers matter of facthave seen 10 to 15percent of uh 10 to 15x of compressionratios using physical storagethe dirt aspect is controlas I mentioned earlier with auto scalingability you can set the Min and Maxthreshold of your performancerequirements need so with that let'sturn to our key panelists here and hearfrom them on how they've been able toplan forecast and optimizing their dataanalyst workloads[Applause]okay so let's um maybe just start with aquick round of intros so maybe your nameyour role and where you're coming fromgunjan shall I start with you on myright here all right hey everyone myname is gunjanuh I'm from Palo Alto NetworksI work in the cloud Center of Excellenceteam and my focus is cloud finops PaloAlto networks is the largest cybersecurity companyso yeahexcited to be herethank you valentes hello hello yeah somy name is valentis vasilakis I'mworking as a tech lead at Mercado Libreand specifically in the cloud economicsteam I'm coming from Buenos Aireshi I'm Anthony Savio I work for Bayerspecifically in our crop scienceDivision and my current title is thedata warehouse engineering lead so veryfocused primarily around bigquery andsome supplementary services thatinteract very well with our platformthank you so first questionwe're you know we're in this world ofJenny I business wants to experimentdata teams want to experiment butactually sometimes we find the worlds ofI.T and Finance in all position becauseFinance like predictable costs they wanta budget that they can they know whatthe runway is they know it's predictableit's easy to understand yet in the worldof technology we're innovatingthere's so many people trying out largelanguage models building applicationsEtc how do you see those two worlds inyour companies coming together or maybenot coming together gunjan over to youfirstsoum it's an exciting intersectionour team cloudfinops team we are part ofthe engineering organization and we workvery closely with Finance so we get tosee both worldsumengineering side of my brain is alwaysthinking oh look a squirrel I want totry that new technology I want to playwith this new thingwhich is unpredictable in nature rightfinances this spreadsheet here verypredictableso in our company at Palo Alto networkswe have we do Business Partnershipearly on in the process so Finance teampartners with engineering teamsto figure out what things they'replanning to use and how do we take thatinto considerationand for the things that happen likeJenny I just came up you know it wasn'tplanneduh for that we take that intoconsideration for uh forecasts andplanning and how do webring in fin Ops to optimize itgreat so apartnership between I.T and businessblunt is what about Mercado Libra howdoes that partnership work cool so asMercado libret teams build abstractionlayers for the business to useso a developer is not managing aninfrastructure they don't go on theconsole they don't spin any resourcesCentral teams manage and optimize all ofit so we can talk to them and drive thestrategy with them because it's a teamof dedicated specialistsalso consolidation rather than tryingnew tools or new technology so I knowbear you've actually gone through aconsolidation across clouds you'veConsolidated to Google Cloud how hasthat consolidation versus wanting to usenew technology or experiment how doesthat balance workuh so that's a tough question I mean westill do have a multi-cloud presence nowbigquery is our centralized datawarehouse solution so we're going totake advantage of things like Omni andbig lake to try to reach out and let thedata sit in the original cloud providerbut still give us that single pane ofglass so we can actually go throughaccess do our analytics and try to youknow Drive new new business revenue ornew insights for us particularly withthis yearwe've historically used on-demandbilling model for bigquery so slots wasyou know a key Focus area and ourfinance teams have been very pleasedwith our success the last few monthshere so I think we've targeted our top10 spend projects and that was you knowa good portion of our overall Cloudspend Bill going forward we're going totry to expand that to be a hundredpercent of all of our projects becauseit's not just those you know Commonpatterns that you always see it's thatrandom data science query someanalytical process that maybe could havedone differently that ends up you knowcausing you an unpredictable bill thatyou don't want to deal withso Anthonyum since you mentioned about optimizingyour data costs so how do you go aboutmanaging and monitoring those data andresponse yeah great question I meanthere's low hanging fruit there sousing budget alerts using monitoringtrying to make sure you're keeping umkeeping an eye on your slot utilizationnumber of concurrent queries Etcadditionally having you know onecentralized team focus on some of thosenew release functionality componentsbecause not everybody's going tounderstand how they can specificallyapply one new feature that couldactually really drive your bottom linedown so last year one of our key Focuspoints was using table value functionsand by embedding that into our kind ofreusable workflow we were able to reduceabout 40 of our capacity and then thisyear we did something similar withmissilet additionshow about you voluntisso first I want to give some context ofthe infrastructure we're managing atMercado Libre there are 19 000 people ofusing bigquery every month we have morethan 290 000 data sets powered by morethan 150 000 slotsso my team or the team I work in thecloud economics team we have developed aset of tools to monitor all thesebecause deviations can hit us very hardwe have developed cost alerts havedeveloped a series of dashboards and wehave created meetings with all theimportant teams internally to know whatthey do to expect their deployments whena deviation happens we know about itbecause we receive an email and then wecan talk to themin terms of bigquery our platform Furyis 100 on slots however we have a smallpart that is off platform that smallpart we have daily quotas per user soyou cannot exceed a certain amountbefore you could and it was interestingnow you cannot anymorewe are monitoring the slots utilizationacross the entire organization in ourcase it never drops below 95 percent 97it's quite stable because certainplatform Services have been built aroundthe feature of sharing slots so theyknow when the reservation of one team isnot fully utilized another team willtake it offthat's very interesting to hear and andfor context for our audience so bigquerysold through slot compute units and wehave this concept of idle slot sharingso it's it's interesting that you havethese quarters by user but actually theidle slot sharing means each user canhave a quota but if somebody else isn'tusing their quota the you know afree-for-all yes so actually the quotais for the analysis part for thoseprojects that are not covered with slotswe allow them to use analysis but notexceed a certain threshold all theplatform is with slots due to theamazing team amazing work of certainteams that centrally manage all thispredictable workloads or enable yourteams to do experimentationum similar to what Volante said we wealso enable slot sharing idle slotsharingin the default poolsbut also for unpredictable unpredictableworkloadsum we also said a bunch of alerts andautomation of flex slot purchase as andwhen needed for those unpredictableworkloadsgreat and we and we actually do see anumber of customers do that they use thepay as you go on demand kind of modelsin concert with annual three-yearcommitments the annual three-yearcommitments are great for those verypredictable Mission critical workloadsand it sounds like uh Palo Alto Networksyou're you're using that pay as you goon demand to support that ad hoc needamazingumso in a cost optimization climate whereyou know this year when we hit this yearevery organization I spoke to thoughtthere was a recession coming budgetswere being slashed where we need toreduce cost for storage for computebut with Jenny I coming has investmentopened up new investment come into IDhow are you how is your organizationlooking at now in a cost optimizationyear doing gen AImaybe start with yourself volantis yesso optimizing is very big in MercadoLibre nowadays our motto internally isgained efficiency to continue winning soeverybody is really really focused onthis where my team is searching for acost optimization opportunities acrossour multi-cloud environment activelywith alerts with the monitoring and weare measuring the time it takes us todetect an opportunity or an alert andthen the time it takes to the internalteam to implement a fix or to write anew solutionwe then measure the Delta of the costbetween that occurred between the timewe let them know and the time theyimplemented the sharing the the newsolution and annualized savingsjust to give you an idea of what anannualized saving is we did have analert earlier this year about storage ina specific project then we let the teamknow they didn't knowthe cost grew grew grew eventually itgrew to more than twenty thousanddollars a day extra costthey did work with a Google team theydid Implement a solution and eventuallydropped back to a few hundreds ofthousands a few hundreds of dollars andif you make the math an extra cost oftwenty thousand dollars a day it's sixhundred thousand dollars six hundredthousand dollars a month or more thanseven million dollars a year so it's abig deal monitoring those alertsthat's amazing so setting up it soundslike really sophisticated actuallymetrics on top of data and workloadgrowth setting up alerts so that as ifthose are moving and going above certainthresholds you're actually doing somereally close monitoring and and helpingthe business to manage their costscorrect correct because with so manyusers when the alert goes off it's a bighit yes we're not talking about tendollars twenty dollars fifty dollarswe're talking about thousands of dollarsif we do not see it coming it will hitus very hard so we have team we havepeople dedicated in doing thatany thoughts from yourselvesum so from our perspective I think we'vekind of taken a heavy emphasis on howwe're actually creating our projects sonot having one centralized project we'refree-for-all ad hockey and automatedprocesses are running but actuallysegmenting those that fits very nicelyso we understand what are thosepredictable workloads and we understandexactly what that capacity is but thenfor the unpredictable ones we canSafeguard ourselves from actuallyincurring those costs so trying toforward think how you want to Definethat resource hierarchy will actuallypay immense dividends over time becauseit gets much more difficult to breakthat out once you already have it up andrunning and people are using it day overday so having that Forward Thinking candefinitely go a long waythat sounds great so that'sthat's projects as a way of splittingdifferent workloads and being able tomonitor potentially even do chargebackmodels around thoseexactly so that's a really good point sowith slots essentially all of your billsgoing to show up under that that BQ slotadmin project so one of the techniqueswe've used which kind of ties back tofinops is actually calculating theamount of slot hours being used perproject within each reservation and thenessentially getting a percentile thatthen we can bump that up against ouractual slot Bill and Define this projectis going to spend x amount of dollarsand then with fin Ops tagging as we'resaying at the project level we canactually have that relationship andunderstand however deep we want to gowith in our org this is where the costsare occurring yeah Anthony thanks forsharing that you know one of the commonchallenges that many of our customersface today especially around you knowcost allocation for bigquery as a dataanalyst workload is considered as moreof a sheer Services platform how so ableto go into that level of granularity ofallocating thoseslots by usage back to the individualline of business become very importantso so yeah I think that's a great ideaand I think perhaps to to gungeon rightyou know going back to the AI questionsright at Google if it was an advantageright that we bring AI to your data andI think that's one of the key things ofcustomizations cost savings but alsoyou're not you know incurring anynetworking grass costsand the other aspect is also security sohaving said that how do you go thinkabout securing your data you know as youdeploy your genif AI modelsso like many other companies right nowuh Jenny is the boss everyone's playingaround with itumwhen I think about it it's any projectthat has to do with Gen AI 70 to 80percentis a data prep work before it even getsto the alarm part need to do a datacleanup organize your data all thatstuff rightuh we at Palo Alto networks we have aalmost an exabyte of data and bigqueryso we are operating at that scaleand as a larger cyber security companywe have to think a lot about datasecurity data privacyso we're taking that extremely seriouslyso with that big of a surface area ofdatait's always better to bring AI to yourdata than takeall that data to the AIso that's an advantage with bigquerythat's great and especially as Andrewyou mentioned earlier bigquery Omni aswellum so so you know one other key thingthat would love to hear from each of youmatter of fact is can you share some ofthe learnings and best practices rightum as you were going through thisjourney thinking from a finiteperspective customization so what aresome of the key Lessons Learned so maybeyou start with your engine uh sure so Ithink we touched on a couple of them sotrying to think through your resourcehierarchy and how you want to break outthose projects having that fin Opstagging approach kind of baked into thatupfront model and not trying to reactand apply it after the factclustering partitioning is going to becrucial for bigquery and we've kind oftaken a mode of we're going to defaultwith clustering across all of our tablesBecause the actual negative impact isvery minimal but the positive impact isincredibly substantial whether or notit's the actual field you're filteringon or not we're still seeing a prettysignificant return on on that that spaceadditionally pay close attention towhat's being released and and some of itmight be intimidating from the listprice buttake that take that risk explore learnsomething new and if you're able to doit successfully and apply it and scaleit out across your entire organizationyou can definitely reap some benefitsthereso to all the platform teams that managebigquery centrally I believe you shouldall build the guard rails that help youavoid altogether the impact of mistakesor minimize it because mistakes happenwill happen users will do queries thatare not excellent and you can help yourcompany avoid the costthat's why you should buy slots youshould plan your workloads by computepower so you drop the the cost of thosequeries and using information schema youcan actually see what is happening youcan see per table there is partitionedif it's clustered if not and you canoptimize those environments to justreduce your costI'll talk about two things at a highlevel one is a high level recommendationand one is a specific that was alreadysort of covered so at a high levelI would say work on your data Foundationyou need a very solid data Foundationwhich meanslabeling tagging your resourcesand your organization structure hygieneyou need to have a very good hygienethereso it might seem like an overwhelmingamount of work but it's worth the effortafter that for specifically for bigqueryunderstand the pricing model andnot just understand what it says in thedocumentation but have a mental modelfor bigquery pricingand have a common Mentor model that yourfinops team has Finance team andengineering teamso you are speaking you're all speakingthe same languagethird one is Define and measure kpis soDefine agreed upon kpisand do a detailed measurement and inmeasurement there's two areas one isbillingso you need to have an understanding ofwhere you're spending moneywhat which skews which operations whichprojects are spending the most moneyum and the second one isum usage so slot monitoring Etcuh there is a great project from Googlethat's on GitHub about slot monitoringwe have implemented thatbut both of these will give you an ideaof where you need to work on there's alot of recommendations out therebutwe are a huge believer in80 20 rule so 20 of the query is 20 ofthe projects that end up costingeighty percent of your bill so you needto figure out what those are whichqueries are costing the most whichprojects are costing the mostand one specific recommendation asAnthony mentioned earlier uh clusteringandpartitioning you can think of it assort of groceryum if you're trying to buy cheese youknow which I'll go to right it's likeall the dairy is clustered together itmakes it easier for you to go and finduh cheese ormy favorite candy aisle but it also hasan impact on cost sothis is where we are also exploring genAI to see how what is the data accesspatternand if we can use the Nai to haverecommendation on how to Cluster andpartition the dataamazing it sounds like analytics on yourdata and analytics understand how datais being used well with an exabyte ofdata it's really important I guess toreally understand where that data iscoming from how it's being partitionedstored managed and then on top of itanalyzing also from what volantis wastalking about and yourself how arepeople using that data setting up alertsand being beingum thoughtful about the projects thatyou set up I I know at Bear they have areally sophisticated naming structurethat helps with that chargeback modeland I wanted to emphasizefrom a product perspective as we look atbigquery we're hearing a lot of feedbackaroundthings like chargeback models how can wehave kind of visibility of who's usingslots or even who's using idle slots andand have chargeback modesum we're hearing thoughts around thingslike alerts and metrics and actuallywith our roadmap and where we're takingum bigquerywith no duet AI there's a step of yesproviding some of those controls tocustomers but then automating some ofthose so when a user does write anon-optimal query can do it yeah Iactually rewrite that query to be moreoptimal because it knows your data itknows the schema it knows the partitionsor indexes so I'm super looking forwardto what our engineering teams areworking on in the space of duet AIbecause I think it not only will providesome of the controls but actuallyautomate some of that for for in thefinal space so that's all we have timefor today on the panel I want to thankyou again for sticking around for thelast session of the day and thank you toour paneliststhank you"
}