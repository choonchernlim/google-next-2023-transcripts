{
    "title": "Petabyte-scale data migrations the Google Cloud way",
    "presentation_type": "Breakout",
    "categories": [
        "Architects, IT Professionals",
        "ARC214"
    ],
    "video_id": "8uK6bWCGnrk",
    "time": "Aug 31 12:15 PM - 01:00 PM CDT",
    "transcript": "foreign[Music]good morning good morning all of youhope you had a great evening yesterdayand if this is your first session todaymorning then welcome to day three ofcloud nextI'm sure by now you would have seenvarious different very interesting AIMLand analytics demos and Innovationswhich Google cloud is bringing to themarketin fact if you haven't seen the Wendy'sdrive-in at the expo hall I reallyencourage you to go and try this it'sreally interestingand I'm sure many of you want to go andtry out some of these projects get itstarted in organizationbutand there's always a part for any ofthese projects to Really Happen thefirst step is moving terabytes or maybepetabytes of data to Google Cloudand if you're wondering if this datatransfer at scale is easythe answer is yes and this is thesession we are going to show you howmy name is manjul sahai I'm one of thelead product managers for Google CloudStorage I'm joined by my colleagueajitesh who is a product manager fortransfer servicejoined by hilame Beren from Global oneof our Marquee stream customersfrom our partner thoughtworksand togetherwe're essentially going to cover threethings number onegive you an overview of Google clouddata transfer portfolioand how it makes transfer really easysecondwe'll talk about best practices formoving data from on-prem to Cloudas well as for setting up multi-clouddata pipelineand finallywell here the real worldstory of global and how thoughtworkshave them migrate to Google cloud andit's a very interesting one andhopefully it brings together what wetalk in the earlier parts of thepresentationgreat sobefore we get started I just want torecapture that data transfertools what we talk about today is a partof our storage portfolioand storage Computing networking all ofthese together from our workloadoptimized infrastructure offering fromGoogle Cloudand once againthese services are built to help you runyour aim in the workloads for sureas well as modern container workloadsand are equally well set to run yourexisting workloads like sap VMware andall thatso let me start it with the firstsection to give an overview of the datatransfer portfolionow there are multiple use cases andscenarios for data transferso there are customers who move datafrom on-prem to Cloudand they may run their own customAnalytics tool on gke GCEand there are other customers who areclosing data centers and moving theirworkloads to Google Cloudboth of these cases data transfer is aproblem to be solved forthere are many other customers who aremoving data across cloudand they're setting up multi-cloud dataPipeline and you will see some veryinteresting data points on this maybejust another two slides down the downthe presentationagain they want to take advantage ofGoogle Cloud's leading industry-leadinganalytics productsand then there are of course customerswho are moving applications from otherCloud to World cloudthere's also the third scenario which iscustomers once they have their data inGoogle cloud in Google Cloud Storagefile storethey want to manage that data better andthis could be for Dr reason backupreason or you know archivaland the tools we will cover today arethe services we talk about todayalso help you catered for this thirdcategorybut in the presentation we are going tofocus on the first two scenarios andshare more on thatnow data transfer at scale is not easyif any of you have done this before youknow that performance is a big concernhow do you make sure that you use themwhole of network connectionand don't use so much that you causeproblem for your running applicationyou want to make sure your compute andnetwork is optimizedyou of course have to on many timescustomers end up writing scripts whichthey have to manage upgrade all of thatandhow do you make sure that data issecurely transferredmake sure that the Integrity is thereand validated you don't want to deleteyour data from the source and then youfind out there are some issues on yourtarget sideso all this is toughand the other one which is not there butthere's always a big elephant in theroom costhow do you do this transfer at the rightcost and not end up where the transferitself is 20 30 percent of your overallproduct and you know leads to Bigquestions on Roiso moving data at scale is complexthe good news is the Google Cloudproduct portfolio in data transfer ispretty robust and simple and helps yousolve for thatso first we have transfer Appliance youknow I was in a customer conversationjust yesterday you know they're usingthis appliances very well they move datafrom their regulated environmentson-prem scenarios to Google Cloud usingtheir clientsthere's another very interesting variantof transfer Appliance if you're notaware so if you have data gettinggenerated in Edge like we have acustomer who are using this applianceswith Rovers you can have the data cometo Appliance in an offline scenario andthen whenever the appliance is put in aconnected environment it will move datato Google Cloud so there's this hybridoffline online approach also availablewith head clientswe have gcloud storage tools which werecommend for small data transfers forboth on-prem to cloud or within GoogleCloudand then our most popular tool mostcommonly used by customers is storagetransfer service which essentiallycaters to all the three scenarios whichis on-prem to Cloud within gcp andacross different Cloud so this is simplethree product portfolio but very robustbeen around for a couple of years tocater to these requirementsnowI believe and you know we think thatthese products really make it easy butnothing better than that you know ratherthan taking my word why not you knowhere what customers have said about thisso today is the first time we aresharing this publicly that we havethousands of customers multiplethousands I don't mean one thousand oneonly thousands of customers who aremoving data every monthfrom AWS and Azure to Google cloud andmost commonly they use it for runninganalytics workloads on Google Cloudthousands of customers and we have hadthis pattern so these customers havesome of these workloads running on otherCloud they have set up this regularmonthly data Pipeline and then they usebigquery vertex AI data proc these toolsthese services on Google Cloudso if any of you have been consideringthis kind of a multi-cloud scenarioabsolutely feel confident that there aremany other customers who do it today andthey've been doing it very successfullyat scalenow if we talk about on-prem to Cloudagain one of our esteemed customer TimBrazil they move petabyte scale of datafound it very easy seamless saturatingat 20 gbps connection so again someproof points to share with you thatthese tools have made customers lifeeasy help them do this transfer at scalein a seamless waynow let me talk about storage transferService as I shared this is one of ourmost popular tools used most commonly bycustomerssoquite a comprehensive set of featuresthe service has around been a couple ofyears so security performance managedfeatures controls we have built on allof these different aspectsthe two of them which I'll highlightstarting with the first one is over thelast 12 to 24 months we have done asignificant amount of work to reallyimprove performance to improve thatperformance of transfer especially forsmall and large files so you know Iwould say that the performance which youwould get from Storage transfers isservice is significantly better thansome of the other competitive optionstherethe other one which is a recent releasedis even driven transfer so you couldhave data coming in and using Pub subnotifications or real-time event havethose data transferred across Cloud toGoogle cloud or within Google Cloud forprocessing in Google Cloud Storage soit's a very interesting feature withthis you could start taking use ofvertex AI in some of their real-timeanalytics Services even if your data isnot exactly in the same locationso if you haven't you know somethingworth checking out if it fits your usecaseand here's a quick recapture of some ofthe recent features and releases ofstorage transfer Service as well as ourother transfer products what we havedoneI'm not going to go through each of thembut maybe highlight two of themthe first one is a new support for anyS3 compatible object storage so theircustomers who may have an S3 compatibleobject storage in their on-premiseenvironments and now you can use that totransfer data to Google Cloud Storageor even if you have data in S3 or someof these other on you know Cloud objectstorage offerings this gives you a wayto shape the network by the networkconnectivity and data transfer to getbetter costs or performance in somescenarios so it's a very interestingfeature helps with that we're also aboutto launch hdfs support So for customerswho are running on-premise defenseenvironments and they're looking for away to move that to Google Cloud we aregoing to support that with storagetransfer service again the performanceof this in art as we have found issignificantly better than some of theothertools which are available today this isa service which runs outside of thecluster so if you're trying to vacatethe cluster you don't have to consumethose resource and you can getsignificantly better performanceso once again my section what I wantedto leave away is we have a pretty robustset of tools Services which can help youmake the data transfer easy at scale andagainhopefully you noted some of thesecustomer testimonials to prove they havebeen doing that pretty regularlyI'll now invite ajitesh to come and giveus some best practices details for bothon-prem to cloud and then multi-cloudtransfers ajiteshthank you manjul so I'll walk youthrough the best practices for on-premto cloud and multi-cloud data transferso starting with the first oneso when it comes to data transfer thereare three critical Parts the first partis figuring out the planning that whatdata sets you'll move and who will dothe data transferone thing to note here that legal andsecurity play a pivotal role theyapprove the software that you need todeploy the hardware that we'll be usingand what data sets move to Cloud so makesure you engage with all thestakeholders early in the cyclethe second step is figuring out how todo the actual transferand this has two parts the first is whattransfer tool will you usefor example if you are operating out ofdata center where there is lowconnectivity and the cost of setting uplink is really high you should use datatransfer Appliancein the case when you have goodconnectivity or want to land data asquickly as possible use storage transferservicethe second part of this is whatmigration approach should I useto a large extent this depends on howmuch downtime the application which isusing the storage can takeone of the popular approach ishighlighted in this in the bullet 2 herethis is a two-step transfer where you dothe seed transfer that is bulk copy thedata from source to destination andduring this time you keep reading andwriting from the sourcewhen you take a downtime at the sourceand do a sync transfer where you capturethe Delta all changes from the source todestinationthis is popular because it results inminimal downtime and also requires lessengineering effortonce you finalize the approach then youdo the actual transfer and then move tothird step which is validating transferthere are two approaches to validatetransfer first is to match aggregatestatistics that is total number of bytescopied total objects copiedor you can go object by object andcompare size and md5we offer you a couple of options to makeit easy for youso that's the high level approach nextlet me cover specific best practiceswhen using STS to move data from on-premto Cloudso on the right hand side here is thehigh level architectureso we ask you to download STS agentswhich are dockerized application theseagents run on a VM close to the sourceand when you create the data transferjob these agents work in parallel toread data from the source and write tothe cloud storage bucketjumping to the best practices in theplanning stage we recommend doing quickand early POCin doing the POC make sure you aretesting with one to five terabytes ofdatathat's critical because if you are usingless data it will not give you the rightpicture STS is designed to operate atterabytes and petabyte scaleone other thing that we recommend isdon't directly write a colder storageclass if you have to take any correctiveaction this will result in high API Opsand early delete chargesthe next one is pertaining toPerformanceif you want to scale performance justadd more STS agents if adding agentsdoes not lead to higher performance thatmeans there are bottlenecks in otherpart of the systemone of the things that we see commonlyis that in the older storage system youhave lower rate throughput limits somake sure you are using tools such asfio to measure the read limitsSTS provides lot of Rich controls somake sure you're using the right one forexample we often see that the datatransfer workload and the operationalworkload uses the same network pipe toread and write from the cloud storagein that case you don't want transfer toconsume the entire bandwidthin that case you can use STS bandwidthcontrol to monitor the and control theSTS bandwidth that STS consumesand finally make sure you are validatingthe transfer we already spoke aboutcouple of ways you can do thatso that's on the best practices foron-prem to Cloud sidenext let me cover specific bestpractices for setting up multi-clouddata pipelineso when we speak to customer about thisthere are three major consideration thefirst and the top one is the egress costeager is typically account for 90 to 99of the transfer costand there are multiple ways to egressdata from other cloudI'll be covering specific details in thelater slidesthe second major consideration isperformance and this has two dimensionthe first one is throughputif you want to scale throughput just addmore STS transfer job a single STS jobcopies 1000 objects per second andscales up to 10 000. if you trigger 10jobs on Parallel prefix you can rightaway get 10 000 object copies per secondthe second dimension here is latencyfor a lot of workload like PredictiveAnalytics or low RPO backup you want toland data as quickly as possible in thiscase you can use sts7 driven transferhere we'll listen to change notificationat the source and copy the data in nearreal timethe third factor is security andreliability to a large extent STS takescare of this out of the box we encryptthe data in transit do automatic retriescheck some matchingbut you can take certain steps to makeit more secure and reliablefor example when giving STS access toAWS resources make sure you are usingSTS Federated identityin this case STS assumes a role in AWSenvironment and generates short-livedtokens dynamicallythese are much more secure than usinglong-lived credentialswhen it comes to reliability one thingwe recommend is to use cloud logging andCloud monitoring with Cloud monitoringyou can get notified when theperformance metrics are down and thenyou can use cloud logging to dig deeperto understand why that happenedwe offer multiple options for you toegress data from AWS S3simplest and the easiest one is to useSTS and bigquery data transfer servicethough they do result in higher egresschargeanother option is to use bigquery Omnidata transferthis is suitable if you are usingbigquery Omni and want to land data toBQthe other two option options highlightedhere in the blue box result in sixtypercent lower egress chargeI'll be covering these options namelyegress via private Network and egressvia Cloud front in the next two slidesso how do you egress data from privateNetwork sorryso there are three easy stepsthe first step is that you create thiscross-cloud infrastructureyou can use either cross Cloudinterconnect which is a gcp offering todo thator you can use third party vendor likeMega port and equinix to create thisonce your network is set up you go tothe gcp site and deploy STS S3compatible agent on a GCE or gke nodewhen deploying on the gkgc Node makesure these node can speak to S3 VPCendpointwhen this step is done you go to STSconsole and create a transfer job thatcan speak to S3 VPC endpointnow your transfer will happen via thisprivate Network and result in 60 loweregress chargecouple of things to note on this slidethis requires deploying you this fixedinfrastructure which result in certaincost and that's why it's more suitableif you are moving 100 terabytes Plusdatasecond when we interact with multiplecustomers they also have these top-downmandates to not use public internet inthat case this offers a viable path andalso much more predictable performancethere is one more approach to egressdata that is via Cloud frontso this is how you set this up the stepone is you create a cloudfrontdistribution for your S3 bucket you cando that going to as AWS console clickingfew buttonsthen you come to STS page and create atransfer job supplying this new Cloudfront domainnow your all transfer will happen in twosteps first it will go from S3 tocloudfront the Loading which is free andsecond that will egress from the cloudfront which is substantially cheapercouple of things to highlight about thisapproach this feature is currently inthe control GA phase so if you want toget access to this reach out to theaccount team they can get you enrolledsecond this does not require paying forany fixed infrastructure so it'ssuitable even for a small amount of dataso megabyte terabytes will also doso that's on the best practices sidenext I'll invite on stage our Marqueecustomer Global and our Google Cloud'sPremiere partner thoughtworks to walkyou through the successful migrationstory of global I'm really excited tohear that thank you so much everyone[Applause]thank youso my name is Glenn meding I'm from RoboI manage the Big Data platform thingsthereand Global is the largest Media companyin the Latin AmericaGlobal Health broadcast channels pay TVchannels and digital products for newsentertainment and sportsGlobal has more than 100 million usersper monthand Global play that's our streamingservice has more than 30 million paidentry users in our platformin 2018 Globe starts this is its digitaltransformation behaving the way tobecome a media tech companyand thoughtworks was one of the keyPartners helping us in this journeyhi my name is Vera I'm a technicalprincipal at thoughtworks uh tortoise isa leading Global consultancy thatintegrates strategyConsultingsoftware engineering design wecollaborated with global owners projectthis project is a super part of a largerdigital transformation of global andtotals is also a premier partners ofGoogle Cloud2022 globally created it's all on thaton-premise workloads to gcpand these workloads are from deliveringall the content for our digital productsBig Data pipelines and back-offs toolsin this presentation I will talk abouthow immigrated the global play Big Datapipelines through the cityso before gcpthis is what's Global play Big Data highlevel architecture we have a dataingestion system that captures userBehaviorfrom all all Global digital productsthey collect things like page viewsvideo views clicks and everything thatyour users interact with the productsin the in that time we use nginx go onkubernetes and Spark with streamingHadoop to store out this dataand Global play process all this data towith pi spark and Hadoop to understandhow the user is behave in the in theproductso they can do user engagement andperformance of the contentbut this solution has one problem theit's out to scaling because we need topredict the highest peak of the year andprovides and all the hazards that weneed in the data center for itand this approach leaves a lot of idleresources during the yearso after migrating all the data wecollaborated with global to modernize adata pipeline from the older proprietarysolution to a solution 100 based on gcpuh before we get into the technicaldetails it's important to contextualizeeveryone about what business problemsare resolving and how this data will beused and what business what value willbring it to to the businessso Global play is a streaming app itgenerates a lot of data from the user'sinteraction using the app this data wego into two different domainsthe first domain is the marketing domainwhere we are answering questions such aswas the performance of our marketingcampaigns how can you engage users moreuh how can they can we reduce userschurn and so onthe second of the main is theintelligence domainwe answer questions such as how can Ihave a 360 view of our users across alldigital products of global what shows abig watch and so on uh if you one oneinteresting aspect of this use cases isthat most of the user cases are designedfor business people they're not uhuh people with hard coding skillsthey're not hardcore programmers sodesigning a solution that's easy tomaintain and easy to debug and seewhat's going on that was one of thegoals of this projectit expected that these people willevolve the the solution in the futurecreate new data products as the businessneedssoum as I said before there was a Lexisolution in place for the dataprocessingso one important question to ask is whywhy do we need to migrate data pipelinewhy can't I just use the same datapipeline that was already in placeI think that this lines trying to showthe rationale behind this decision Iwill not go into all the details uhyou can take a look closer uhthere's a a lot of information and wealso have a slide talking a little bitmore about the architectureuh at the time we had the feeling thatwas possible to do something that wascheaper faster and more easy to maintainuh but was still a quite vague idea atthe timeso covering two two important topics uhthe first one is cost transparency uh inthe new gcp solution we have nolicensing cost we just pay for the costof the workload itself we have much morecontrol of where this cost is occurringwhat's going on whether the lack ofsolution was just a single cause forjustprocessing and Licensing and we don'thave really visibility what's what'sgoing onuhfeature-wise what's uh we were able toimplement new features that was notpossible before or at the least toimprove these features we're talkinghere about uh automatic scalabilityscalabilityeasier reprocessing of your jobsprocessing only newly generated data andso onso after the entire data pipeline wasmigrated we measured all the results andthey were outstanding much better thanwe have ever imagined withoutconsidering uh improvements inperformance uhin governance or observability wemanaged toachieve annual cost reduction of 78percentthat's a big number that's this numberalone justifies why the work was doneand was a great return over investmentfor for this projectwe also reduce the processing time by 97percentdata sets volume reduction by 34 failurereduction by 92 percentso why so so good results is it magic isit what uh I think that we've improve ituh transparency more control we itbecame possible to be smarter about theutilization of resources and processingstrategies and overall optimizationsthat was not possible before I thinkthat's the one of the main reasonsuh of the success of this project insuch a massive uh data set that's asolution I would recommend for all yougoing forwardafter we created the solution to the CPwe change our data ingestion system touse all the gcp tools and like jka andcomputer engine for processing power andgcsm degree query for persisting layerso Global play because after work loadsare now using SQL pipelines in bigqueryusing data Farmwith composer Orchestra orchestrate allthe process togetherwe also use data catalog data studio andvertex AI to explore a generating sitesit's worth noting that we use the thistransfer service to move all thehistorical datathrough Hadoop filesthrough cloud storage and was very easyand help us a lot in the processnow our research and allocation is nowis not not driven by the peak momentsanymore we also scale everything withthe audience of the global plane so whenthe global play has a higher audience wehave a lot allocate a lot more resourceswhen there are audience decrease withdecrease the release of the resourcesin Disney solution was a greatImprovement for us in cost speed andit's a lot faster for us now adding newfeatures in this pipeline like a new uhsegmentation or a new process in the inthe pipeline so that's what's helping usa lot to evolve all the metrics that weneed for Global play"
}