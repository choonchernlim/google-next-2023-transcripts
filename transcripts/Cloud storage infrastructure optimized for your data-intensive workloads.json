{
    "title": "Cloud storage infrastructure optimized for your data-intensive workloads",
    "presentation_type": "Breakout",
    "categories": [
        "Architects, IT Professionals",
        "ARC208"
    ],
    "video_id": "w-K96mZVAAA",
    "time": "Aug 29 06:00 PM - 06:45 PM CDT",
    "transcript": "foreign[Music]welcome everybodyum I hope you all are having a goodCloud next and uh it looks like it's abrilliant day outside but I'm know we'reall excited to hear about Cloudinfrastructure for your data intensiveworkloads so thank you for coming andwe've got some interesting uhinformation we want to convey to youtodaywe have some great speakers I'm going tolead off and then we're going to haveSandeep Kohli from senior vice presidentof engineering from Flipkart tell youabout their Journey from on-prem toGoogle cloud and then we're going tohave Jason Pohl senior director of datamanagement for databricks and he's goingto tell you about how Google anddatabricks are working together to builda collaborative solution to help you runyour data intensive workloadsall right and then we'll have a q a anduh Marco and vivac who are both productmanagers for Google Cloud Storage aregoing to come up on stage and help withq a and I don't think I mentioned it I'ma product manager in Google CloudStorage as well soall righta common message that you're going tohear at a number of our sessions is howGoogle is working very hard to enableyou to run your AI ml workloads yourCloud native workloads and your innerclassic Enterprise storage workloadswe're helping to make them performantsecure and reliable and so this is justlook for this messaging throughoutyou'll hear it several times in othersessionslet's dive into Data building dataecosystems and let's start out make surewe're all level set so when I say dataintensive workloads what I'm talkingabout is just answering questions couldbe real simple questions like how manycustomers do I have it could besomething more involved like whichcustomers are placing which orders onWednesdays that sort of thing and thereason we want to do it is we want toyou know make our businesses grow andand meet our business objectives and wedo this by running workloads that we allcommonly refer to as AIML which is superpopular these daysanalytics more traditional reference andhigh performance Computing so when I saydata intensive workloads we're talkingabout all these things that we typicallyidentify withso what do we need to run theseworkloads together effectively if youjust start say go and you spin up yourVMS and you attach your storage to itwhat invariably happens is you end upwith a bunch of data silos there's a lotof problems with data silos they arethey have a problem with availabilitythey'reum there's a lot of copies of your dataends up all over the place things getout of sync you have high egress costsand it's time consuming keep themrunning so really the solution is uh toget all your data in one place and withGoogle Cloud Storage we give you asingle source of Truth to place yourdata it also gives you a global endpointand Geo redundancy automatically and italso places your data where you need itall right soum we find that people are like that'sgreat but we've invested heavily inthese in in a bunch of tools and we wantto make sure that we can still use thosetools we're 100 behind you and we doeverything we can to support you whetheryou're running uh first party Googleproducts such as bigquery or open uhSource Products like hdfs or spark orpartner products like databricks andwe'll be talking more you'll hear morefrom databricks in a few minutes here sowe're here to support you you choose thetool and we'll support you in GoogleCloud Storageall right now if you're using uh Googleservices we've built an entire ecosystemto help you run your data intensiveworkloads we put your data at the centerof this ecosystem so when you want toingest your data you can use things likestorage transfer service or the dataAppliance and then once the data is inin the cloud we provide services likedataproc and bigquery to do dataprocessing now as your data grows andyou get more and more of it it getsdifficult to manage you don't know whereit is you have security issues weprovide additional services likedataplex to help you manage thesedifficult data management issuesand of course we provide world-classsecurity and user experience and iamall right so specifically in cloudstorage uh how we help you run your dataintensive workloads isum we focus on making available in theway that you need it so let me talk alittle bit about our location types wehave three location types uh Regionaldual region and multi-region if you'rejust bringing your workload to the toour cloud and you're saying hey I wantedto perform well and I want it to be costeffective and I'm not sure what to doRegional is a great choice for youyou're going to get the best performanceout of Google Cloud Storage if you putit in Regional bucket and you will get avery good costbut we know a lot of you have additionalrequirements aside from just running theworkload you have data protectionrequirements you need the data to be intwo two regions not just one and youalso need to be able to run workloads inmultiple places you don't want to betied up if you're through the region itcould be because of Providence reasonsor perhaps you have you you have acertain compute shapes you're looking touseand dual region buckets you set up youcan pick two regions yourself or you canpick two regions that are automaticallyuh paired together already and uh wewill automatically replicate the dataacross those two regions it'll give youan active active failover capability soif a region goes down and you're you canstill read from that endpoint and thedata will be retrieved from the regionthat's still runningthis is a perform this is a performantuh buckets they're they're on the samelevel of performance as a regionalbucket andumit it uh it allows you to run your datalike I said seamlessly in in twolocations now a lot of you are sayinglike two regions are great why not morewellum oh I'm sorry well I also shouldmention we um we added a couple featuresto help you run uh dual region bucketsand that is uh replication monitoringand uh the popularity has been growingso we've also added the capabilities torun it in Canada and Australia all rightso now those of you that want to uh lovethis capability but you'd like to havemore regions what we offer is amulti-region bucket so the multi-regionbucket essentially covers an entirecontinent you can see all the dark bluethere covers most of Europe right wealso offer multi-region in the U.S andAsia and what this allows you to do isyou can run your workload anywhere inthe Blue Area essentially across Europeas shown and then it'll it'll pull thedata from that bucket and multi-regionbuckets will store your data all acrossthat region in two locations so you havecopies of the data and you'll be fullyprotected you'll have active activefailover experience the same as dualregion now because we you're allowing usthe ability to place this data where wefind it to be most cost effective we'resharing that cost reduction with you andso this is a more cost effective way toachieve data protection and through Georedundancyall right now this has been a reallypopular storage location type and peopleare trying to use it for all kinds ofthings and one of those things is dataintensive workloads and we thought wellwhy don't we make it even better and adda cache right so what we're announcingtoday is uh I think we're calledanywhere cash the anywhere cache allowsyou to put a cache in the same Zone asyour compute instance and you can see onthe screen I've got two caches and zonesA and B no cash in zone C you can put acache in all three zones or one zone orany combination of zones you want youdon't pay to enable the cash you onlypay for what you use and you scale towhat you need so the cash is a Zonelocal cash so it's fast it's simple youenable it on the bucket and then as youread data it automatically gets ingestedinto the cache and the subsequent readsall get served from cash it's a fullyconsistent cache so if the data changesunderneath the system will will know andand do the and serve a cache Miss andretrieve the change data from the bucketrepopulate the cache you'll always getconsistent dataum it's also great when you combineanywhere cash with the multi-regionbuckets as if you recall the themap that I showed you of Europe withthat a multi-region bucket with anywherecash would allow you to pick any Zonethroughout Europe or in the US if youhave a multi-region bucket in the U.Sand and run your workload there and haveit have a Zone local experience so whatI mean by that is like in the U.S youhave a multi-region bucket any Zone inthe U.S you can place the cash inthe cash will be as you read it will beingested into that cash in the same Zoneas your VMand then all subsequent reads will comefrom that zone local cache instead ofdoing across Metro readsum and so you'll have the localexperienceall rightum and we have other things to help youwith your data uh intensive workloadsone of the common patterns we see ispeople with object storage createfolders by uh putting prefixes in frontof the defined prefixes in front of theobjects these Define prefixes thenare what the folder is and what we do isin with this manage folders uh productthat we have just released is allow youto create IAM policies based on theseprefixes so essentially you have theability to set I aim policy IAM policieson each folderrightum a few other things I'd like to showyou new products we've got four otherproducts I'd like to mention we have abunch more Beyond this but I think fourproducts that would be worthwhilementioning here for this crowd for dataintensive workloads the first one is alot of people are running hdfs workloadson-prem we would like to help you getthose into the cloud so we have atransfer service to move those hdfworkloads to the cloud for you in asimple easy waythe next one is a cloud storage fuseproduct for machine learning this allowsyou to have a client a localfile client uh giving you file systemaccess to your GCS bucketI'm fully supported by Google Cloudthe next one is performance relatedthose a lot of people care a lot a greatdeal about performance when you'rerunning data intensive workloads grpc isa great choice to uh to run accelerateyour workloads and it'll help save yourwall clock time and minimize the amountof idle CPU time that you may have andsave you moneythe last thing I want to mention iscustom audit information custom audioinformation is a great way to help youkeep track of what's going on on and howyour data is being used it passes theapplication and identity information forservice accounts so that you can trackthemand so these are the products that Ijust want to make you all aware of andlet's give Sandeep a chance to talkabout and tell us about flipkart'sJourneythanks Bradso um just a quick brief about FlipkartFlipkart is India's leading e-commercestartup for last 16 yearsand it is one of the most successfulstartup stories in IndiaI'll uh talk about Flipkart scale togive an idea of the complexity and thesize of things but in generalwe have more than 450 million registeredusers on platform which means the entirepopulation of United States and Canadaand then we still have 100 million moreleftto account forand in terms of scale which I'll come toslide later but to give an idea it is weingest more than 100 billion events on adaily basisso a couple of years back we decided andwe also run our own private data centerso for most of our history from day oneto 14 years we were running on our owndata centers and we have two off of OurOwn Private data centersand the data platform that I'm going totalk about today we were running it onon-premthen why is it that we decided to moveto gcp and that is what I'm going toshare my insights with youso when westep back and looked at what people weredoing and I had some of the smartestpeople I believe on my teamwe were doing a lot of work on managingthe infra because at this level of scalethere was a huge effort to manage theinfra and we were also managing scalerequirements which is like every we havea yearly sale called the big billion Daysales which our sales spikes 10x to 20xduring that sales period and thatbecomes a technical challenge so a lotof team members were busy on managingeither the infra or the skill issues andI said like data platform teamthat doesn't seems to be totally rightand my belief is that if we want tochange something whatever the teammembers are doing or thinking today thatis what the state of the art will be sixmonths later in the companyso we decided to change and the thingsthat we wanted the developers and thedata platform thinking special to thinkwas not infra not scaling but thingsabout data platform data products andalso think aboutoptimizationso what kind of data products I wantedteam to think about see we do areal-time funnel analytics at a scale of100 billion of events ingested every dayduring Peak seal sale period givingreal-time information to our leadershipwe wanted to prepare data for our ML anddata science algorithms we want to spendmore effort in doingdata quality checks upfront so that it'snot the customer who's reporting dataissues and data quality issues but wecan run our own data unit test and dataIntegrity test Etcto remove almost a layer of analytics inbetween the end user the business userand the platform by using gen AI kind oftoolsthe other thing is optimization so it isgreat that uhgcp can provide you withunlimited storage and unlimited computeand I think it puts us into a little bitof confident situationbut I'll tell you it is not your and myresponsibility to test that out it isvery important that the data isconsidered and compute and data areoptimized otherwise soon they become anext another level of problem that isunmanageable and we don't want to dealwith thatin the gcp what we found because they'vedone the compute storage separation asBrad was telling you and they supportbothumthe open source as well as their ownfunctional Services stack I think wedecided that we will be able to useall of thatso here uh just a little detour thenumber that we process on a daily basisI think 130 billion messages I wassaying 100 millionduring this transition we had to alsotransfer 30 petabytes of data which ishot data we also have more than 45petabytes of cool data overall closer to100 petabytes nowhow big is 35 petabytes of dataI think if it transfer data in the bestcase over a 10 gbps link 30 petabytes ofdata takes a year to transfer if thereare no hiccups in the networkso having said that about the scale Ithink the data optimization problem forus was really important because we knewfrom experience that if we take our eyesaway from the optimizationwe will be soon dealing with problems ofscale that we don't have to deal withand fortunately the gcp stack gave usthe flexibilityto either use pop sub orfor ingestion to use data proc bigqueryfor compute site but we also could useour own stack for which we are familiarwith which we have optimized over theyears of our own framework which wasbuilt over the open source stack and Ithink that helped us inmaking sure that we are running the datapipelines and other processes in themost optimal fashion due to which wecould runmuch more number of processes much morericher algorithms over timeto give an idea of the stack that we areusing if you look carefully maybe thisis a complicated figure but basicallywhat it says isthat we we use Kafka to ingest from ourserver side data to take care of theback pressure and ingest it into Pub subfrom the millions hundreds of millionsof mobile devices the data is ingesteddirectly to Pub sub we do not have toworry about scale and other issues andthen we also run Hadoop we run certainof our workloads on hive which arealready suited or been optimized forthere we extensively run on bigqueryalso and are able to sometimes recoverfrom errors or when we have to re-do thecomputation due to human or certainother errors related to data ingestionwe are able to do it quickly but overallwhat this stack showsis that we are able to mix the best ofthing coming from native gcp platformofferings as well as open sourceofferings and that's been one of the keyreason why we are also able to run in avery optimized fashionanother interesting fact is that we didcomplete this migration in under inaround six months of timeand this is a live migration where thedata pipelines are running 24 bar 7 hourmonth and closure reports accountingreports which have to be closed andaccounted for are getting closed everymonth after month and in six months timeI think with a lot of support fromGoogle side both onTech stack and on the other aspect ofmigration including optimization thiswas pretty successfuland to close off this I think in postmigration I have seen from my personaleffect the speed of innovationhas really increased many folds uhpeople are now developing data productsthey're churning it out at much fasterrate the data quality related assertionsand able to cache data quality problemsmuch earlier asuh been actually at a very good Stateand could be famous last word I don'tknow what's happening nowum and it's been a successful storywhich has given us a breather to go andinnovate where it matters for thecompany and not worry about scaling theinfrastructure or managing theinfrastructurewith this I'll hand over to JasonI think you have just one more slideherecoolrightall rightgive it up for Sandyforeignjust wanted to say a thank you to Bradfor inviting me to share the stage withthem Google cloud is a strategic partnerof databricks and I feel honored to beable to present to you guys today sofirst of all who is databricks if youhaven't heard of us we are we are thelake house company so we are the oneswho created and pioneered the concept ofthe data lake house and what that is itmeans you should be able to write yourdata any type of data and one one timein the place that's most scalable andthe cheapest place which would be GCSand then leverage that one copy of yourdata for all your different use caseswhether that be batch ETL streaming datawarehousing or artificial intelligenceand machine learning applicationswe are the only um only only vendorrecognized by Gartner as a leader inboth the database Management Systemsquadrant as well as data science andmachine learning platform quadrant sowe're unique in that regardand one thing to note is uh we wereco-founded by the original creators ofApache spark so we've got open sourceyou know coursing through our veins andand since our founding we actuallycreated a couple other open sourceprojects so one of them is mlflow whichallows you to have an open sourceplatform for how to do the completemachine learning on lifecycle and aswell as Delta Lake which is what I wantto talk to you today Delta lake is thede facto standard for how to managetransactions on top of a data Lakeso what is Delta Lakeum it's a it's an open format what itdoes is essentially whenever you'rewriting data out to GCS what you'regoing to do is with Delta Lake you'regoing to be writing a whole bunch ofparquet files and alongside thoseparquet files is a transaction log andthat transaction log is what allows youto do inserts updates deletes mergestatements and do it concurrently acrossmultiple writers at the same time italso gives you performance benefitsbecause now that you have thistransaction log you can treat that as amanifest and so when you do go to doqueries you instead of doing a wholebunch of different list calls to GCS nowyou're just basically looking at theManifest and deciding what's thesmallest number of files and blocks thatI have to read to be able to satisfy aquerywe built Delta Lake from the ground upto be able to do both batch andstreaming so when I was uh listening toSandeep talk I just uh I got um I feltlike oh we've got a lot of differentsimilarities here because we deal withscale on a daily basis so we'reingesting two exabytes of data every dayin Delta Lake and some of our customersare ingesting up to five petabytes a dayon Delta Lake and because you have todeal with data volumes that large youcan't just rely on batch processingbecause you could get in a scenariowhere if you're if your batch processfails and you're you'reum your window is not big enough youmight not ever be able to catch up soyou have to be able to process data in astreaming way and so we designed DeltaLake from beginning to do both batch andstreamingum we have the transaction like I talkedabout offer some other benefits too sowe have something called change dataflowthat's built in and that means insteadof querying the table directly in thestatic form you can query the actualtransactions in the transaction log andget a stream of those transactionslikewise if you want to query a table asit existed at a point in time in thepast you can take advantage of timetravel as well all thanks to thetransaction logand so there's been a lot of differentbenchmarking on different lake houseformats and all of them come out withDelta Lake as being the fastest and ifyou want to look at a really good onethere's one that Berkeley did I thinklast year called the lake houseBenchmark lhb and it kind of like goesto a number of different scenarios forboth querying as well as inserts andmerge statements and gives you aholistic picture of of how all thesedifferent formats operate and what howthey perform and what the readamplification is and right amplificationisso I'm really excited today to talkabout data bricks and how we integrateso well with Google we've um we had areally good partnership and when wefirst deployed databricks we actuallyleveraged gke to deploy the Clusters sothe way the databricks works is yourdata stays within your cloud accountyour Google account within your bucketsand then whenever you spin up clustersto process that data those clusters staywithin your Google account as well andso databricks is basically has a controlplane that's orchestrating all this onyour behalf but you're in completecontroland we've been you know I mentioned thatdatabricks you know is a big believer inopen source technology and I Know Italigns really well with Google's valuesas wellum and the nice thing is once you writeall of your data once into Delta Lake onGCS then you can bring whatever engineyou want to it so you can bring abigquery if you want to query that dataor a data proc if you want to do someETL processing databricks we have ourown engine we built a native vectorizedengine called Photon and it implementsthe spark API so anything you can run onApache spark you can also run on ourPhoton engine and and generally it runsabout seven to eight times faster so youcan take advantage of those performancebenefits on databricks as welland the nice thing is that we'reavailable in the Google CloudMarketplace so you've already got databricks all you got to do is just go tothe Marketplace search for it find ityou get a 14 day free trial try it outsee what you like and because it's inthe marketplace it actually burns downyour Enterprise agreement as well soum there's no reason not to try itand one last thing before we get to uh qa here is we announced something calleduniform earlier this year at our ownSummit and what uniform is is it's partof the open source deltalay project andthere's a couple different lake houseformats out there so there's Delta Lakewhich is part of the Linux foundationand then there's Apache Iceberg andApache hoodie uh now some customers wefound are using two if not all three ofthese and what they really want is theywant compatibility and portability sowhat we've done is we've createdsomething called uniform where when youwrite the data out to Delta Lake it'llautomatically replicate the metadatathat transaction log into the otherformats so for hoodie and Iceberg thisallows you to get all the performancebenefits of writing out to Delta Lakeand then if you need the portability ofbeing able to read that same data indifferent engines then you can read itas an iceberg table or the hoodie tablethis is you know really useful becausesome engines they end up implementingone of these formats before the otherones and so and then the customers arecaught just waiting forever all theseengines to catch up now you don't haveto wait anymore you can just use uniformand write your data once and leverage itacross any engine so with that I thinkI'm gonna invite up aum Vivek and the others[Music]"
}