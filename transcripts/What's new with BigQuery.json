{
    "title": "What's new with BigQuery",
    "presentation_type": "Breakout",
    "categories": [
        "Data Analysts, Data Scientists, Data Engineers",
        "ANA100"
    ],
    "video_id": "NxMuLljlCko",
    "time": "Aug 29 02:15 PM - 03:00 PM CDT",
    "transcript": "[Music]thank youhi there good afternoon everybody uhwelcome to the what's new with bigqueryuh sessionum my name's Dave Nettleton I'm going tobe one of your presenters todayget the right direction of the clickerum with me uh two co-presenters uhDaniel who's uh from Discord one of ourfabulous customers and has fans alreadyGod all right I didn't even get I canget a wooum and then Thomas talius who leads theengineering team uh for bigquery andthen myself Dave Nettleton I lead theproduct management team uh for bigqueryand I'm very excited today to walk youthrough uh what's new with bigqueryum I'm probably gonna I'm gonna covertwo main sections of uh materials someis announcements at next but I also wantto take a little bit of the time to kindof go through and just highlight some ofthe releases that from earlier in theyear as wellum so you know just starting with alittle bit of background and the contextfor how we see and think about bigqueryhow our customers are using it and thethe success we see in the market with itum you know for many many years and I'vebeen very fortunate through maybe 20years or more of my career I've had achance to work with a lot of customersaround how they can take best advantageof data and AIum uh you know this might be from theold days of sort of data warehousingdatabases traditional ETL type systemsthrough the area of um uh data lakes andHadoop and hadooper system but behindall of that is this urge and this needfrom customers to drive more value fromtheir data one of our customers once putthis to me really well and really supersuccinctly which is I only need to dotwo things I need to make money and Ineed to save money and in doing both ofthose things depending on where you areoften in the business cycle there'sdifferent pressures from the business italways comes back to being able to usedata and AI to help generateum that value for your business and wesee consistently customers who are ableto do that are able to generate muchbetter business performance than thosewho still struggle with some of thechallenges of using data and AI so youknow we have the privilege to hopefullyburn then build a product with bigqueryto really simplify this and make iteasier for for customersum I guess I couldn't get very far inwithout talking about uh generative AIyou know it's been a very uh Hot Topicuh for the last yearum you know and this is really sort ofaccelerated a lot of these conversationsaround data right now people have beentrying to get value out of data they'vebeen using AI for many years generativeAI is adding a sort of a net new set ofcapabilities it's one of those rareoccurrences when there's been a stepfunction in the algorithm capabilitieswith the sort of the Transformercapability and the ability to handlemore ambiguityum in for example language the scale ofprocessing power and algorithms throughthe super powerful tpus and gpus thatare available and then also just thescale of data for training right sothose three things have combined intothis really interesting step functionthat has made a new set of possibilitiesum uh super interesting for customers touh to explore so whether that's dataanalysts looking to be more productivein finding insights and data whetherit's customer support calls andconversations and interactive chat Botsmany many uh organizations are exploringhow to take advantage of generated bywithin their businesses todayum so for us you know we just wanted toreflect for a second on why are peoplechoosing Google Cloud today for theirdata and AI Journeyum and uh and how has you know how havewe thought of how generative AI fitswithin that well first and foremost wethink about this sort of unified dataFoundation right we want to make itsuper easy to capture and manage all ofyour data make it very easy to bringdifferent algorithms to bear on thatdata and really operate it um at scalewe want to make it simple so that youdon't have to make lots of copies ofdata back in the old days of databasesand data warehouses you pull data out ofa database you'd ETL it into aparticular format for us a a schema andthen you would then you know ask thequestions you already knew and if youhad new questions to ask that'd be allthese complicated processes that you'dneed to go to so we really want to kindof simplify those processes make it easyfor you to run on the data wherever itis at and then and then finally you knowgenerative AI is going to be I thinktransformational for many businesses andbusiness processes but for us so farinterestingly the way that we'reintegrating it has been sort of veryiterative right a lot of the Investmentsour customers have made in their dataand AI Cloud platform their their dataLake their AI lake house you know ifbuilding on all of that with bigquerysome of the capabilities that we'relaunching that I'll talk through youknow they just build on thosefoundations you know customers have beenon this journey for many years and we'regoing to integrate into that a set ofcapabilities to help to help themand then of course we build this allum on a super scalableinfrastructure take security veryseriously we want to make sure that allall data is secure private Etcso what does bigquery fit in you know wereally see that as at the sort of theheart of our data and AI Cloud you knowuh thank you very much for uh all of theuh the customers and the users today Ilove hearing the feedback from you knowtens of thousands of customers who areusing the product uh every dayum running billions of queries acrossexabytes of data their scale and thenumbers behind the product are reallytruly phenomenal so thank you very muchfor all of theum all of the work that you've beendoing uh with uh bigqueryum you know we're building on you knowmore than a decade now I was justlooking at the release notes the firstrelease note for bigquerys 2011. uh weannounced joins you know so we've movedon a little bit since then and a fewother capabilitiesum uh but if I go back over the timelike you know we've been on this littlejourney for a while of moving beyondjust you know SQL queries running overstructured data right like we've movedto different types of programminglanguages we've added geospatialcapabilities uh stream analytics we canstream data in and have queries uhavailable to you know instantly querythat data we've reached out to otherclouds through bigquery Omnium we've extended to data that's outsideof the thought of the traditional datawarehouse management with uh Big Lakeum for customers that want to share dataanalytics Hub we've been building mlinto the uh uh into bigquery uh foryears so so lots and lots of innovationto build on and one thing I do want tohighlight because that you know this issomething I I it was one thing'sactually attracted me to come work atGoogle was just some of the underlyinginfrastructure is is very very uniqueum you know and there's three key partsright we have a this very very fast uhscalable file system called Colossusthat means we can write data into itreally really quickly that sits behindthings like the ability for streamingfor example we can stream data in datais instantly available for query becausewe can write it quickly read itimmediately we have Borg which is ourmassively scalable compute architecturethat let's scale our query processingengine Dremel to really large scale andwe can scale up and scale downdynamically on that and then of coursepowering that sort of serverlessarchitecture that compute storageseparation we have the Jupiter Networkwhich gives us sort of petabitconnections between our data when youtake these all together it means thingslike you know you can stream data in atjust really large scale and have it beinstantly available for query you don'thave to batch load and as you bring datain data infrastructure you know we weback huge like tons of disks are actevery day right like we discs fail allthe time you never see any of that rightlike just our underlying infrastructureand reliability means that you know youcan run your workloads very reliablyum efficiencyum you know I talked a little bit aboutthat compute storage separation it meanswe can do some really interesting thingsyou know you don't have to think aboutscaling in units of VMSum right that are sort of isolated unitsof compute um you we have the ability toallocate resources through reservationsyou can scale and share them but we alsocan do some really interesting thingslike if the capacity isn't being used inone of your reservations in a fewletters you we can go share it withother reservations because we have thisyou know this large pool of compute thatwe're optimizing over right so some veryvery powerful uh capabilities um underthe coversokay so let me talk and go through nowum some of the announcements that wehave so this was mentioned in thekeynote so very excited to introduce abigquery studioum so one of the pieces of feedback wehad from customers is that while youknow your data to AI Journeyum is uh is is very powerfulum but you know what we'd actually liketo doum a few more things with that and alsocan you make it a little bit easier forus please so a few big things that we'reannouncing here is number one we'regoing to put a notebook experiencedirectly inside of uh bigquery so thatJourney from data to AI for folks whomight want to use Python for example issuper simpleum and Beyond just sort of the userexperience integration uh we also wantedto make it easy to operate and at scaleover data you know often if you're adata scientists are working with datathat might take a copy of their data atthe warehouse load it into theirnotebook work on a subset of the datathere build their algorithm and then youknow figure out about productionproductionized running it at scale andthen running it in production so we'realso introducing a data frames APIdirectly over bigquery so now for anyonewho's working with you know the commonapis thatlike so for example pandas will now takethat data frameAPI and we'll translate it to uh on theback end to a query in bigquery so nowyou can just run petabyte scale dataframes jobs directly over the datawithout even needing to take it out ofuh bigquery so very powerfulcapabilities to let you now start aexplore and operate on data and then ofcourse the other key part is once youget Beyond experimentation you want togo into production and so we wanted tobring much more of the sort ofcentralized Source control and revisionhistory capabilities that you mightexpect in a traditional sort of devopsenvironment into the experience we hadacquired a company a couple of years agocalled dataform that gave us the sort ofthe key capabilities for this and so nowyou know we can integrate with all ofthe kind of the key CI CD workflows thatyou might have also integrated in theexperience powered by dataplex is dataprofiling data quality data lineagewhich will you know as you start tobuild these pipelines understanding theflows of data what's change what's thequality what's the lineage becomes moreand more important we're going tointegrate that all into the experienceas well so this is a a big announcementfor us this is now going into uh previewI'm excited for for you all to start touse thatum I mentioned you know sitting behindum uh the bigquery studio is data formhelping you now sort of model and buildpipelines giving that um uh integratedCI CD workflow and orchestrationexperience over your data the dataprofiling data quality data lineagecapabilities those are actually allgoing intoGA now so I'm excited to see those arebeing usedumbeyond the sort of the data 2x AIbuilding experience another big areawe've been investing in is being thesort of the manage lake house uh with uhbig lake so you know the traditional waythat a data warehouse is built is you'dhave a a query engine a metadata layerthat would deal with table definitionssecurityEtc and then a storage layer and thatsort of delivered as a vertical kind ofdata warehousing stackso key thing that we've been working onis to split the stack apart a little bitand take that that date that metadatalayer and break it apart umsomething that we call a big lake so nowthis lets you define metadata Definesecurity Define tables in a single wayand that and Big Lake will be able toreach out to data that's in formats likeuh parquet originally and then now we'realso introducing the ability to queryover Iceberg Delta and Hoodie so you cantake data that's in those common formatsyou can register it within the big lakemetadable metadata and create tables aspart of uh big lake that are nowavailable to all of the processingengine of uh bigquery so very excitingfor customers who might have chosen aparticular open format and they want towork with and take advantage of thescale of bigquery uh they can do thatnow and again this is powered by thatcompute storage separation that I talkedabout earlier you know we can store andmanage data separate from compute andtake advantage of the network too to dothat Analytics so in addition to thatyou know the other key element of thisis that you can bring different types ofprocessing to Bear over that data rightso if you want to use dataproc or sparkyou can then process and come over theover big lake data that way so excitedto announce that we we've extended theformats that are available we're alsointroducing fully managed tables uh aniceberg so not only can you read datafrom Iceberg but we will also write soif you want to write data to thesetables we will take care of transactingand writing that data for youum announced earlier in the yearum were something called object tablesum which lets you take data that's infor example Google Cloud Storage so youmight have one example uses call centerrecords call center transcripts youmight have a set of call centertranscripts that are sitting inum in GCS and you might want to doannotations on them and I'll talk alittle bit later about how you mightapply llama llm algorithms to thoseum but you can create a table add acolumn that is and make it an objecttable by adding a column that's backedby a bucket of data that's in GCS andyou can operate and secure and managethat in a single way with uh with biglake so we really want to be able toreach to all these different types ofdata so you can have a single consistentway to manage that and then we'reintegrating this with other parts of thestack so for example you can takealgorithms that are invert xai applythem to data that's in object tables viaBig Lake fine tune modelsby pointing your document AI to the datathat's in bigquery and then taking thatalgorithm and then productionizing it solots and lots of Integrations betweenthe products to support this uh bigqueryOmni so big for bigquery Omni this isthe ability to reach out to data that isin AWS and Azure again you know theprinciple is we want to be able tomanage your data in a consistent way tobe able to reach out to data that's inother other clouds we've just recentlyannounced expansion to uh Island andKoreaum and uh two new capabilities that aregoing into previewum uh announce that next are the abilitynow to join across clouds and also tocreate cross-cloud materialized viewsum so you can now have a single singlequery spanning your Omni and gcp regionsmaking it very easy now and easier tosort of share and operate across thoseuh just under the covers you might haveseen not too long ago that we announceduh some Salesforce data Cloudintegration and that actually is poweredunder the covers by Omni so you don'tsee that as we expose it but if you'rein Salesforce or if you're in bigquerywe make it very easy for you tointeroperate between your different datasets right we want to make it easy tosort of secure and manage all of yourdata but we also recognize that some ofyour data is going to be in other Cloudproviders and in other applications likeother SAS applications like Salesforceso we want to make it as easy aspossible to connect to that data tomanage that data and operate on it inyour environmentum so analytics hubso this is something that we announced alittle while agoum and it uh you know we if you'reworking in Industry where you havesuppliers or a supply chainum or you want to share data sometimeswithin your company but often acrosscompanies like how do you do that youknow we we want to make it easy for youto share and collaborate on your datawith other companies in a safe andsecure way and we have you knowthousands of organizations currentlysharing petabytes of data across thoseorganizational boundaries right you knowagain you might have you have a lot ofdata and we'll help you manage the datawithin your organizational boundary butwhen you want to collaborate with otherorganizations that's something that is avery interesting opportunity as well soanalytics Hub lets you now sort of letsyou curate and manage across thoseboundaries and then we're also excitedto preview data clean rooms so this is aparticular set of capabilities so youmight be sharing data sets acrossorganizations and that's great so longas you're okay with sharing for examplethe contents of that data but if youhave data that is private or or youdon't want to share the details of ityou know you're comfortable with lettingcustomers run sort of aggregate analysison it right likeum like like only showing groups of youknow more than 50 and summations of thatbut you want to protect individual uhprivacyum data clean rooms give you anenvironment where you can agree throughpolicy dial like I will share thesetables and I will give you this join keyand in fact that join key might comefrom a third party and then you share itwith another company and you agree thatyou can collaborate on that data andthen the results across that so you knowoften we see this in you knowthird-party cookies you know that aregoing away and so sharing of data andidentity between organizations isbecoming more important but customerswant to be able to do that in a secureway but it doesn't affect the privacy oftheir individual customers so now theycan create an environment to shareacross two organizations they can takedata that is you know individual dataprovide a third-party join key and runqueries over that in a completely safesecure private wayum and and then we provide algorithmslike K anonymity that says look and youknow we'll only show results if we canbe sure that they're Anonymous to afactor of say 30 odd people so there'sno way to be able to individually figureoutum where a piece of data came from soexcited that that is now in preview andthen one other thing that we're going toannounce so this is a roadmap so this iscoming later in the year so continuousreal-time analytics so these are thinkof them as sort of continuous querystreaming queries where you can write aquery that is running all of the timedata is written into the system thequery is always responding to that maybethings like tumbling Windows Etc wherewe're able to keep a query running evenas new day later is coming in andcontinuously outputting the resultsthose results can also be output toother systems like bigtable one commonpattern we hear from customers is I'mdoing a lot of my analytics in bigqueryand then ultimately my serving happensthrough a bigtable or for example orvertex AI sometimes as well and what wewant to be able to do is power thatunder the covers so that you know youdon't need to build those pipelines wecan with continuous query just with asingle query now export data directlyinto something like bigtable or verdexso thatum yeah you know your your serving layercan be always fresh and up to date soexciting to see what customers are ableto do with thatum okay second section intelligent soum we announcedum uh as part of our sort of platformtoday expanding do it AI more broadlywithin Google Cloud platform excited tobring a number of those announcementsinto uh bigquery we'll have a codeassistnatural language assessment assistSQL completion and all of this will beall all sort of within a highly secureGoogle Cloud environment will operateover your data in a secure way provide areally great assistive capabilities overyour data and algorithmsuh for bigquery ML I mentioned earlierwe've been using had bigquery ml builtinto bigquery uh for a number of yearsand uh we uh are extending that with anumber of capabilities uh to uh call outto other algorithms so we had somecustom algorithms built into bigquery wewant to make it easier to reach out toother algorithms through our inferenceengineum so you can go now to pre-trainedmodels you might have in custom computeor uh or vertex and we're integratingalso with llm API so that you can nowgenerate call lime apis and do thingslike transcription of text sentimentanalysis on textEtcum another roadmap thing that we areannouncing is uh vector embeddings andindexesum so often when you're dealing withumsome of the training data or custom datato your organization you want tofine-tune models or uh um and you youtake an embedding which is basically asort of a mathematical featurization ofyour of your data and you then mightwant to you know run that over say youknow 500 images and then ask you knowhey what images are similar well sowe'll make it easy to create thoseembeddings and then create an index thatwill then have capabilities to do thingslike clustering Etc on those and thenyou can use these bigquery Vectorindexes to sort of really power yoursystem of record behind some of the mlworkflows that you will build and thenwe'll integrate with so for examplevertex feature star and matching engineso that you can serve these uh reallyreally uh quickly as part of yourworkflowsum I want to run quickly uh just in thelast couple of minutes before Danielcomes up uh run through just some othercore platform announcements that we'vehad over earlier in the year uh just foruh completenessum earlier in the year we introduced anumber of new ways to sort of manage andthink about overall price performanceand total TCO we introduced threeadditions in bigquery we introduced anauto scaler and we introduced a new uhformat of storage for compressed storagegiving more choices on um on how tomanage your overall TCO depending on thedifferent workloads that you have aspart of that auto scaling was releasedso this is an auto scaler that lets younow run a query in a capacity-basedmodel we'll scale the query up to meetthe needs of your workload and then asthat workload dies down we'll scale backdown capacity and then we'll just billyou for theum that capacity that we've scaled to soit's fine grain scaling and then comingdown as well we want to make it assimple as possible so you don't have tothink about managing capacity as you'rerunning workloadsum lots of improvements on performancewe've been working on over the uh overthe year I put some examples here manyof these you'll see our goal is tosimplify it so we do things under thecovers like adapted file sizing to dealwith different size of data setsumDynamic concurrency control so lots ofthings that we're just continuing towork on under the covers to includeimprove our price performance anothergood core capability table snapshots andclones so you can have a table a dataset and you might want to be a you mightwant to take a copy of that so that youcan then think about backups or orreading off of it or you might want toclone a Setter and those are read-onlyreplicas if you want to then snapshotsif you want to then clone a table sothat you might want to do some a btesting or production testing you cancreate a writeable clone as well sothese are two great capabilities thatwere announced earlier in the yearmigration Services making it easier tomove various workloads from a range ofdifferenttargets to bigquery we're enhancing ourmigration capabilities with llms so thatwe can simplify those last minute uhthose last mile translations you sawsome demonstrations of that fordatabases in the keynote this morninghopefully and just making it easier foryou to interact with this through an APIand then finallyum I just want to talk a little bitabout Partners so you know we reallybelieve in having a really cohesive RichPartner ecosystem so very excited tocontinue to grow and work with thosePartnersum one thing we've been excited to dowhich is now uh generally available uhjust a little earlier in the year wasactually build within the bigqueryexperience a partner Center experienceso making it easier to discover partnerIntegrations uh having customizedgetting started experiences and we'llbuild on that in the future with betterin in contextum experiences as well so I think withthat that's my section complete Danielis going to take over for five tenminutes and then we will take a q a[Applause]thank you Dave I'm continuouslyimpressed by the iterations feed onbigquery so many cool things coming outthis yearyeah I'm Daniel I come from Discord Irepresent the data platform teamvery happy to be here[Applause]sooops I go the wrong waycool so a little bit about Discord wewere founded in 2015 we are a placewhere the world goes to hang out it'stext voice and video communicationand we have 150 monthly active userssome of the interesting data challengeswe face as a company is that it's areal-time platform texting and voiceit's very quick so we need to be able toreact to Data Insights very quickly inorder to be successful in our mission anexample of this is that we banned 300million spam accounts last yearso the goals of our platform let me lookat okay here we goyeah when we set out on our journey withbit with bigqueryuh we set out with three goals the firstone is self-service data platformbecause Discord we believe in small andmighty teams providing High leverage toour customers and stakeholdersand in the data platform that meant datagovernance data quality observabilityand all of the tools people need tooperate their own data setsindependently from a central teamscalability this is a way to unlockgrowth without us going back to thedrawing board every couple timesand multi-purpose as we've seen the usesfor bigquery has grown every year withthe expansion of gen AIbut even from the beginning we knew thatwe needed analytics businessintelligence ml experimentation causalinference and a multitude of differentrequirements in order for us to meetthese needs an example is we built ourown reverse ETL system at Discord andnow bigquery came along and released itthemselves this yearsothe circumstances and impact what wasbasically the beginning of our journeywith bigquery it started in 2019actually right before I got to Discordwhen the team had reached the maximumsize of the redshift cluster so we knewthat we were going to have to pick a newredshift cluster but costs had alsobecome a problem so they went to Firstprinciples andthey arrived at bigquery and honestlyI'm very thankful they did because Itook over the data platform team in 2020two months before covet hit so we hit500 percent increase in event volume ina two-month window right after we wenton bigquery and it really uh it was veryuseful very very helpfulum so yeah also uh data quality and datagovernancedata governance is a key piece becauseevery year the compliance landscapebecomes more complicatedand historically workflows were blockedby a centralized teamand we had course gain coarse graineddata governance controls which meantthat frequently we would just lockpeople out of data or delete itso this is where we started and I'mgoing to tell you a little bit about thejourneyuh here's our architecture so at thebottom of this diagram I think is thecritical piece which is our self-servicedata platformthis is where we've realized that whatstakeholders need in order toto accomplish their goals is they needtooperate independently within the companyso for us that means providingobservability data quality checksmetadata management as well as automateddata data governance policies so there'sstill centralized places within thecompany that Define the process for datagovernance but when they actually go todo their day-to-day work policyautomation comes out of the boxso the benefits and value that we'velearned through our journeythe first one was actually justcontinuous integration and deploymentsome things continuously deploy otherthings we have human steps along the wayespecially with data governance ensuringthat there is verification before thingsgo to production but the ability forpeople to deploy rapidly has reallyincreased the speed and the quality ofdata throughout the systemand scalable we've put bigquery throughthe ringer a couple times in the pastcouple years one time we scaled from ahundred thousand slots to 170 000 slotsin uh one dayshout out to Zay methani she was on theother side of that call when we formedwhether we needed 70 000 more cores likeat that moment and then most importantlythe ability for us to scale back down soonce this great need was over we couldscale back down and not be paying payingfor more than we neededand then lastly data governance andquality honestly I see it more of a asan ecosystem around bigquery bigquery islike the the foundational piece butumyeah DLP dataplex and now a lot of it'sgoing directly into bigquery withbigquery policy tags has allowed us toautomate policies and Implementfine-grained data governance policiesmaking it easier for data to beaccessible across the companywhoopsso yeah lessons learned it's been quitea journey but what we definitely learnedis that bigquery does scale efficientlyand workload management is a key pieceof us achieving uh cost and performanceone of the important ones I like toalways call out is uh we build executionprojects to dedicate bigquery slotreservations and you can attach biengine to to Dedicated ones so you canget low latency query access andquick responsive business intelligencebut based on your use case you canallocate the resources and I think thatwas a critical pieceuh Lessons Learned is that self-servicedoes unlock teams teams are just waitingwhen we provide them the observabilitytools the policy automation it it wasquickly adopteddata governance is much easier wheneveryone contributes this is somethingthat no matter how many tools we'vebuilt and how many systems we put inplace some of the highest leverage valuewe've had is through education acrossthe company andeverybody participating so that we canhave a robust privacy compliance anddata governance systemand lastly data ownership increasescollaboration and data quality so as weprovided the tools for people to claimownership of their data setsit's allowed our stakeholder teams tohave 10x leverage across the companysharing their high quality data productsto other teamsthat's just a little bit about ourjourney on bigquery[Music]foreign"
}