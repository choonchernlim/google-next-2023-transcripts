{
    "title": "Build an open format analytics and AI lakehouse",
    "presentation_type": "Breakout",
    "categories": [
        "Data Analysts, Data Scientists, Data Engineers",
        "ANA205"
    ],
    "video_id": "Tdrsc0jZ_Nw",
    "time": "Aug 30 10:00 AM - 10:45 AM CDT",
    "transcript": "[Music]well hello and welcome everyone let'sget things started for day two of nextlet's start by a quick stop pole uhraise your hand if you ever had to buildan ETL pipeline which was largelycopying data so that you can enable abusiness usecase quite a fewkeep your hands up if you felt thatafter creating that pipeline youincreased the complexity of managingthat data or security andgovernance well quite a few so that'sreally the Crux of the session today wewill really be talking about how how youcan actually build a architecture that'spredicated on a single copy lake houseone that allows you to make sense of allof your data with all analytics and AIengines and do so in a secure governedand performant mannermy name is gorov Sak I'm a group productmanager here at Google Cloud focused onlak house Ai and analytics and I havethe pleasure of joining me today isRohit danan who is uh Chief technologyofficer for acutance Tech platform whowill be sharing insights and learningsfrom their lake houseJourney so first of all I do want tostart off by saying that this is a superexciting time we are at the dawn ofgenerative Ai and there are lots ofreasons that you might have heard whypeople are excited about this but I wantto point out two specific things thatare very relevant to the lake housetopic well first of all if you look at atypical Enterprise today we are onlyscratching the surface in terms of thepercentage of data that's reallyrecurring into value and a large partreason of that today is if you look atbusinesses today they generate a lot ofdata every time a support interactionhappens every time uh a customer leavesa feedback on an online platform orcreates a video that's a lot ofunstructured or multimodal data thattingcreated the problem was that the toolsto unlock that was very expensive wellnow generative AI is going to has thepromise to solve that particular problemand really make sense of all your datasecond the assistive technologies willhelp every data practitioner each one ofus to do more and this is why it's superexcited that it needs to be kind ofgrounded in the context of yourEnterprise data well and while a lot offocus has been on AI models the heart ofAI is data and this is why many you havebeen already thinking about these usecase or trying out these use cases todayhave beenasking how do I connect data to usecases easily and to do so in a securemanner governed Manner and to be able todo it without writing a lot of custominfrastructure let's talk about how agcp lake house helps you achieve thesethings so the biggest problem when itcomes to connecting data to use cases isreally the siloed nature of the dataLake the data warehouse and the AI stackeach layer in the stack has its own wayto access data and its own way to managemetadata well what this really means isif you are going to start up an AIproject or a AI use case and if thatdata is present in a warehouse you willnow right to have ETL job to pull thedata out and as pointed out through astop Pole that really becomeschallenging well that complexity onlyincreases if that data is in differentformats or in different clouds besidesthe security and governance challengesall of these the physics of moving thedata really comes in the way and whilethis is extremely painful for us dataEngineers ultimately this slows downvalue generation and and we are todayhere to look at how we can really helpthis kind of use casesnow last year to address some of theseproblems at at this very conference weannounced Big Lake big lake is a storageengine that unifies your data Lakewarehouse and AI workloads it does so byproviding a single fabric of data overwhich multiple engines can interact withand do so in a secure and govern mannerit abstracts and decouples some of thecommon shared infrastructure from queryengines such as metadata and securityand abstracts into a runtime on its ownthis is what allows query engines tointerface with all the data underneathspanning across formats and Cloudboundaries and to be able to access thatdata with built-in fine grain accesscontrol and also with priceperformance ever since the beginning ofthis year we have seen a lot of growthwith many of you building these lakehouses in fact processing over big lakehas really increased by 27x beginning ofthis year and the first question thatmany of you are asking is what tableformats does pig L support table formatshave really taken off in the last threeyears because of the promise ofproviding Rich data managementcapabilities over open format dataApache Iceberg Delta or hoodie arepopular names we have specifically seena lot of interest in Apachi Iceberg andthis is why I'm excited to share thatthe support for apachi iceberg is nowgenerally available in Big Lake you cancreate or modify an iceberg table fromopen source engine such as spark andthat table can automatically becomeavailable for your big query users to goquery and consume this is made possiblethrough a shared meta store architecturebig lck meta store is a shared metastorebetween open source engines and bigquery making it easy for you to uhaccess your open source tables directlyin bigquery and quer it when it comes toquerying big query natively understandsIceberg metadata and and this is whatmakes it really easy for big query tonot only deliver a performant queryexperience but to deliver it with fullfine R Access Control model it Big Lenforces find access control over yourIceberg data and enforces it when you'reaquaring from Big query as well as fromopen source engines when you're goingthrough the big lake interface thisreally makes sure that not only runningopen source Iceberg but you are actuallyrunning in a secure Manner and you arerunning in a performant manner and youare integrating all of the data whichwas already present in bigquery or inotherplaces while Iceberg has dramaticallyaccelerated the adoption of open lakehouses on Google Cloud some of you havealready been using Delta or hoodie foryour workloads for specific use casesand this is why I'm also really excitedto announce today that Delta and Hoodieis Now supported through big querymanifest you can now create a big Ltable by pointing to manifest generatedfrom apachi hoodi or from from Delta andthat table can now be queried from Bigquery the Manifest support now supportspartitioning as well which means we scanless data every time you're runningqueries against these openformats lastly the same security modelthat we spoke about of iceberg incontext of find and access control allof those features also work on Delta aswell as on hoodie which means you caneither use big query or open sourceengines to query these tables with rowcolumn access control or through Dynamicdatamasking besides the support for tableformat many of you also ask about queryperformance it's really one of the chiefcharacteristics when choosing aparticular technology stack to run onand performance has been an importantarea of focus for us this is why what wehave been doing is really expanding theunderlying infrastructure that we'vealready used for big query it's calledBig metadata that really represents Richmetadata data profile I all extended toopen formats so that we can capture RichPark status and metadata and use thateffectively for query planning all ofthis a meant that on The tpcs Benchmarkwe are now 4X faster than we were beforeon big query external tables and thisincreased performance by the virtue ofleveraging metadata also means that wehave to scan less data reducing the costof running these queries and posing a75% reduction when compared to bigqueryexternal tables so if you are runningbigquery external tables today it'sreally a no-brainer to upgrade those tobigli tables and get better performancelower cost and fin an access control foryourworkloads put put together these toolswill really help you connect more ofyour data and more of your formats andacross clouds to more use cases andreally help Advance the development ofanalytics andAI now now another big area that keepscoming up all the time is that many ofyou have said that we really want topursue an open format strategy andreasons range from long-term vendneutrality or being compatible orinteroperable with multicloud or hybridarchitectures a single format can makethat easy but management of data ispainful you have to ride a lot of thesebackground jobs to constantly optimizefile sizes garbage collectionor be able to do features that willoptimize your overall query performanceand impact the TCO of your overallworkload this is why and this particularproblem also gets even more complex whenyou are doing use cases like highthroughput streaming change data capturewhich results into the dreaded smallfile problem that really ends upimpacting query performance and hadrequires a lot more infrastructuremaintenance and management to just keepthe workload really running in aperforming manspecifically when there are use caseslike gdpr which often require you toperform deletion operations at agranular level those becomes reallyexpensive because the open formats arereally modifying uh multiple files andthe toity could be a batch of files souh these are some of the challenges thatmany have been asking and they aresolvable but they require a lot of workthat creates U infrastructure this isI'm super thrilled to announce somethingspecial that we've been working on overthe course of last year it's called BigLake manage tables that provides a fullymanaged lake house experience for youropen format data it provides DML andtransactions on your parket workloadsfor your data deciding in your cloudstorage buckets it provides a fullymanaged lout experience with automaticbackground storage optimizations such asfile compaction and garbage collectionto help you make sure that you arerunning Optimal Performance and costand it provides an interface to doextremely high throughput streamingingestion that scales to tens ofgigabytes for the most demandingworkloads Paving the way for CDC andother Advanced analytics use cases thatyou can now run on open formatdata the key underlying principle for ushas been openness where data stays inyour buckets and it it's managed in openformats all of this is enabled in afully open architecture by bu buildingin full compatibility with Iceberg BigLake manage tables provide Icebergmetadata so that your open sourceengines can query it just like if theywere query ice spooktables Big Lake manage tables reallybrings the best of Google manage storagemanagement infrastructure that hasalready been tested at massive scalewith big query and innovates on it tomake it optimized and work on your openformat data we are very excited aboutthis there are three key ways in whichyou can use biglick manage tables let mestart by calling this out first one youcan use bigquery to perform DMLingestion into your tables just like youwould do on a big query native table andthen big query will uh Big Lake willactually uh perform all of theseunderlying data management operationssuch as file compaction autoc clusteringclustering of data automaticreclustering as well as garbage ction tomake sure that you don't have to ridethese infrastructure task and you stillget a very performant experience at theright pricepoint if you are building streamingpipelines that's the second use casewhere you can now use the bigquery rightAPI to actually write data to thesetables using connectors from popularopen source engines like spark or justbuild your own client Library basedconnectors to write it from your owncustom engines and this really scales toseveral gigabits per secondpart of the reason why this becomespossible is that historically when youare building High throughput steamingpipelines you are constrained by theright throughput from object stores orthe transactions on the openformats Big Lake manage tables take anapproach where inje is First ridden intoa right optimized storage that ismassively paralyzed and then it'sconverted into parquet into colum nerthis really allows inje to really scaleup to extremely high limits for the mostdemanding workloads but yet provide yourate zero rate Stillness your queriesactually merge the data between par aswell as the right optimize store to makesure there is no latency when you'requering theresults and then files uh the smallfiles are automatically uh compacted inin optimal manner to ensure the bestperformance uh and third is that you cannow also use open source engines toquery these tables via the icebsnapshots and the big link zero trustsecurity model of find an access controlalso applies in these use cases so theseare just some of the ways we are alreadyseeing customers build capabilitiesaround this and with that let me inviteRohit from rakuen to share a little bitmore about their Journey on lak houseand key learnings from it thank you GVand thank you for sharing uh sharingwith us the exciting features of uh ofBig Lake uh so my name is Rohit Divan Iam the CTO of the platforms group at uhat rocken and so while got talked aboutall the features of Big Lake I wanted toget more into kind of our motivationsaround moving towards the lake housearchitecture and some of theconsiderations we had in uh in choosingbiglake so rockan is a pretty fascinatingcompany we were actually founded back in1997 as the first uh e-commerceMarketplace in Japan and starting in theearly 2000s we started diversify intocredit card travel um other fintechbusinesses and kind of fast forward totoday uh we're a conglomerate of uh morethan 70 businesses that connect togetherin a ecosystem through a commonmembership and a common Rewards programknown as uh known as superpointsum if you're in Japan you pretty muchcan't miss us I mean almost 3% of JapanGDP goes through Rakuten in some way sowe had almost I think last year about 34trillion Japanese yen in uh in intransaction volume and so as you canImaginee we have pretty much veryincredible breadth and depth of uhdata so we have a I mean you can see ourmission statement up there I imaginemany of you in your data platformorganizations have something uhsomething similar um but this going tosimple Mission hides a lot of complexitybehind it so one is just a sheer numberof users we have uh more than 2,000 dataanalysts hundreds of data Engineershundreds of data scientists and also asa global ecosystem company um we havedozens of people who are working ongovernance audit security making surewe're compliant with gdpr CCPA and thevariety of kind of data protectionregimes around the world manyAcquisitions also means that we're deaomulticloud so we're using all the majorhyper hypers scalers and uh and we'reGlobal as well so at our level ofcomplexity I think one of the keyconsiderations is that any core datadata platform process needs to beautomated was any manual step simplywould not would not scale at at our atourlevel so starting in about 2000 we westarted on our I'd say data platformmodernization journey and back then wewere actually a purely uh kind of onpremise system uh but as a verydatadrivenorganization um the pressure on thatsystem was growing very very rapidly Ithink at some point point we're lookingat 1 or 2% growth per week whichcompounded is is absolutely massive soin order to achieve kind of moreelasticity and scale we decided to movetowards a hybrid data platform and so asof last year so this is October of lastyear uh we are now fully hybrid so wehave our data both on premise as well ason gcp now of course this comes with itsuh own set of challenges we move fromjust having say object storage andHadoop on Prem to now also having datareplicas in GCS and also copying thatover into into big query storage as wellso from a governance perspective this isincreasing our uh our our load becausewe need to make sure that permissionsare aligned across all these differentuh all these different systems um at thesame time from a data engineering orscientist perspective there's a longtime to get value from that data so oncedata was produced we we need to runpretty heavy ETL jobs now to copy itinto say big query where the bulk of ourusers were then going to consume it sothey might be like several hours or ormore of delay to get that uh time tovalue and additionally we had to as wemade copies through these uhreplicas uh we needed to be able tomonitor quality to make sure that thecopies are available and consistentacross each replica which is much morecomplicated than it sounds so this waskind of real motivation to say we shouldconsider a lake housearchitecture so fast forward to todayand so we were one of the early launchPartners uh for big lake and we've beenable to reduce our number of datareplicas in half so on Prem we uh wemoving to just object storage and on ongcp uh we're able to keep all our datawithin GCS and for the most part nothave to copy that into into BQ storageso this is of course now reducing thetime to data um it also simplifies ourour governance and access because ourgovernance teams can set uh set policiesum right essentially in one place andhave them apply to our data whether itbe on Prem whether it be on gcp in AWSor Azure um in addition kind of all allconsumption is is going can go throughbig lake as well so there are big lakeconnectors for all the popular queryinterfaces at least the ones that we useum like BQ and uh andtrino um I should mention we also havevery significant cross use of data andthat makes our governance even morecomplicated um but definitely moving toBig Lake has uh has helped us tosimplify that uhsignificantly so as we were going uhmaking this journey to Lakehouse therewere these were I'd say the five kind ofprimary considerations we had so one wasperformance so we were definitelyconcerned that moving to um not havingdata and big query storage would lead toa a significant reduction in performanceum however in our testing uh we foundthat if metadata caching is turned onthe performance nearly approached I'dsay like 98 99% approached the nativeperformance um if that data were in werein BQstorage um we wanted to have fewercopies of data and definitely that'ssomething we were able to achieve withbig lake as well and so getting muchfaster and realtime insights we're ableto set policies in in in one place sodefinitely it's our ciso is having fewersleepless nights as uh as we are aswell Iceberg support is something thatwas important to us I think as a companywe believe that's uh that's going to bethe standard uh standard format goingforward and so in a future phase uhprobably later this year we'll also beadding taking advantage of uh this theiceberg support um that's part of thebig lake Big Lakeecosystem so net net I think big lake ummeets all these considerations and hashelped us to kind of achieve the nextstep in our lake house journey Idefinitely wish you all the best uh inyour next step I mean so many of youshowed up for an 8 a.m. session whichtells me a lot of you are thinking aboutthis topic as uh as well um I won't bepresent during the Q&A but I will hangaround uh for some minutes after theafter the session uh to answer anyquestions you might have thank you somuch for your[Applause]time let me pass back togarvpartnership uh well Rohit mentioned alot about how getting to a lak housesimplified their old data architectureand really enabled uh many personas tocome together and consume that singlecopy of data uh now as many of you beentalking about uh lak houses uh one ofthe big challenges that we've beenalready hearing a lot is that how do wesolve the data problem of the AI let meexplain what I mean by that there's alot of value that many of our customersare exploring in the foundation modelsbut to really make it relevant andreally work for their Enterprise usecases it needs to be grounded in theirdata it not only needs to be grounded intheir data uh but it needs to happen ina manner that's secure govern privateand do all of that in a manner that doesnot really blow up price performance soto how do you really do AI in a scalablemanner by bringing your own data withsecurity and governance that's alreadybuilding that you've already curated andthis is where the big lake houseopportunity is to really simplify thosechallenges for youso uh we mentioned that the analyticsand AI lake house has for long beensiloed where analytics has largelyfocused on the structured data but thenAI especially generative AI which is nowunlocking all this value fromunstructured data had almost like aseparate path to get to the public Cloudobject stores where this data is relystored so the challenge we asked to theteam back is how do we help customersget access to their data using the toolsthat they are already amiliar with usingthe governance Frameworks that they arealready using and this is why I'm reallyexcited to announce the generalavailability of object tables objecttables are a layer on top of publicCloud object stores they representmetadata of underlying files that arestored in public loud buckets and theyare special kind of table because theyhave the ability to actually retrievethe underlying object and serve it tothe processing engine to run AIcomputation on top of it these objecttables are also really simplified todevelop pipelines when you add newobjects to the pipelines let's say newimages got added to a bucket the tableautomatically updates and reallysimplifies end2end development of AIpipelines and finally object tablesenforce big lakes fine security modelwhich means if you are securing aspecific Row in the object table you areactually securing the underlying imageand the retrieval of that image that canhappen to the signedURL these capabilities make it reallyeasy to build these use cases let megive you some examples of use casestoday that we are seeing well first up alot of companies today as they arebuilding their own versions ofEnterprise llms or fine-tuningFoundation models to build their ownversions are first needing to dopre-training this is where we call abouthow do you actually take the data thatyou have collected from various sourcesin Object Store like GCS but still beable to uh transform it parse itor be able to filter out for things likeopts outs or training license rightswhich is largely stored in structuretables or it's extracted from thisdata through big query object tables youcan now join object tables with rest ofyour other big query tables where youcan write simple expressive SQLstatements that help you curate yourtraining carpus for this fine trainingto happen this makes it really reallysimple that otherwise would take a lotof coding todevelop the other thing that we arealready seeing is once you actually getto a model how do you know that it'sreally safe for your use cases how doyou know it's really high quality andgiving good responses this is where youcan create object tables and call thatobject call that model uh directly usingbig query whether it's hosted in verx orwhether it's hosted in managed by you onyour own custom infrastructure throughsimple lines a few lines of SQL we arealready seeing examples where Customsare creating this object table overimages and then use it bigquery SQL tocall Vision API to figure out safetyelements of the image and then be ableto take action on it one of the actionsis really how do you actually securespecific Pi objects if you detectspecific metadata and you can then cutoff that access to specific groups oruser admin personas to be able to makesure that the AI development that youare doing is consistent with thegovernance that policies that you havedescribed for your ownorganization we're now taking thesecapabilities one step further where ifyou think about the documents use casesa lot of data whether it's PDF files ortext files resides in object stores wellnow you can use document AI workbenchand with just a few clicks you can traina custom llm extractor to extractspecific things asked from the documentyou can ask a question um to reallyfigure out what specific fields andschema you want to Define for yourextraction and that's created thatreally generates a API endpoint thatgets deployed well that API can now becalled using big query remote models youcan simply register that remote model inbig query with SQL and write SQL onobject tables on PDF files to be able toinvoke that API and then be able toextract text from the documents find GREaccess control like I said is alreadybaked into the framework so you canreally control if you want to secureaccess to specific documents thatcontain sensitive information or PRinformation well you can then builddocument analytics in bigquery bycombining with rest of your structureddata or you can also do other manyinteresting things with it we are seeingopportunities to take that extractedtext in big query and with the latestcapabilities that we announced yesterdayon embedding generation you can nowgenerate embeddings on those textcolumns that embedding an index can thenbe sync to the vortex matching enginewhere you can actually leverage vectorssearch to implement your Enterprise llmchatbots that is grounded in your datato implement techniques like retrievalaugmented generation and all of thesecapabilities make it really reallysimple with few lines of SQL to bringyour data to make custom tuned LMapplications to do and to do it bypreserving security and privacy in agovernmanner so I'm really excited with allthe capabilities that we shared with youtoday and the experience from Rohit andhis team to help kind of build theselake houses and we can't wait to see howyou'll actually build in your own[Music]Journey"
}