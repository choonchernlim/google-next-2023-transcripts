{
    "title": "Learn how Discord and Tamr deliver ML-driven experiences with Bigtable",
    "presentation_type": "Breakout",
    "categories": [
        "Database Professionals",
        "DBS202"
    ],
    "video_id": "dwBj3S9H1_o",
    "time": "Aug 30 10:00 AM - 10:45 AM CDT",
    "transcript": "foreign[Music]good morning everyoneit's 8 A.M on a beautiful San Franciscomorning thanks for joining us uh whatbetter way to start your day thanlearning from two Innovative companiesabout how to use machine learning atscale to solve real world problemsmy name is Borah I have the pleasure ofbeing accompanied by two of myco-presenters Gandalf from Discord whichyou'll hear from soon and Daniel fromtamerI hope our session will have somethingrelevant to everyone in the room becausewe'll go after different use cases soindependent of the industry you're in orwhere you are in the ml life cycle or agrowth trajectoryhopefully we'll have something for youfirst Tamer will talk about how theyoffer a data platform for Enterpriseswho are trying to work with data qualityissues especially as they merge datafrom disparate sources then we'll haveDiscord talking to us about how theybuilt this machine learning platform forthe data scientists and app developerswhere they can more effectivelycollaborate and reuse the data and servelow latency real-time predictions totheir usersI'm sorry okay but before we do thatlet's talk about bigtable the databasethat is behind a lot of these use casesthat we're about to go throughthis clicker is very confusingGoogle offers many different databasessome of them are relational databasessome are nosql databases and caches anddepending on uh where you are sometimesyou may want to use third-party enginesin Google cloud like Oracle SQL ServerMySQL Google also has its own databasesthat is homegrown and bigtable fallsinto that categoryand we may say it's one of the mostinfluential because it was arguably thedatabase that led to the nosql databasemovement uh 18 20 years ago which led toopen source projects that are verypopular today like Apache Cassandra andhbase they were inspired by a big tableand big table has been in production andcontinuous production for the past 15 18years at Google and it's the databasebehind a lot of Google's billing userplus services like Google analytics adssearch maps YouTube Etc it currentlyserves about 6 billion queries persecondanswer uh handles over 10 exabytes ofdataand for the past eight years uh bigtable is also available as Cloudbigtable Google Cloud users and it'sused by some of world's largest Brandsonline from social media to streaming tofinancial services crypto you name itso bigtable offers consistent singledigit millisecond latencies and highread and write throughput and you candeploy it in a multi-mastermulti-primary setup where you could haveclusters across the world in a globaldeploymentfurther reduce the latency is if you'reserving users and you can bring the datacloser to them it helps you reduce thelatencies further as a result of thisit's good for many different use casesbut since we're talking about machinelearning today I'll highlight some thatare immediately relevant to machinelearningso in the domains of AD Tech retail andmedia personalization for example is oneexample of this and if you want to learnmore about how this works Google adspublish the paper on Google Cloud bloglast year which goes into the details ofhow they use bigtable for serving adspersonalized ads at scaleacross financial services iot and otherareas again future store online featurestore is a very common use case itserves different use cases like frauddetection and credit decisioning we alsosee a lot of big table usage in iotTelemetry for stuff like alerting andpredictive maintenance where big tableis high read and write throughputbecomes very handyand we have even seen use cases whereour users got created with hashing therow keys and they use bigtable forthings like embedding search nearduplicate detection Etcso I walk you through some big table usecases and a quick overviewlet's hear from Daniel about how to usebigtable in productionall rightthank you Borahhi everybody good morning uh veryexcited to be here opening for LL Cool Juh never never imagined in a millionyears that would be happening but herewe areall right let's seelet's see if I can figure out theclicker there we go that's my generativeAI right theremy name is Dan I am a co-founder ofTamer and today I'm going to tell youabout Tamer and bigtable unsurprisinglyand uh and you know we we began to adoptbigtable along with hbase about five sixyears agoand it's been a huge success story forus uh from a technical adoptionperspective and our our usage ofbigtable as a system and as a keycomponent in what we do has has grownand grown over timeum so it's a fun story let's get into ituh first let me introduce you to Tamerand who we are as a company uh Tamer isthe world leader in ml driven datamastering and data unificationumwe were co-founded uh by you know amongamong uh others among myself uh by MikeStonebreaker and Andy Palmerwho previously co-founded the companyvertica systems another large-scale uhuh distributed database and uh uh youknow Mike as you may know is the touringAward winner for his contributions tothe field of database systems so it goeswithout saying that we're folks who knowa good database when we see one uh andand so so you know that's that's part ofwhy this is coolnow what we do at Tamer is we take datawhich generally speaking is a huge messall of the time and we turn it intosomething useful we take data and weapply all sorts of different techniquesto clean it enrich it combine it matchit together so that you can get highquality outputs that are actuallyactionable for a business we do thiswithin large Enterprises and we do it byapplying machine learning and artificialintelligence techniques to essentiallydeliver results faster at lower expenseand uh and with with just generallybetter results than you could achieveusing traditional methodsuh our product and our engine is basedon over a decade of research anddevelopment in this field of applyingmachine learning and probabilisticmethods to problems in data integrationand uhour product bakes all of that in andpresents it in a way where end users cantake advantage without themselvesneeding to you know hold a PhD inmachine learning or or neural networksor anything like thatnow uh that's a little bit about Tamerlet's get into uh some of the context ofhow how we use bigtableI'm going to go throughGeneral context for why we chosebigtable as a system and as kind of aCornerstone of our product in the firstplace uh and and then I'll get into somespecific highlights of experiences I'mhoping that kind of that range ofcontent will appeal to folks who arecompletely brand new to bigtable on onehand as well as to practitioners whohave been playing with it for a longtimeso first of all some high-levelarchitecturethis is tamer's solution architecturefrom 30 000 feetuh it's fundamentally a very simplepicture on the left hand side you haveSource systems So within a largeEnterprise this is anything that holdsdata or generates data so applicationsdatabases API Services whateverwe're going to load all of that datafrom these sources into our systemprocess it improve it in a myriad way anumber of ways and then we're going totake those improved results and deliverthem to Downstream systems on the righthere so that could be bi toolsapplications Services apis AI modelsdata warehouses you name itso fundamentally this is a picture of apipelineand you know it's take data from point Aprocess it make it good put it down inpoint Bbut the architecture is also in a way astateful databasewe store results and intermediateresults from these processing pipelinesso that we can support features likeincremental updates so that we canprovide apis and a user interface whereusers can come in and interact with thedata and improve the pipeline and theprocess as they go and we can also actas a system of record within a contextof like Master data management so all ofthat depends on being able to take thislarge mass of data that's kind ofin-flight processing and store it withina distributed database engine that cansupport all the work workloads that weneeduh looking inside the Boxthis is some of our internalarchitecture I want to focus really onthe the bottom left corner here whichare the three pillars of what we callour data planeso these are Cloud dataproc whichprovides distributed processing andSpark It's bigtable which is our primarystorage for data scale artifacts andit's bigquery which we use for views andad hoc queries that support featureswithin our user interfacenow let's talk about the problem thatwe're solving and why it's so criticalto have a highly capable distributedstorage system within it the mostcompute intensive problem that we solveis clustering records matching recordsacross all these different Sourcesystems and doing it in a way where wedon't have a join key this isessentially a fuzzy join where we'reusing probabilistic techniques and anensemble of models to learn whatconstitutes a join uh and you know whathow do we map records from all thesethings to the same real world entitiesso that's fundamentally an N squaredproblem you need to compare every recordto every other recordum algorithmically of course we don't doall that but if you want to achieve Highrecall you really do need to explore alarge portion of that quadratic scalespace to be successfulwe do this on data sets that containbillions of records and that are updatedmillions of Times Daily and so we needto be able to handle processingworkloads that that can apply our ourpipelines at that scaleour system supports sub second ad hocmatches and updates so that's anotherlayer that needs to be added into thisso at that scale we need we need a backend that's extremely performantso bigtable satisfies all of theseworkloadsit it supports the high throughput batchprocessing that we need both in terms ofreading and writing data it supportstargeted incremental updates eitherwithin pipelines or via apis and then itsupports low latency interactions andpoint queries and all sorts of sorts ofthe the good lookups you get from ahighly capable key Value Storeuh but it doesn't just satisfy the therequirements of our workloads it reallyhas these major advantages that put itover the top compared with uh othersystems in you know in the domainfirst of all it really sits in thisoptimal position in the trade-off spaceum thanks to the internal architectureof bigtable uh and you know this this iswhat Bohr was mentioning about whybigtable kicked off this nosql trendit's capable of uh maintain uhessentially write performance and readperformance does not degrade with thescale of data stored within the systemand that's critical for us because we'rewriting huge amounts huge volumes ofdata and we need to make sure that thesystem doesn't suffer the more that thethe user puts in ituh another key Advantage is theelasticity of bigtable you can scale ItUp and Down based on workload it has anauto scaler built in or you can manuallyresize clusters on the Flyand I'm going to talk for uh first in asecond a little more about how thatgives you free performance but the thereally critical thing just fundamentallyis that allows us to balance batchworkloads with low latency queryworkloads and handle both of thosetogether in a way where we can stillmeet our slasfinally as a managed service the totalcost of ownership of bigtable is justit's like orders of magnitude lower thanrunning comparable systems yourself wealso support hbase and Cassandra andbigtable makes things possible on GoogleCloud that are you know simplyUnthinkable on other clouds or operatingon-premall right so that that's kind of thecontext on why we chose bigtable let'sget into some of the the details of ourexperience with itum just some quick and dirty numbersthis is like a typical customer uhdeployment we're processing about 15terabytes of data can range between 10and 20. uh and we're going to have abackground query load of about you knowsomewhere between four and eightthousand queries per second uh targetinga P99 uh latency of 10 milliseconds uhwhich we typically achieve at the scalewith just a small three node cluster soextremely performant extremely reliablethen customers will run periodic batchrefreshes so let's say like a weeklyrefresh of all of the data toincorporate holistically whatever'schangedand uh when we do that we'll launch aspark cluster that's going to do allthis processing back and forth Hammerthe systemand so the system will scale up to let'ssay a 30 node maximum and uh and give usextremely high throughput while stillallowing us to to hit those uh thoselatency slas for the the interactivequery loadnow uh this I think is is some of thecloud magic that bigtable brings and andthat that Google services kind of bringin generalumuh essentially if uh we extensively useelastic and ephemeral resources withinGoogle Cloud so when we run large-scalepipelines we're going to spin up adataproc cluster on the Fly Just for thethe purpose of that pipelineuh and and we use Auto scaling withinbigtable uh so what what we're able toachieve so that the chart on the left isshowing run times for clusters ofdifferent sizes and what you can see isthat essentially you know we'reachieving near linear speed up whilewe're scaling up the size of uh of thehardware that we're throwing at thesepipelinesum so that's great it means that theproblem is well parallelizedum what's really remarkable is the charton the right though so this is the costof the runs for those different sizeclusters and what you can see is thatthe the cost on the large clusteris you know within 10 the same as thecost on the small cluster but it wentthree times fasterso it's because we're we'reparallelizing the problem very wellachieving extremely high utilization wecan throw more resources at the problemsolve it faster and then get rid ofthose resources and stop paying for themand so like this is this is some of oursecret sauce like this is amazingumlast thing I just want to share somelessons that we've learned from workingclosely uh with bigtable uh and thesekinds of use casesfirst and I think this is like big table101 is that key design how you designyour your primary Keys uh is is criticalthis is especially true for uh batchprocessing use cases if you want to getgood parallelism like we saw on theprevious slide in that linear speed upuh you need to use key design that'sgoing to spread the data across thewhole cluster and give you that liftum if you're doing that one neat trickthat we learned that will speed up yourright throughput by a factor of five isto sort batch rights this is likesomething nuts that that we we sort ofstumbled uponum this chart here is like midexperiment like the thing on the leftwhere it says April 23rd is like withoutthis trick and then like that Spikethat's off the chart is like we turnedwe made a one-line code change and we'relikeholy crapum so basically if you're hashing yourkeys it's going to spray data all overthe cluster if you sort your batches itreduces overall Network load in the theservice layer and uh amazing thingshappen so definitely do thatum finally this is sort of an unexpectedadvantage that we discovered as we weworked closely with bigtable it's anamazing platform for performanceengineering it comes built in with awide set of Tools around observabilitymetrics and the key analyzer that giveyou just an incredible developerexperience so it's great to just dive inget your hands dirtyand that's itall right thank you Gandalf take it awaythank you so much didn't know about thesorting and we might have to do somework later todaythank you all so much for being hereso let's let's jump inso first of all I'm very happy to behere today to talk to talk to you abouthow Discord leverages gcp Technologiesspecifically bigtable for a future starmy name is Fernandez and I head up theml platform team at Discorduh but first if you're not familiarfamiliar with Discordthis could release about giving peoplethe power to create space to findbelonging in their lives we want to makeit easier for you to talk regularly withthe people that you care about we wantto help you build relationship withfriends and communities even if they'relocal or if they're across the globe inthe eight years since we launched webuilt up a dedicated user base over 150million users that interact in over 19million communities some of ourcommunities are huge uh you may haveheard of mid-journey for example butmany are small groups of friendsclassmatespeople who share an obscure hobby I'vecome across people who built 1970synthesizers I've come across peoplethat are very very particular about themagic the Gathering rules that they playthey find their communities on this cardso what about machine learning andDiscordwe use machine learning to keep ourusers safeand detect potentially harmfuland abusive content and also to keepunwanted actors outwe also use machine machine learning tohelp keep our users up to date with whatthey care about and help them find newcommunitiesbut machine learning is nothing withoutdata and data is nothing without a fastreliable and scalable way to get at itand that's why we're here today to talkabout how we are building our futurestart Discord using Google Technologiesuh but before we jump inwhat is a feature storeand in case somebody here is not veryfamiliar with machine learninga feature is a piece of data that isused as an input into into eithertraining a model or once you havetrained them all to make a predictionto us a feature store is a layer on topof our existing data warehouse topresent the data in ways that make sensefor machine learningthe feature store makes it easy toaccess data for machine learningtrainingand abstracts Away the complexities ofwhere the data is stored of how to andhow to make predictions at real time athigh scaleand also a feature store should make itreally really easy to discover andunderstand featuresfeatures can be accessed in multipleways data scientists they love to usejupyter notebooks and want to easilyconsume them via pythonas they build evaluate and iterate ontheir modelsour backend and machine learningEngineers need them in real time to makeprediction based on user actions atScales when you have tens of millions ofviews that are online at the given timeour underlying data warehouse of Discordis bigquery basedbut we don't want our machine learningEngineers our data scientists to have tocraft complex bigquery joints to gettheir data we want they want to haveaccess toumspecifically we want to give them an APIthat makes it very easy for them to dotime traveltime travel is the ability to accessfeature values as they were at aparticular point in time in the pastso you want to say at time T when I'mlooking to make a prediction what arethe values of all the features that Icare about some might have happeneda second beforehand some may havehappened a day beforehand and thesequeries can get extremely hairy if youdo themand if you mess upyou start pulling out data from thefuture your machine learning model whenit's in it's going to work really reallywell when you put it into productionit's going to work really really badlyand this is uh yeah it sounds funny butit actually is a real actual problemwhen you have data that lines up onalong different time scales so we wantedto just fully abstract that awayand just gives a user an API saying giveme all the values up to time t for whatI care aboutanother thing that we see is in machinelearning in general is that a commoncommon anti-pattern if you don't have astrong feature store is that each teamor even individuals within a teamwill do the exact same featureengineering they'll recreate the samedata point over and over and over againthey may not they may find even findsomething and say well I don't know do Itrust this I'll just I'll just make myownumadditionally in a typical data warehousethe main unit of abstraction is thetable you know we look for we look for aparticular table and we say well butwhat are the keys on it and so on but inmachine learningthe key abstraction is really is thefeature or if you're mapping that to adatabase Paradigm is really the columnif you're looking for the popularity ofa server at Discordyou shouldn't have to care that thattable also hasthe category of the server or some othermetadata about itwhat you really are looking for is Ineed the popularity of the server sowe're trying to abstract that away andhave users think of it in terms ofindividual features that are keyed offof individualindividual keys so you know what do Iwhat do we know about our users what dowe know about our servers what aboutknow about our user and serverinteractions and they could all comefrom different tables but we present onepercent a unifiedinterface to our to our usersuh when the feature storm becomes themain level of abstraction it makes it alot easier to for discovering featuresit makes it easier to reuse featuresum and by making this access easier ourdata scientists are much more likely toshare features because they can pull itout run some some simple analysis on itsee that it serves its needs and theyuse itwe also need very strong support forreal-time features if a great momentthat has happened that we think ourusers will love will love we need totell them right now we can't tell themhey yesterday there was this awesomeconversation that we really thought youwould have loved nosame work if there's a bad actorhappening or there's spam happening weneed to detect that as soon as possibleas soon as possibleand we want to be able to again abstractthis away from from our use from our usea consumer of a featureshouldn't have to work with the featurein different ways if it was created inreal time or if it was calculated inbatchalso Discord has has had great successbuilding on top of bigquery for datawarehouse and bigtable for multiple usecases and we want to continue we want tocontinue with that so therefore it wasvery important for us that we were ableto leverage our feature store on top ofbigquery and on top of bigtableso let's take a little bit of a look atour at our architecture hereas mentioned we bigquery for our offlinestore and bigtable as our online storewe support three different ways ofgenerating data you have all batch whichcould potentially be consideredtraditional machine learning featuresare calculated periodically daily hourlyin our case we have jobs that arecalculated every 15 minutesumuploaded into a feature store and I usefor serving this model is pretty easy topretty easy to implement you can usebigquery you could use black spark ordata flow or any other any other job butyou don't have the real-time aspect ofitthe second version is the Lambdaarchitecture where you have slow movingcomplex features still calculate thembatch in our case using bigqueryuh while now is starting to bring inreal time stream real-time featureengineering you know when was the lasttime the user sent a message what wasthe last server they interacted theyinteracted withand we merge them together in the futurestore so that a model that needs to makeprediction or an engineer that isworking on a modelthey access the data in the same waythey don't have to think about if it'sreal time I need to go over here if it'sbatch I need to go over hereand the Third Way is the Kappaarchitecture where we're now movingeverything to real timeprocessingso events are coming in we process thembut in order to bootstrap events wheresometimes the features need longerhistory this is specifically trueespecially through in the safety domainwhere you may have events that are veryinfrequentbut you need to react to them veryquickly when they when they come in soin that case we blunder our feature inany streaming pipeline where we feed itusingbatch data stored in iceberg Blended inwith the real-time stream coming comingin Via Pub via Pub subthis unifies with the featurecalculations are doneand we we avoid this kind of mismatchwe're trying to have one slow movingversion of a feature and then a fastmoving version or feature and trainingand training on both of themumbut again the key when building out thisfeature store is that our usersdon't have to care about how the futurewas generated they may want to know thatthis one is updated as soon as an actionhappens but in the way that they writetheir code and the way that they accessthe data it's all Unifiedso let's talk a little bit morespecifically about about bigtablewe do take advantage of big tablesmulti-sonal feed sonal features foravailability and durability we rely veryheavily on big tables Auto scalingcapabilities we have one cluster that isused for writing bash data it scales upheavily when we're doing ingestions andthen we have two read clustersthat I use for the real-time serving andthen we we use big tables likereplication features to get the data toget the data overall of our big table data today is readonly as far as like customer interactionpart is as concerned so we could in mostcases recreate recreate the datalatency requirements varies from usecase to use case but big table is reallynot a significant source of latencywe're seeing p50s on the order of threemilliseconds and our p99s are typicallyaround 10 milliseconds but they do Spikeup to 50 milliseconds when we're doingingestions we know that we can get thisdown lower but frankly it really hasn'tbeen a priority and bigtable is not abig source of latency in our systemwe take advantage of big tables time tofeature to clear out all data whichsaves us the efforts of having jobs topurge out to purchase steel dataumand we're currently actively working onintegrating feasts with the fees featurestore and removing our our existing realtable real-time systemonto feastboth bigquery and bigtable have showntheir strengths and we're very excitedto keep working on them as we keepbuilding out our feature store enamelcapabilities at this card in greatpartnership with Googleumand that is all I have thank you so muchand we are hiring uh you have my namehit me up on LinkedIn and let's talkthank you so much[Applause]thanksokay so you heard about these great usecases and you heard about somecomplaints like I'm doing rights itimpacts my read latencies Etc I'm happyto announce if you want to implementsimilar Solutions yourselvesuh things will get betterso this is the chart both Gandalf andDaniel we're talking about if you runeverything at the same job prioritysometimes if a heavy batch ride or aheavy batch read comes in you may havesome low latency serving workloads thatwill get impacted in this case left sideis before and right side is after thisnew feature recall request prioritieswhen a heavy batch Right comes in ifeveryone is running at the same priceeverything is running at the samepriority you can see these p99s go to inthis case it's four seconds from yourtypical sub 10 millisecond latencies butwith the request priorities which is inpreview now and we'll have a blog postthat will come out tomorrow that'll havelinks to sign up for all of thesefeatures that we're talking about hereyou'll be able to run those jobs batchreads or rights at low priority whileyou're serving workloads running at highpriority then we'll be able to throttlespread out de-prioritize those batchprocessing jobs that way you will seethe chart on the right hand side wherethe impact when those jobs kick in arefairly noticeableand if you combine this with autoscaling things will get even betterbecause this is a way of absorbingthings by spreading out the loan what ifyou have to load more then you can turnon auto scaling will also combine thatwith thisso we're very excited about this thissimplifies a lot of things you don'thave to copy your data out to do heavybatch processing to other systems so itsimplifies it reduces the cost for youof course anytime you take the data outyou have the problem of things going outof sync you want to serve it I have topublish it materialize it into an onlinestore like bigtable and this is a verycommon flow and you've seen this in achart like three minutes ago Gandalf hadthatbut you need to have data pipelines itcould be a data proc cluster data flowCloud composer there's some sort of ETLtool that you have to configure or payfor have a data engineer work with youso there's a lot of impedance there andif you want to innovate you want to movefaster so we worked very closely withthe bigquery teamuh to build this directly into bigquerynow you'll be able to write a SQL queryand just export directly from bigquerythere's no tool in the middle uh youcould do this using the apis you canalso use the the UI in there to quicklyconvert your SQL code into an ETL joband this comes with a lot of flexibilityagain another feature currently inpreview you will be able to evenschedule these things you could say Iwant to run this job every 15 minutesbigquery has this appends table valuefunction that will give you things likewhat happened since this time so you canlook at deltas and even you know figureout how to write those Deltas only intobigtable so it gives you a lot offlexibility a few clicks you go from aSQL query into an ETL jobagain you will have links to this in theblog post but since this is such an easyfeature to adopt and play with we alsomade it convenient and added a QR codefor you if you want to take a picture ofit you can sign up and it will happilygive you access and I think again withthis feature and more that I can't evenmention because we are out of time a lotof the things that we discussed theseuse cases will become a lot more easyfor you"
}