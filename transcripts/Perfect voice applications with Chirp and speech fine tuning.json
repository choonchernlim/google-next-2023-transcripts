{
    "title": "Perfect voice applications with Chirp and speech fine tuning",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML114"
    ],
    "video_id": "xT348FrrFZ0",
    "time": "Aug 31 12:15 PM - 01:00 PM CDT",
    "transcript": "foreign[Music]thanks for coming today on the last daynext to talk about uh perfecting voiceapplications we're going to be talkingabout our new speech models talkingabout how to apply it but before we getstarted I'll tell you I'm Calvin BarnesI'm responsible for the product andEngineering teams part of the speech andvoice features here in Google Cloud havewith me a couple colleagues my name isHarris I'm the product manager forSpitzer text too uh I'm Jeff Curry's andI'm A customer from Bill Canadaso we're going to be talking about a fewdifferent things today I'm going tostart off telling you about the speechapis everything has changed in the lastyear since last nextwe'll talk about our brand newFoundation model for a speech chirp uhthat maybe you heard about in thekeynote or as part of the other vertexAI talks Jeff is going to talk to youabout how they're using speechtechnology uh to change a business afterCanada and then Harris is going to giveyou a preview of some of the things thatare coming next to the speech apisfocusing on speech fine tuning andimproving the models even further usingour foundation models foreignbut let's get started talking about thecloud speech apis most of you if youattended the keynote or other sessionsso far probably have heard about vertexAI uh and our speech models and speechservices uh are in the model garden andwe expose actually over 750 differentspeech models including speech to texttext-to-speech uh first partyproprietary models from Google in themodel Gardenbut we make those availablethrough the speech to text API butbefore we get into that I think it'simportant to think about uh everythingthat Google has done in the speech spaceover the last 20 years so Google's beenworking on speech to textuh for over 20 years nowand there have been a number ofadvancements in that time so probablysome of you are familiar with the veryearly products like voice search uhdictation and gboardand some of the more recent ones Googleassistant or recorder and dialerfeatures on the pixel phonesin uh 2016 we launched thespeech to text API which was the firstversion of making this available tocustomers uh to Enterprise customerssince then we've continued to innovateand invest in research for speech atGoogle to push forward uh what'spossible and bring speech technology tomore and more both customers and usersuh last year we talked about ourstate-of-the-art conformer models whichare still available today the fastestand highest qualitystreaming speech models available andtoday we're going to talk about our newresearch the universal speech models andthe chirp Foundation modelall of this has come togetheracross Google to create this set ofservices so even if you've never heardof the speech to text API or use thespeech to text API from Google Cloudor experimented with vertex AI youprobably have experienced speech fromGoogle whether that's using Googleassistant or Google Voice search whetherit's dictating with gboard on yourAndroid phone reading subtitles onYouTube live captions on me the listgoes on but what we've done in Googlecloud is take all of this research inall of these areas that we've used toperfect voice and speech for a number ofdifferent 1p applications and we makethat available to Enterprises anddevelopers in our Cloud speech APIthe cloud speech API within the vertexpre-trained apisgives customers super high quality outof the box models I talked before wesupport over 135 languages and we havemultiple variants for multiple differentuse cases like telephony dictation longaudio in those languages so you get over400 out of the box models that give yousuper high quality to get started todayuh but you can also use the API to takethis further customizing your outputwith punctuation diarization uh as wellas fine-tuning towards specific wordsand use cases and today we're excited toannounce new features on fine tuningthat Harris will talk to you about laterfirst though we want to talk about ourrecent GA of the V2 of the speech APIwhich happened this past Julythe V2 API is the next generation ofspeech to text serving at Google like Isaid in the previous slide we've beenmaking speech models available toEnterprises for almost seven years nowand it was time to modernize some ofthat infrastructure and that's whatwe've done with sdtv2 so the same greatmodels that we were exposing before thesame endpoints and features that you'reused to but now it is available for dataand use residency in any gcp region andalso in the US setting us up for fedramphigh and il-4 for those of you runningprotected workloads and other individualrequirements for data residency acrossthe worldit also exposes new tools for servicecontrols encryption and fine-grainedpermissions on individual models andrecognizers so that regardless of yourapplication for speech to text you canbe sure that we can meet your Enterpriserequirementsuh finally we added a new endpoint thatI'll talk more about in a second calledDynamic batch which allows customersthat have uh workloads where they maybehave large amounts of speech that theywant to transcribe but they are not timeor latency sensitive to transcribe themover a 24-hour period instead of gettingresults right away this allows us totake advantage of uh gaps that we havein our infrastructure usage and providetranscription at much much lower costsour hope is that this enables even moreuse cases to be able to take advantageofautomatic speech recognition technologyby bringing it in at a much lower pricepointum V2 as you'll hear about is also ourstarting point for new models and newfeatures like our trip Foundation modeland custom model fine-tuninguh in addition to these new featureswe've also listened to customers andheard that one of the biggest blockersfor adoption was the pricing of speechto text Services everybody's use case isdifferent and the amount of value thatyou get from transcription can reallyvary depending on what you're doing sowe wanted to work with customers to makeit possible to adopt uh regardless ofwhat their use case was so we've droppedthe base price on speech to text 33 from2.4 cents per minute to 1.6 cents perminuteand we've also introduced native tieringso as your workload grows your costswill grow non-linearly all the way up to83 out of the box discounts for verylarge workloads in addition I mentionedDynamic backs before so starting youknow with whatever size of workload youhave you can transcribe uh for much muchcheaper if you're willing to accept theadditional latency in getting thetranscript backokay so now that we've talked about thatI think we should talk about our newFoundation model which we also madegenerally available in July called chirpuh so this is in addition to all of thespeech text models that we supportedbefore and Chirp is based on Google'sUniversal speech model technology sowe have an initiative at Google calledthe Thousand language initiatives wherewe want to try to support uh the vastmajority of the spoken languages in theworld and support very well uh everylanguage with more than one millionspeakersbut we knew that this was going to be ahard problem with the current technologythat we had for speech to text andspeech to Tech modeling because speechmodeling is a very data intensiveoperation requires a lot of labeled dataand it just you need a lot of data foreach languageso what we've done with the universalspeech model is created a universalmodel with one unified encoder that wastrained on over 12 million hours ofunsupervised audio so this is audio thatwe have not had to label or get humantranscripts of in order to use it fortraining in a wide variety of languagesand this allows us to not only supportmore languages that where we only havevery small amounts of data it also haslifted quality in a number of languagesas well so once we train that unifiedencoder we then train decoders for eachindividual language that we support andtoday we're only exposing 135 differentlanguages through the chirp model but aswe label more data in more languages andtrain more decoders we'll be able tovery rapidly add more languages comparedto how long it would have taken us toadd a single language beforeso to show you some of the effects ofthis you can look at the performance ofchirp on a variety of languages and soyou can see thatuh we get improvements across the boardhere so on common uh English academictest sets Libra speech we achieved over98 accuracy on English with the chirpmodeluh but for lower resource languageslanguages like Romanian polish uh wherewe have a lot less data we achieved over300 increase in those 300 accuracyincrease on those languages versus whatour current internal state of the artwasumand across the board on our YouTube datasets we have an improvement across everylanguage that we have data for uh whenyou combine it all togetheryou also get gains in what we call headlanguages like English French ItalianGerman and Spanish especially on veryvery challenging data sets so 98accuracy will not be possible for everyuse case like it is on Libra speech ofcourse but we also are achieving 88accuracy on Coral which is consideredone of the hardest English languageacademic data sets anduh much much better than any internalfine-tuning on just that test set thatwe had done beforeso we're excited to bring chirp uh to avariety of customers in avariety of languages and we will keepadding more and building on thefoundation model of chirp over 10. butit is generally available in theselanguages now in the speech to text V2APIall right so now to talk about how youcan use uh these models uh chirp and V2and our existing models in everydayapplications to make it uh better foryour users I'm going to hand it over toJeff from Bell Canada to tell us aboutwhat they're doing thank you sirum let me go I'll walk you guys throughhow we're using it today just to tostart with for those of you that aren'tvery aware of what bell is basically forthe the non-canadians Bell is prettymuch Canada's largest telecommunicationsat work we sort of split it into threedivisions one being Wireless so roughly10 million Wireless subscribers uh wireline so your home phone TVand internet services about 9 millionsubscribers and then there's a mediaside to the business whether that'srunning mixture of ads TV radio and soon but that's sort of the overview ofwhat we arefor myself I think it was about threeyears ago they brought me in to leadthis newly formed speech analytics groupso think about it is taking all of ourcall center information uh our callcenter calls turning them into text andtrying to drive value out of thoseslightly different approach from whatmost people do so most people the firstthing they want to do is go in there andjust build va's and chat Bots and so onwe sort of took a slightly differentapproach where we were going to build AIproducts that helped Drive the businessand improve the customerexperience across the business so wesort of brought together a bunch of NLPdata scientists some analysts to sort ofhelp prove out the business cases andthen product managers to actually runeach of these Solutions of its ownproductumso from there where we sort of led to tohow this Evolution would work if youthink about a general call center andhow it kind of works you have twoprobably sort of normal ways in whichyou would call listen in the past so onebeing if you're old enough to know is asort of a y Jack solution or what youwould probably do as like eavesdroppingon the customer agent interaction todayso you would do that in real time andpotentially provide real-time feedbackto that agent the other is basicallygoing into a tool like variant whereyou're kind of listening to those callsafter they happen maybe days or monthsor weeks after but for the most part allyou're really doing is probablylistening to maybe less than one percentof all those calls so when you thinkabout that for somebody who's reallysort ofreceiving about 10 million calls a yearplus even on those 10 million calls it'sabout 500 000 minutes of call listeningum and then you kind of do thatbackwards math to kind of figure out howmany resources that would take that's athat's a lot of resources that are justdedicated to call listeningum and the other thing that comes withthat is that bias you know like is everylistener the same did I forget them tolook for it to certain topics so did Iask them to look for empathy and if Ididn't ask them to look for empathythey'd have to go back and redo all thatcall listening so the way that we'vekind of approached this now is we weinternally term it a virtual coachingassistant but that virtual coachingassistant listens to 100 of all callsand then we're able to go back andrescore all those calls for differentattributes that we may want to to wantto go back in and sort of analyzeumand how we did this sort of at a highlevel here's kind of the high levelarchitecture of how this kind of cameabout for the most part you can imaginefor us anyways there's an on-prem to letstack it telephony systems that sitthere from those we kind of stream theMana this is a very high levelexplanation but we stream them into thatspeech to text pipeline we then used aseries of other connectors behind thatwhether data flow pubs up topics and avariety of other things write them intostructured databases and then run themthrough the entire sort of vertex AIstackumwhen we originally built this at thetime sort of vertex AI was a work inprogress at the same time so we weresort of building all these things asGoogle was sort of mastering them aswell too so it was a great experience tokind of get them all in place the otherthing too is now we've relied on theseoutputs so much that they're pretty muchingrained in a lot of our day-to-dayoperation systemsumsome of the key areas that you'll seehere the top three are probably the themain three that we kind of focus on butsort of that sales enablement area amaking it right program so you knowtrying to trying to sort of intervene inconversations and interactions that justfelt like they went sideways that wejust want to immediately try to correctand then just general Improvement of thecustomer experience so with all thiscoaching and all these tools how do wejust generally make that experiencebetter and we sort of found enormousgains through these speech to textapplicationshere's a list of probably six that aresitting in production todayum some of my favorites here probablyare what we call age and languagequality so if you think of this whatthis is at its core is basicallyevaluating the conversation that theagent's having with the customer andtrying to evaluate was that agent clearenough and how they're speaking to thecustomer do they stutter are theyspeaking too fast are they speaking tooslow are theysort of just is the customer constantlylooking for clarity from that agent andto be honest there's probably no way todo that at scale in a in a sort of OldWorld perspective so now we're able todo that at scale and again on 100 ofthose calls and sort of evaluate thoseagents and performance manage thoseagents based on Clarityum another amazing one too which waspricing representation so you couldthink of it as a day-to-day Telcocustomer you're kind of calling in I'mdoing an adjustment on my bill maybe I'madding a new service for moving a newservice what we're doing here isensuring that that agent is actuallysort of clearly explaining to thecustomer what those changes and impacton the bill is going to be and if theydidn't do that we can then sort of sortof fix that with a few different ways ofof attacking the problem some of itcould be again going back to the agentcoaching to make sure they're alwaysexplaining those issues it could befurther documentation or furtherexplanations that we actually send outto those customers so there's a fewdifferent ways that we could tackle itbut this was a huge use case for us andthen the last one that I'll sort oftouch on a bit is sales pitch qualityum I'll go into that right now and andthis is probably our Premier speechproduct that we've put togetherum there's a I'll walk you through theflow of how it works so basically ifyou're a customer calling in I want tobuy internetone of our sales inbound reps sort ofpicks up the phoneum talks to the customer and then sortof closes on that sale so we startedtransacted we finished the sale of thatinternet product the key thing that wesort of push for is any company wouldpush for is cross-selling and upsellingon top of that product so I've sold myinternet service now I'd love to go inand actually sell them some Wirelessservices so hey let's go in make surethe agents are pitching correctly andmake sure that they're pitching thoseWireless services soone way that we looked at this was youknow are they even making that pitch somore times than not the agent's making apitch to the customer and we're they'retrying to close that sale but then theadded value on top of that ended upbeing pitch Effectiveness for us pitchEffectiveness kind of boils down tothree areas those three areas areexclusivity so language around like heythis is a one-time deal this is a dealyou know specific for you as anindividual and then also mentioninghighlights around high speed or networkquality basically how good our networkis and how good it'll sort of run yourdifferent services and then explainingthe planned features and in full form sofor us those are the three key elementsthat we saw as sort of a premium pitchand had a kind of better sell as a as anagent on the other sideum within the first 30 days of launchingthis sort of pitch Effectiveness modelwe sort of saw a 24 increase inadherence which is massive I think partof that ends up being like here's arecipe for you to sell more and for youto make more money as an agent so likeif you put yourself in their shoes howdo I make more money and these are thethings that we need you to do to makemore money and once they see that it'sactually working it's something they'lleasily follow the next piece is thatsort of then translate you're not goingto close on all of those but what it didtranslate to is a 15 increase incross-sellum which is massive so if you think ofevery opportunity you're getting inyou're kind of closing now 15 more ofthe timeso like for any business out thereregardless of the cost of your productsif you can sort of lift by 15 within 30days it's a massive win so this is oneof our our biggest products from aspeech AI perspective that kind ofallowed us to continue doing a lot ofthe work you do so if you think of ifyou're in my shoes on the other side andhow do I sort of continue to get mycompany to invest in me to do stuff yougot to Anchor yourself on value-addproducts so this is probably our biggestvalue-add product that let us exploreinto all the different initiativesumso next steps for us so one of thethings we have at Bell is we sort ofhave this B2B offering where we workwith large Enterprise customers theselarge entry price customers who provideall different services to so one of thethings that we've learned through thiswhole process is how to effectivelymanage one in ml platform in Cloud tothis end-to-end speech to text pipelinein Cloud so what we're going to do nowis try to we've brought this to Marketand we're going to start offering it tosome of our Enterprise customers as aservicethe next piece is then applying all thedata we have against vertex gen AI usecasesumif you think about that again too thekey thing to these gen AI use cases isyou need data to to sort of providevalue out of the great thing that thespeech to tech services have done it'screated sort of massive amounts of ofdata so we basically have tens ofmillions of calls transcribed sitting instructured BQ databases that we can nowleverage and throw against differentGenie gen AI use cases so and for thosethat are really close to it all it is isessentially a API call through a managednotebook or even through a direct SQLstatement so it's it's allowed kind ofour developer team to go crazy hereum and then finally the which Harriswill walk through next is kind ofcollaborating with Google on some ofthese speech fine-tuning initiativesthere's obviously little pieces oflanguage throughout your conversationsthat get transcribed incorrectly andwhat we've seen is the better thetranscription the better the modelperformance so even though thetranscriptions come through at a veryhigh level right now the better that wecan make them the better the modelsperform and then the greater the outputat the end of the day so for us we'revery motivated to work with these guysto kind of help improve thatum so with that I'll kind of I'll leaveit off to Harris to kind of explain toyou a little bit about what they'redoing on the speech fine tuning sidethank you Jeffso speedfight tuning as Calum mentionedumwe do a couple of different things onspeech to text we try to offer the bestout-of-the-box quality for the majorityof the use case that we have for ourcustomershowever we are seeing that whenever wesee the use cases accuracy is not asingle metric that is the same foreveryone so we try to come closer to toas close to our customers as we canso how do we do that we do it withpre-trained models Straight Out of theBox you call it through the API eitherversion one or version 2.when you do it through chairmultilingual model very big onenon-streaming but you get what theout-of-the-box quality and therobustness of a very very large speechmodelwe do it through biasing when forexample you have couple of productspecific names to your company that youwant this speech model to recognizecorrectlybut the final step is to try to toum to condition the models around boththe acoustic conditions that it listensin your use case and also the Lexiconlet's see how we do thatso here's an example of a real exampleactually from a call center at Bell soyou can see that this is a conversationbetween an agent and a customer hey Ilive in Montreal and I would like topurchase a belt 5 internetnow pay attention uh here because youcan see that Montreal unfortunatelybecause of the acoustic conditionsbecause this thing is is gettingstreamed through telephony you can seethat because of the sampling rate waslower you can see how it wasmisanscribed here and at the same timeyou have a product specific name to Bellwhich is called fivebecause the statistical representationin our training modelsfive has a much better representedrepresentation there you can see how itwas misanscribedif you multiply this thing by the loadthat Bell for example has which is halfa million minutes per day you can seethat only two words out of eightbasically you throw out your um your umyour uh your application your Downstreamapplications whether that be analyticsagent I think agencies or anything elseso how we do it how do we do that againas I mentioned previously there is amechanism for biasing right in essenceyou tune the the language model you turnthe the weight of the of the languagemodel to kind of like condition it tokeywords but that is not enoughso how we do How We Do Itwe offer ourof the bookof the of the self-ready models to befine-tunedspecifically we attach at the end sizeof the model adapters and we performadapter efficient fine tuningyou you as a customer come and in yourtenant project that you have in your gcpbasically you have a budget with atleast 100 hours of audio data and weperform the training for youspecifically that happens through theconsole through the UI or through theAPI you point the model the base modelwhich means the language of your choiceand the local and the GCS packet thathas all the audio data and then weperform the trainingthat specifically means that you as acustomerend up with only one model and askGoogle we don't have any access to thatyour weights are your weights your IPwhich is reflecting your data set isonly your IPwith training happens in our TPU V2 andV3 and it takes about a day maybe maybetwo in accordance to how many uh all thehours you grant the model access toconsume and last but not least this isall available through the V2 APInow what happens as soon as you havetrained the modelwith a two-step process you canautomatically Benchmark and you canautomatically deploybenchmarking how it works basically onanother GCS packet or in the same oneyou have a separate directory which youhave audio and text we suggest abouthaving 10 hours worth of audio with acorresponding ground truth so we do thebenchmarking for you and then when withthe simple click into our Google Cloudconsole you can deploy the model intowhichever region you wanthow how that thing though translates tothe end at the end in product inproductionon the vertical axis you can see howmany hours of audio data we have um wehave provided our model into ourinternal testing here and you can seewith only 100 hours of audio you can seethat we get about 15 a relative worderror Improvement if you crank that to400 hours you can see that we're gettinggains of 30 to 35 percent relative Worlderror Improvement and mind you it's notacross the board it's specific to youruse case your product names and exactlythe audio conditions that we seein your use case that is massive for ourcustomers again tying back to the storythat column said coming as close as wecan to our Enterprise customerswe'll be around afterwards thank you somuch and uh yeah don't forget to rate itif you can thanks[Music]"
}