{
    "title": "Jumpstart your AI/ML program with Vertex AI",
    "presentation_type": "Startup Lounge Session",
    "categories": [
        "AI and ML",
        "SU115"
    ],
    "video_id": "CfDNH5325YM",
    "time": "Aug 29 03:45 PM - 04:15 PM CDT",
    "transcript": "foreign[Music]so um if you're here at next uh on thisstartup AIML track you've probablynoticed things are a little bitdifferent in the AIML world than thelast 12 months just a few things havechangedum and so we're really at a pretty biginflection point for AI here rightum what's happened here is that we'renow interacting with these massiveFoundation models using a completelydifferent Paradigm than we'vetraditionally interacted with AI and mlso these are multimodal models that wecan interact with through the Simplicityof natural language we can buildincredibly powerful consumer experiencesso many of you probably started out yourawareness of these Foundation models byusing consumer chat Bots or stablediffusion or some of those otherproducts and very quickly those consumerSolutions have jumped into theEnterprise and business space so we'reseeing really rapid adoption bycompanies up and down the scale fromEnterprises to seed stage startupswanting to transform their businesswanting to build products and wanting toprovide generative AI experiences totheir usersso this is a pretty big change fromwhere we were at from an AIMLperspective in years past so if you'vebeen in the AIML space for a while youprobably know that the early days ofAIML required a lot of really uh expertresources to build uh tune train andmaintain models so you had models thatwere built from scratchthey had to be trained on enormous datasets and training and maintaining thosemodels was no small job you had verylarge teams that were responsible forthis which essentially meant that mostsmall companies were kind of locked outof using these tools you had to havelarge expert expensive resources andteams in order to implement theseSolutionsand furthermore you didn't know exactlywhat you were going to get so you mightspend a long time building a model youmight spend a long time training a modeland then all of a sudden you're testingthat model and you're getting resultsthat are maybe not quite aligned withyour expectationsso models were powerful but a bitdifficult to control and certainly veryresource intensive to manage and deploybut the AI and mL of today is foreveryone so today we can offer youpre-trained Foundation models and taskspecific models that can be tuned anddeployed with no code so any developercan can find these models and deploythem we have model options for everytask so regardless if you're doingtranslation or you want natural languagegeneration or image generation we have awide variety wide variety of modelchoices for youin addition we have robust tooling andinfrastructure support to make thatmaintenance incredibly easy for you sowhat this all means is that anydeveloper any company whatever stageyou're at can get started today withvery minimal expertise or background inAi and mland so the solution that Google cloud isoffering specifically in this space toempower developers to get started withAIML quickly is vertex model Gardenso just to provide a little bit ofcontext if you're newer to Google cloudor newer to Google Cloud AI vertex is aplatform has actually been in market fora number of years and initially vertexwas built to provide an easy toolingsolution for developers looking to buildand maintain their own models and soessentially we had an infrastructure setup so that you could bring your ownmodels you could tune them you couldtrain them and you could maintain themand deploy them to an endpointmodel Garden extends all that capabilityby offering open source and first partymodels directly in an easy to findinterface which can then be deployed onvertex so you can continue to bring yourown model but you can also find a numberof models in model Gardenand so really what this experience is isa One-Stop Discovery shop so we're goingto see in the demo in just a minuteum it's easy to search easy to find andeasy to filter for the capabilities thatyou want based on the modality of themodel the topics that you're interestedin or the capabilities of the modelitselfyou can use Foundation models directlyas a frozen pre-trained base model oryou can tune those Foundation modelsusing a one-click stop from vertexand for the first party models youactually have direct API access to taskspecific Solutions like Vision speechdoc Ai and other modalitiesso what we're seeing in the foundationand task specific models right now thatcustomers are using is that even thoughthere is a trend towards multi-modalitycustomers are still interested in Choiceacross modalities so that means thatthere's very few customers that arepicking one model for all of theirlanguage needs or one model for all oftheir image needs instead selectionvariety and choice are key criteria fordevelopers looking to deploy theseFoundation models and so you can seehere these are some of the foundationmodels that we offer today acrossmodalities and this list is is growingevery dayso hopefully that gave you a little bitof basisaround what you can expect in modelgarden and vertex but you might bewondering how you can actually getstarted with these things we're going toactually go through a demo in just amoment but I want to give you some tipsbefore we even think about how to getinto that environmentso before you start on your journey ofdeploying a model you're going to wantto Define your use case clearly and thereason that this is important is becauseas you select your model you're going towant to have those criteria in mind sowhat are you building who are youbuilding for and why are you building itthis will help your model selectionprocess go a lot easiernumber twostart testing with a pre-trained modeloften customers think that they need totune a model right away because theyhave a very specific application or usecase in mind well we definitelyencourage tuning and we offer a numberof different ways to tune we alwaysrecommend that you start with the basemodel for one it's easy you can startdoing it right away and number two itgives you a sense of where thefunctionality or or output gaps might beso once you get those results from thebase model for example you might get asense of what types of test data thatyou want to use for your adapter tuningor for whatever tuning that you want toimplement so starting with that Frozenbase model always a great stepthree obviously you're going to want toassess the results and the data that youneed for additional training so the goodnews about using that base model tostart with is you're going to get abunch of data from it you're going toget a sense of the performance of thatmodel ideally you're testing multiplemodels if you want to kind of get abetter sense of the ecosystem that'savailable to you and you're going towant to compare and decide do you needadditional tuning is this the rightmodel for your use case do you need todo some additional exploringand then finally once you're kind ofnarrowing in on the model and thetraining that you want to do you'regoing to want to train and test usingvertex model evaluation and other toolsavailable in the Google Cloud ecosystemso again thinking back to that previousslide about the old days of AIML youknow testing and benchmarking was reallyreally difficult because it was oftendone ad hoc by developers writing codefor the specific testing and tuningneeds with vertex these days we have anice toolkit that requires or thatallows you to get started really reallyquickly without writing any code at allall rightand with that I'm actually just going tohop into a demo hereand give me one second I just want tomake sure this is uhclicking through here all rightbeautiful okaysuccess on the first click okay so whatyou're seeing right now is model Gardenso if you have a Google Cloud accountyou can find this today this is GA so umyou can see there on the left we have amenu that allows you to filter bymodality by task by a few different uhcriteria so I would definitely recommendthat if you're just getting into modelGarden for the first time you mightstart exploring there based off of youruse case and just sort of poke aroundbut let's kind of Imagine a use casehere so let's say I'm a developer at abiotech or a healthcare startup and Iwant a model that's specifically beenfine-tuned on medical dataso I'm going to go in search for medicalI found a pretty promising result herePalmyra Med and right now what you'reseeing here is the model card and so themodel card every model in model Gardenhas a model card and it's going to giveyou a bunch of core information aboutthe model's performance about itsattributes about other kind of importantinformation that you need to know so Ican see here that this model excels atsummarizing medical information which isterrific I want to summarize medicalreports so this model is looking prettypromising to me already and the greatthing about model Garden as you'reseeing here on the screen is that mostmodels come embedded with a collabnotebook so that you can actually testthe model directly from that model cardand so what you're seeing right now iscollab so you probably might be familiarwith this environment this is ournotebook environmentum and so all of this is set up you'llnotice that this all came pre-packagedall the metadata was there I'm justrunning this directly from that modelcard page and so basically what I'mdoing here is I'm providing a little bitof metadata about my project about myaccount about the buckets that I want tosend the data toum otherwise everything's written hereI'm not writing any code and so I'm justgoing to run a few slides or a few cellsafter I provide this metadata hereand again this is something that youguys can do today in model Garden rightawayand so once I provide that metadata I'mgoing to initialize the vertex APII'm going to define the docker containera few other common functionsandthis is all being done just throughQuicks again I didn't write any of thiscode and then finally I'm going toupload and deploy my model and so whatyou're seeing right here apologies forthe small font is some configurationsaround the Hardwares and hardware andaccelerators so again you can choosethat you know choose whatever is rightfor you based off of your location basedoff of your cost structure and thenultimately I'm gonnadeploy that model soum you saw that instantaneously that wasthe magic of time lapse that actuallytakes about 20 minutes so if it takesyou 20 minutes don't be surprised but wenow have a deployed modeland so what you're seeing right here isactually there's a few pre-writtenpromptsum and so you can go in there and eitheredit those prompts run them directlywrite your own prompts uh adjust theparameters so for this particular modelwe have two parameters one of them isMax new token the other is temperaturethose are just settings to determine thelength of the output as well as thestatistical variance so temperature HotTopic in LMS right now but you canadjust that as you need toand so I've decided to deploy my modelI'm back now in model registry you cansee that I've found Palmyra Med rightthereand so let's say I'm ready to put thisinto production I'm going to go andclick deploy and deploy and test andthen I'm going to deploy to an endpointand so again I want to I want to mentionthat I'm creating an API from that modelagain having not written any code thisis all purely from The Notebook and fromthe UI and so once I'm in this deploy toendpoint tab there's just a few simplesteps I have to step through here sonumber one I'm going to name my endpointI'm also going to select the location ofthe hardware where I want to deploy itand adjust a few access settings if youwould prefer different ones you caneither manage your own keys or haveGoogle manage your keys for you throughthe standard settingsand so once I've sort of decided onthose settings and that I'm happy withmy results I can continue onand so this page is just going to let mefigure out how to distribute my computeresources so this is really important interms of scaling my model appropriatelyagain these are things that you canmodify after you've deployed your modelas well but to get started you'll justwant to make a few decisions about howyou're going to distribute traffic whichmachines you want the number of computenodes all that kind of stuff and againthe good thing about vertex is that youget a pretty robust set of documentationto kind of help you make those decisionsif you're feeling overwhelmed with thenumber of boxes to click or the numberof fields to fill in the good news isthere's quite a bit of helpful tips hereso once I've done all that I'm justgonnamodify my traffic split here continue togo onand then finally I'm going to implementmodel monitoring so model monitoring isoptional you don't have to implement itbut obviously we strongly recommend thatyou use model monitoring modelmonitoring will make sure that yourresponses are not drifting away fromyour desired output so again this isn'tsomething you have to build yourselfit's something that Google Cloud offersas a solution you just need to opt intoitall right and then finally uh gettingtowards the last few pages here there'sa couple more pages that I can select onso um in terms of the monitoringobjective there's a few differentcriteria that you can use eithertraining serving SKU detection orprediction drift detectionum since I didn't tune this model myselfI'm going to use prediction driftdetection but you can always usetraining serving SKU detection it'squite a mouthful of words if you do finetune your modelum and then finally just a few moreSelections in terms of the criteria thatI want to make for monitoring and otheralert thresholds that I can optionallyuse at this stageand I'm ready to deployall rightbeautiful so there's my end point rightthere in the model registryand that step also typically takes about20 to 30 minutes again through the magicof time WAP so I'm just going to hopover and show you what the API actuallylooks like once I've set it up Googlecloud makes it super easy for you to getstarted as a developer here we're givingyou an SDK as well as some uh somepython samples if you want to startrunning that in your Cloud console oryour CLI of choiceall right I do want to mention one otherthing though if you do want to tune yourmodel there is a training uh a trainingtab in Google Cloud that allows you todo this soright here you're seeing a couple uhexamples of data sets or annotation setsthat you could use to do that trainingas well as the objectives that you canset so again we've chosen a medicalmodel here so these are probably not theright training data sets or annotationsets but it gives you a sort of sense ofthe the options you haveone option that you can consider ifyou're looking for open source data setsis using a solution like Kegel so forexamplethis is the view in Kegel right now ofjust Healthcare data sets ranked byusability and you can see they havequite a bit all of these are opensourced and scoredso I go into the disease symptomprediction tab I can explore this data alittle bit they have sort of the modelcard equivalent on their data sets aswell as upvoting and a few othercriteria and I can see here on the rightthat it has a usability score of 9.7 outof 10 which is amazingso just to tie back to model Garden herewhat I want to emphasize is that thatexperience can be had on any of themodels that you can find in modelGardens so that isn't just unique tothis particular model or this particularfunctionality that ease of deployment isavailable on any model that you're goingto find in model Gardenall rightmake sure thisis beautiful okay and with that I'mgoing to invite May to come up and tellyou a little bit about how Ryder isinnovating in the space of generative AIthank youhi everyone I am so excited to tell youum about writer and how we're workingwith Google and vertex and there's asmall announcement at the end that we'revery excited about but first I want tounderstand the audience a little bitumwho works at a startup smaller than 200peopleokay smaller than 100 peopleokay who is generative AI native I hatethat termokay who's building a generative AIapplication or feature into theirproductokay cool what are you guys doingI'm joking but um what really what isthe interest level what brings you tothe session todayfunny that no one on this side raisedtheir handokay awesome awesome wonderfulum well so excited and hopefully um wewill answer and it fulfilled both sidesof the roomum we've got five models in vertexGarden as ofum uh vertex AI model Garden as of acouple of weeks ago including uh PalmyraMedum which we uh have uh have open sourcedactuallyso all five models are free and opensource and a lot of times when we talkto a customer who had been using one ofour open source modelsum that was on a platform not vertex ourbest models are on vertexum folks are like sothis actually works really well what isthe commercial offering and why would Ipay you and I've had literally like ciosof telcos ask us this question and youknow for both sides of the room I thinkumonce you really dig deep into just howhard it is to go from a POC or somethingthat works some of the time most of thetime to real production scale and it hasto work all the time and be really goodum that is where so much of the effortlies in so much of the complexity and sothe the commercial part of writer isreally everything we've built on top ofthe models and we're going to talk aboutthe models and how we built them as welltoo for you know the folks interested inin scope and what it takes but what wehave put together as our Enterpriseoffering what the you know logos on theprevious slide we're we're doing is whatwe think are really the the threeessential parts of building a generativeAI application that's ready for uhproductionum yes you've got the modelbut paired with that for us is oopsum AI guard rails that include a ton ofpost-processing and some of thosepost-processing services do useum uh llms as well but a lot of them areour NLPum and we augment most of our use caseswith a Knowledge Graph not a vectordatabase which we we should talk aboutas well um where our learnings came fromfor for that and the result is a productthat allows folks to very quickly spinup multiple generative AI use casesso uh we have been doing this now goingonum two and a half years so um myco-founder and I were in the translationmachine translation and localizationbusiness before so we've been workingwith Bert and um uh Transformers for foryears and really seeing the thegenerative capabilities is why westarted writer in early 2020 so the thegood news for us isum in this incrediblyum hot AI market we've been able to talkto people about use cases that have beenin production for for a long time andwe're going to walk through some ofthose but the the story here really isin um you know what starts at the veryFoundation which is the the the llmum and for us the way that ourarchitecture works is uh the the modeland the knowledge graph can actually behosted inside of somebody's privateCloudum this is part of our announcement alittle bit later we are excited topartner with with Google on making thathappen within gcp and then the theplatform above is really customizableummost of what generative AI is good at wecan characterize and kind of classifyacross three types of applicationscreate analyze governand so the the use cases and whether youareprompt engineering plus fine tuning orsome prompt engineering but then you'rereally building that indexwhether you are doing a bit of promptengineering but then like you know it'smultiple layers of fine tuning so muchof it comes down to what the use case isand what kind of capabilities are youdrawing onumfor useverything that we are able to do todayand the Enterprise use cases that I'llwalk through in a bitreally are a result of some of thefoundational no pun intended decisionsthat we've made for for the company uhthose folks ask us all the time uh looklayers you know one through five reallylook like they could sit on any modelwhy did you build your own and yeah abig part of that is being able tocontrol how all those layers worktogether and be able to over time reallyinvest in the increasing capabilitiesthat can still be hosted by a customerin their own cloudcan still befine-tuned in ways that aren't you knowoverly complex or require too much dataways that they can be inspectable aswell so so we've got customers who doask toaudit what is in our training data we'vegot really strictumfrom a data perspectivepolicies as it relates to what goes intothe foundation models training what goesinto Knowledge Graph what goes into theuse case fine-tuning and promptengineering and where those live and howthey get shared between customers tldrthey don't get shared between customersat all at any of those levels but peoplereally are interested in these decisionsnow and for those of you who arebuilding generative AI into yourapplications the the depth andsophistication of the securityquestionnaires we get now the level ofinspection rights people want to have onknowing what is in those models evenrequests to indemnify we have nowactually indemnified customers in bothfinancial services and Healthcare youknow as a result of just being soconfident around IP and copyright risksso none of that would have been possibleif we didn't develop our own models ourwhole business wouldn't have beenwouldn't have been possiblesofrom a deployment options perspectivethat is obviously another big reason toown your own stock and and notbe too dependent on uh other models ifthe applications really are needing tolive in somebody's environment as aresult ofsecurity or privacy Etc that's becomereally important and that played a bigrole in our pursuing uh the therelationship with Google and we'vealways been a gcp shop from uh I meanliterally like the first dollar ofcredits I think we might even still getcredits I don't know if that's allowedbut uh you know for us uh you knowpartnering with gcp and vertex was a wasit was a no-brainer and Google ingeneral so you know that had a big rolein umkind of the the way that we have gone toMarketthe the knowledge graph uh this is uhproprietary this can live in somebody'sprivate Cloud so we've got folks whohost this within their GPC environmentsuh and it what we're what we're doingessentially isum using the modelto create the metadata for everythingthat's in the knowledge graph And ThenThere are a set of compressionalgorithms that decide what we're goingto compress and what we're not going tocompress what we're going to feed intothe the the model raw and the result isa much higher level of accuracy and muchlower cost over time actually for thekinds of use cases where you really havea high refresh rate for the knowledgethat needs to be in that environment andyou know the the reason we think fullstock generative AI makes a lot of senseis most folks don't want to berebuilding uh this infrastructure foryou know each of the one of dozens ofdigital assistants that folks will wantto build you know on any given on anygiven team so this has been a really bigpart of how we've expanded the footprintof the product from our original createuse cases to our analyze and and governuse casesand lastly you know this is the thirdleg of of the product around AI guardrails and and brand governance and youknow folks end up realizing as they'retrying to take uh applications toproduction thatum you know an end user has really highexpectations for uh what comes out ofour AI apps and you want to say but likelook you're 60 faster than you werebefore and like suddenly folks want tobe 100 faster or 200 faster right theywant the solutionum uh uh completed fully and we haveseen so much of umsort of that last mile really be aboutum the the post-processing that needs tohappen to make sure that um uh that theanswer or the generation is exactly whatthe the you know impresses the user orthat the user would do manuallyso um I thought we could spend timegoing through use cases most of thesefolks are folks that we we share withwith gcpum and not everybody is you knowself-hosting all parts of parts of ourstockum but you know these are multimulti-vendorum engagement so I was with somebody onthe gcp team earlier with a shared Telcocustomer andum the gcp friend asked the Telco frienduh wait a minute I thought you guys wereusing us for you know XYZ uh support usecase and the reality is that like in theEnterprise there are so many use casesand the the the the the modelsum strengths and this is what's so niceabout vertex Ai and just the thehopefully the depth of the models thatare going to be thereum what you're using them for umuh is is is reallyum knowing that use case is a big partof uh it plays a big role in kind ofchoosing the right the right model soyou know with that let's uh talk aboutthe the first one exam works is thecompany that you know big Healthcareum payers will Outsource you know acomplicated uh Health claim to sosomebody's got like a 200 Grand surgerythat they need to uh um make sure theclaim is you know uh being supported theright way what we're able to dois number one and you can see the colorcoding uh on the use cases this reallyhelps us even understand and sort ofstart having heuristics for all right umyou are doing a create use case so we'vegot a really wide context windowdepending on the length and thecomplexity we should be able to doeverything in in the promptum a lot of times that's not the caseespecially in claims management whereyou end up actually fine-tuning by uharea so disability separately from inclaims management separately fromaccidents separately from Etc so in thiscase what we were able to do on thecreation side is actually create youknow an automatic summary and thesummary is and this is where KnowledgeGraph comes in really handy what you'rereally doing is taking uh perhapssomething not the patient will havefilled out uh you're going in and you'relooking atum uh their policy you're comparing ituh to anything that might have changedgiven their condition since in thissummaryum uh feeds into a decision-makingprocess where a doctor formally doingall of this manuallyis analyzing the material and actuallycoming up with with a decision so we arecreating the summary that speeds up thedecision then we are helping to come upwith the decision and then there is a QAprocess that they had been doingmanually with uh with nurses that thenactually reviewed sort of the shorthandof doctors and made sure that it waswritten out in in ways that compliedwith uh the theuh the company so uh you know these areall things that we are doing off ofPalmyra Med and actually when we metthis customer the POC was being done offof our open source Palmyra the 20billion model and so they had alreadybeen experimenting this wasn't withvertex AI but it was really in beingable to bring kind of the knowledgegraph and the post-processing togetherand then the techniques around beingable to kind of distill the problem downinto uh different use cases uh thatreally brought the whole solutiontogether now of course we gave them a UIEtc we've got kind of this composableapplication layer on top uh but you knowthat really wasn't what sealed it forthem it was really being able to get tosolving the problem that they thoughtgenerative AI could could solveat CVS this one's a bit morestraightforward um they've you know fora long time had this ambition to help usreally get more Wellness information aswe get prescriptions filled and sothey're able to take information thatthey've got on somebody's conditionsomebody's age and really marry it withwhat it is that's being prescribed for amuch more fulsomedescription of what to do with yourmedication than we normally get withkind of our our shorthand Pharmacy orpharmacist informationand um uh in that previous example Idon't know how to go backwards oh herewe gouh in in this exampleuh the training data didn't really existbecause they had never actually donesomething like this before and so thiswas a um aexample of a use case where you actuallyuse the model to generate some of thefirst training data and you give it tosomebody for annotation right and thatactually becomes what we used for uh forfor fine tunings this was this wasinteresting and you know everything fromthe the Bolding toumhow you structure the bullets toPunctuation is actually happening in inthe AI guard rails part inpost-processing because you need a lotmore data formatted the exact same wayfor an llm to be you know following yourformatting or structure if you're tryingto do everything within the promptin financial services there is a lot ofreally interesting use cases now we'vegot a thank you a financial servicesum uh model that is also coming tovertex and uh we've got some of ourearly Financial Services customers inwealth management and banking um who whohave started using the model so you knowwhat's nice aboutum really having a composable stock ingenerative AI is you can upgrade themodels without needing to rebuild all ofthe use cases and what we're seeing isas folks get more sophisticated theseare some of the considerations which iswhy I think a lot of what you guys areare building at Google is so importantbecause we're still in that phase oftrying to you know buildand use the biggest models possible butyou know over time folks are realizingyou know more focused smaller is goingto be better but you don't want to loseeverything that you have built thatyou've built on top and Vanguard aregreat examples of thatum this is an example of a chatapplication kind of a what we callscaled q aum that they have built within withinwriter I'm going to skip across theretail example and talk about what'snext for for us and Googleum it's been a really awesome kind of uhJourney from the Google startup programto where we are today with with Googleand so many of our joint customers fromL'Oreal to Verizon are very excited thatthey're going to be able to use writerfrom within from within Marketplace so Ican't speak more highly of the theGoogle folks and their appreciation forthe startup journey in all of its phasesuh highly encourage everybody to scanthis code and get all of these creditsum You didn't hear it here but you mayactually get more credits than are onthis slide if you ask real nicelybut thank you so much Caitlyn it's uhit's great working with you alland uh we'll be out front in the loungeif you have any questions but thank youguys so much it was great talking to youthank you"
}