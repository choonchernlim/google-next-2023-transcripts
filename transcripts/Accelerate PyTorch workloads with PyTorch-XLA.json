{
    "title": "Accelerate PyTorch workloads with PyTorch/XLA",
    "presentation_type": "Breakout",
    "categories": [
        "Architects, IT Professionals",
        "ARC301"
    ],
    "video_id": "PSpmRtWuMs8",
    "time": "Aug 31 12:15 PM - 01:00 PM CDT",
    "transcript": "foreign[Music]I'm an engineering manager at Googleleading Frameworks Solutions andecosystem at Cloud ml compute servicestoday with my amazing colleagues AlexDamion and Carlos who have put togetheran exciting session to walk you throughhow to accelerate pytorch workloadsusing pintouch xlathis week you have heard a lot about thelatest accelerators that are availableon gcpin particular TPU v5e is our latestseries of tpuswhich provides efficient versatile andscalable Solutionsfor ML workloadsregardless of your ml framework ofchoice Cloud TPU can help you achievethe great performance for training andinference of your modelsso whether you're using pytorch jacks ortensorflow you have the easiest apisavailable to you to train your modelstoday we want to show you how we can usethese TPU acceleratorsto accelerate your pytorch workloadsusing a key technology that enables highperformance and scaleusing a distributed compiler in xl8pytorch is able to utilize open xla as abackend which brings incredibleperformance and scalability to usersthe pytosh xla project has three maingoals the first one we want to supportpie Crush community of developers withwell-testedcode examples and optimizations ofmodelsto enable them achieve moresecond the gcp accelerator customers canproductize their pytorch workloads fortraining and inference on tpus providinggreat interoperability with gpusfinally ensuring that the pi torchecosystem is well integratedand supported across the whole TPUproductwith pytorch 2.0 released back in Marchthis year pintorch introduced torchDynamo torch Dynamo rewrites pythonbytecode in order to extract sequencesof pytorch operations into anintermediate graphwhich is then just in time compiledusing a customizable backendone of these backends is open xla usingthe open xla backend pytush operationsare converted into an establish cellographstatement is a portability layer betweendifferent ml Frameworks and ml compilerswhich provides backwards compatibilityof its offsetafter this conversion the xla compilercan optimize the execution at a scaleand then use the same runtime thatpowers Jax and tensorflow to run yourpinecrush modelin other words using pytorch xla we haveunified the stack that powers all majorFrameworks and any optimization andImprovement that is done in the xlalayer can be used for any ml frameworkuserforeignprojectstarted back in 2018with close collaboration of course withmetathe project was announced at Defcon 2019jointly with meta and Salesforcein 2020 pytorch xla became generallyavailable with support for cloud TPU V3pods as well as supporting uhnitrous lightningor the next year the team really workedhard to make sure that we improveusability by providing TPU VM supportand migrating to this to use the sameruntime as Jaxin 2022 we started working on PorscheDynamoand this brings us to todaywe are delighted to release pytorch 2.1with integration of openxlawith this release pytorch users can usepie crust 2.1 to run large languagemodels such as lama2at very large scale on TPU v5eand we are further investing on pushingthe boundary of a scalability with pitorchlooking ahead we have three maindirections first interoperabilitytoday xla Powers more than tpusin fact using Piper xla you can runpytosh models on many different computeenvironments including gpus as well astrainium and inferentiasecond performance is the key we want toprovide the most performant solution forall pytorch models and all pytorch usersfinally we have been working closelywith our friends at meta to support thelatest apis for torch exportto enable hydroch users to run theirmodels at Edgefor example this enables pytorch modelsto take advantage of the incredibleperformance optimizations on AndroidphonesForge export is available in preview inpytos 2.1and I'm excited for our closecollaboration with pytorch and pytorchfoundation and with that I would like toinvite Damian serenito join me on the stage[Applause]thank youthank you so much for uh welcoming mehere todayum so my name is Damian Serenity I'm anengineering director at meta and Isupport the pytos team within meta and Ijust wanted to come up here and you knowspend a couple of minutes just saying afew words about collaboration that we'vehad we have friends with Google andacross the industry onxla so as Shaheen mentioned right thisis not a new story we've been workingtogether since 2018 since the beginningsof by George xla and that collaborationreally has grown over timeum of course since since 2018 a lot ofthings have changed right so one of thethings is pytorch has grown reallysubstantially as a framework back in2018 I think it was still a bit of anupstart and now it's really grown into[Music]uma leading ml framework acrossyou know everything from researchstartupsum you know large Enterprises and ofcourse companies like meta that rely onpaytorch for uh for production andthroughout all that we've kind of reallyhad a philosophy that pytorch is reallyan at hard and open source project it'san open ecosystem and so that's why wevalue these collaborations and kind ofbuilding out in the openso one of the exciting things thathappened last year is we decided toactually formalize that and create apytos foundation which we co-foundedwith Google and a number of othercompanies and now has nine board membersacross the entire industry for you knowworking together to further theecosystemand you know as um as Shaheen mentionedwe all the other thing that excitingthing that happened last year wasreleasing python 2.0 which we announcedat the conference in December andreleased in March and really even theway I think about pytos 2.0 is we wantto bring the same user experience thatpeople love about pytology and that'sbeen enabling a lot of like iterationand research and AI which is kind ofthisveryfast-moving ego mode developmentexperienceand marrying that with the power of allthe exciting back ends that we've beenbuilding along the way and includingopenxla so it's really natural that thatcollaboration has you know turned intous working together on open xla and nowwe're really excited to seeum what that's going to bring to thePietro's ecosystem and the value that'sgoing to bring to our usersum so of courseall of this is really about performanceright like the 2.0 story is how do wekeep pytorch the python experience andmake it performant and we're reallyreally excited today to see the nextgeneration of cloud tpus be releasedwith pytorch support and not like stealthe Thunder from Alex who's going tocome after me but we're seeing somereally exciting numbers around theperformance ofum play George on TPU v5e and includingour models like our next Generation opensource uh language model number two soyou'll be hearing a lot more about thatthe other side of performance is scalingright how do we scale training up tolarger and larger distributed trainingand I think again you're going to hear alot more about that from Shaheed laterin in this talkum so really in all of this right I thethe message I just want to convey is Ithink we're really excited about allthis is happening in the open and inpartnership with Google and otherindustry Partners we're super excited tosee where openxla leadsand there's a couple of directions thatI think you'll be hearing a lot moreabout one is definitely the large-scaledistributor training and the other istotal export and how we can bringkind of your pages modelsum into ads and mobile devices andthat's going to be a big theme I thinkcoming up for uh for both of usum so with that I just wanted to thankShaheen for having me and also pass iton to Alex to Deep dive into herperformance numbers[Applause]thank you so much Damian I am so excitedto see the collaboration between Googlemeta and our other industry Partners onpytorch fighter Chick-fil-A under theauspices of the pie torch Foundationhi everyone my name is Alex spironov andI'm a product manager for pyrochex La atGooglenow as Damian mentioned and as I heardfrom so many of you during theconference over the last few daysperformance and performance per dollarof your AI workloads is really top ofmind for many of you here and so I havethe honor and the privilege today toshare for the first time our performanceresults for training and inference ofthe Llama 2 model with pytorch xla oncloud tpus and Cloud TPU v5ellama 2 is the next generation of meta'sOpen Source large language modelsnow I'm sure all of you in the audiencehave heard about llama2 but just in caseyou somehow missed all of the excitementover the last couple of months let'squickly cover the fundamentalsllama 2 uses an optimized Transformerarchitecture and it ranges in modelsizes from 7 to 70 billion parametersnow the largest 70 billion parametermodel also uses eight-way modelparallelismand all model sizes use sequence lengthsof up to 4096 tokens for inferencenow that's a lot of numbers but thebottom line is that llama 2 is astate-of-the-art large language modeland it really puts AI accelerators tothe testso how did Cloud TPU do with lava 2training and inference well here are thehighlightson training Cloud TPU achieved up to 52and a half percent model flopsutilizationthat is a key efficiency metric andwe're going to unpack that a lot more inthe next couple of slidesnow of course after you train or finetune llama 2 you will want to runinference on it and so on inferenceCloud tpus achieved up to two and a halftimes greater performance per dollarand up to 1.7 times latency speed upcompared to the previous generationCloud TPU V4now just let those numbers sink in for amoment52 and a half percent model flopsutilizationtwo and a half times higher performanceper dollar1.7 times latency speed upthose are amazing numbersnow how is this possiblewell the magic comes from the combinedhardware and software optimizationsand I'm so impressed by our amazingengineering team who makes this magicpossiblenow let's take a closer look at ournumbers and our resultswhen it comes to training performance weuse the industry standard metric ofmodel flop utilization or mfunow if you've been working with llmsyou've probably heard about mfu but youmight not know exactly what it means solet's unpack that a little bit morewe start with model flops now modelflops is the total amount of floatingPoint operations that are required tocomplete one forward and backward passduring model trainingnow the great part about model flops isthat their hardware and implementationindependent they only depend on theunderlying modelnow model flop utilization measures howeffectively the model uses the actualunderlying Hardware during trainingso in our case on cloud TPU V4 on thelargest llama2 model size 70 billionparametersCloud TPU V4 achieved 52 and a halfpercent model flop utilization52 and a half percent that is veryefficientnow why does it matter to youwell because you want to get the valueof your AI accelerator investmentand Cloud TPU and Partridge xla and lava2 uses your AI compute very efficientlyand that generates tremendous value foryou for training large-scale languagemodelsnow one of my favorite quotes comes fromAlan K one of the early pioneers ofcomputing who famously said that if youcare about software you should buildyour own Hardwareand that's exactly what we did at Googlewith Cloud TPU v5ewe designed and built TPU v5e from theground up to provide high performancecost efficient training and inferencefor large language models and generativeAI workloadsand because of that cloud tpb5v providesa cost efficient scalable and versatileinference platformv5e achieves up to 2.5 times greaterperformance per dollar on inferencev5e is also highly scalable it allowsrunning inference on model sizes with upto 2 trillion parametersand of course Cloud tpv5e providesexcellent integration with leading AIFrameworks including of course pytorchand we also integrate with services likegke for orchestration and vertex AItraining and prediction servicesso let's take a look at our inferenceresults using the latest and greatesttpusnow when we talk about inference we usethe industry standard metric ofthroughputand to calculate throughput first wemeasure a latency per token after themodel has been compiled and loadedand then to calculate throughput wedivide the batch size by the latencynormalized over the number of chipsand so as a result throughput measuresthe actual performance of the model inproduction independent of the number ofchips that are usedand so on inference when serving llama 2for the largest model size Cloud TPU v5eachieved up to 2.5 times betterperformance per dollar which we obtainby dividing throughput over our listpriceup to 2.5 times greater performance perdollar on cloud tpb5v compared to CloudTPU V4and again you might be wondering2.5 times that sounds pretty great butwhy does that matter well it reallyreally matters because from talking tomany of you at the conference the metricthat really matters is how many tokensper second how many examples per secondhow many images per second can I serveon my AI inference platform whilekeeping the total cost as low aspossibleand Cloud TPU v5e was designed todeliver exactly thatnow of course in addition to Performanceper dollar we also care deeply aboutlatencyand latency is really important becausewhen we build AI applications we wantthose applications to be highlyresponsive to our users and to ourcustomersand we're very happy to share that onlatency Cloud TPU v5e achieves up to 1.7times speed up compared to Cloud TPUbeforeso as you can see Cloud tpv5e provides acost-effective scalable and versatileinference platform that deliversexcellent performance and performanceper dollar on the latest and greatestlarge language models and generative AImodels including of course llama2and now please welcome back to the stageShaheen who is going to share with usthe newest Innovations and capabilitiesin the upcoming pyrochexole 2.1 release[Applause]so thank you Alex I'm really excitedabout those performance results forllama 2 on cloud tpus we are reallylooking forward to the community to takeadvantage of all of these capabilitiesthat are now packed into the 2.1 releaseso the release candidate for 2.1 isavailable right now the final release isscheduled for early Octoberthe pikish 2.1 brings a host of featuresand capabilities let me walk you throughsome of themas you heard the support for cloud TPUv5e is one of the most important thingsthat we are providing in cloud inpyotech slate 2.1 releaseon top of that workloads are now capableof using an spmd APIto scale for training and inference sothis is one of the most impactfulfeatures that I think the community isgoing to love and I'll go into moredetails about it laterthis allows for utilization of ourmulti-slice technology as well thatprovides performance even when yourworkload spans multiple different partsthis release also brings you the abilityto export your models and save them instable hlo together with tighterintegration with torch Dynamo pytotechLa users can now convert the graphproduced by pythorg xla by pytorch andsave it as an establishello modelthis allows them to put this into aserving an edge or server much easierand finally as you sawwe have ensured that the state of theArts machine learning models such asLama 2 can really perform great usinghydrochex like 2.1so in order to run inferencewhat you will do is that you will createan xla devicein your code and then use the same torchthat compile apis that you're familiarwith and pass a parameter for open XLEthis essentially allows your pie crutchmodel to use openxla as its backendindeed training a modelis no different all you need to do is toget the xla device and then use the sametorch.compile API with the openx laybackin parameter to train your modelthis will ensure that the model isoptimized and executed using the xlastackyou can see how easy it is to use thisincredible API from pytorch to run yourpytorch models on cloud tpusnow one of the biggest challenges oflarge models are is power utilizationyou want to be able to take advantage ofthousands of interconnected chips totrain a model at scaleoffers several options to achieve thisthere is a limit to the size of themodel that can be effectively servicedon a single chip so to scale a modelbeyond that we distribute the model overmany chips this is called Modelparallelization if you have the samereplica across multiple chips and youShard your data that would be dataparallelismfully sharded data parallel or fsdp isone way to achieve model parallelismusing pi toxla during distributedtraining each devicecan store a specific model Shard and allgather the full model wastes when it istime to perform the forward passwe built microchick's life sap supportdirectly into the hugging face trainerclass so that any model using trainercan Leverage fsdpnested fstp wrapping is both flexibleand simple to use these new featuresmakes it easy to train a wide range ofhiking face model at large scale oncloud tpusnow fstp is amazingbut there's one more thingpython Chick-fil-A introduces spmd APIthe xla compiler transforms a singledevice program into a partitioned onewith proper collectives based on theuser annotations and shardinginformation hints that has been put inthe codethis allows developers to write pietouch programs as if they are on asingle large device without any customsharding computations or CollectiveCommunications or complicated codingsome of the key highlights of spmd arebetter developer experienceeverything happens with a few shardingannotations from the user the code ismuch simpler cleaner easier tounderstanda single API that enables a largevariety of parallelism algorithms youcan do data parallelism you can dostandard uhfstp you can do spatial partitioning andyou can combine many of these togetherto achieve the best performance for yourmodelsecond industry leading performance inlarge model trainingenabling pytorch and Jax developers totake advantage of the same underlyingxla API to scale their modelsthis API also brings the multi-slicetechnology to pytorch in fact for llama2 training on CPU v5eusing a single CPU V5 epod or two podsor eight pods there's only a few changesin edits in your sharding informationfinally pytos 2.1 brings a newexperimental API for torch export withthis API you can export your pytorchmodel into an unstable hlo graph andsimply take it for deployment on serveror on edgeall these amazing features are availablein pytorch 2.1and we are really excited for you guysto try itnow if you remember we said one of ourmain goals from the package x-layproject is to ensure that the ecosystemaround Pi torch and cloud tpus is strongand healthy and what better way todemonstrate thatthan inviting my colleague Carlos fromlightning AI to talk about actorslightning and pytos xli[Applause]thank youthank youand thank you all of all of you forcoming in todaymy name is Carlos mussoli and I'm aresearch engineer at lining AItoday I'm going to talk about how youcan supercharge the process offine-tuning large language models usingtpus on Google Cloud thanks to Liningsopen source ecosystemfirst let's talk about lining lininggives you the best tools to get startedtraining deep learning models usingpythonsline installs offer maximum flexibilitywithout sacrificing performance at scalewe designed our software withresearchers in mind allowing you tofocus on your work instead ofimplementing the same boilerplate overand over againsince it's released lining has gainedtremendous popularity it has beendownloaded almost 70 million times andwe have a strong community of over 22000 projects using lightning with nearly900 total contributorsnow let me show you what lining bringsto the tableon the left you'll find a simple pythontraining script that uses python xla torun on tpus the code is long complexunlimited and what implementswhat it implements and it doesn't evenfit on this life furthermoreit will only work on the TPU environmentrequiring difficult code changes to tryout on your personal computeron the right side you'll find a featurecomplete example that achieves the sametask using python sliding our trainerobject provides total customizationand the trainer process can be extendedwith the lining module a class thatoffers you overridable hooks where youcan add your custom logicwith this you can fully focus on solvingyour task while the trainer takes careof Performing the training process andmodel distributionin the middle you'll find fabric ourmost recent releasefabric offers seamless modeldistribution while still allowing you toDefine how the model is trained itprovides expert control over your Loopwithout worrying about the boilerplateof supporting distributed environmentsnow that you understand the advantagesof using lining I'm thrilled to announcethat lining fully supports the newlyintroduced PPU b5e architecture throughpythons xlato demonstrate this we used legibilityis a hackable and simple implementationof state-of-the-art Open Source largelanguage modelslegit leverages lining fabric forseamless mobile acceleration on tpus andprovides you with single file recipesfor text generation pre-training andfine tuningand here's the best part of all of thisby torch lining legibility and liningfabric are completely open source andaccessible via GitHubnow let's talk about fine tuning llmsfine tuning is an essential process thatinvolves taking a pre-trained model andadapting it to perform a specific taskby training it on task specific dataonce fine-tuned these models can beincredibly valuable toolsfor various applicationswe have validated TPU support byfine-tuning llms we fine-tune bothFalcon 7 billion and Falcon 40 billionmodels using the Dilemma adaptertechniqueon both TPU before and the newest TPUb5v architecturesince the models are too large to fit ona single chip we use the python xla infoolisher data parallel implementationsupported by lining fabricthis fstp strategy shares thetransformative blocks across the chipsin a TPU bottlethe scalability of tpus allowed us toreach close to 50 percent model flowsutilizationwith 7 million parameters and and 40model flop utilizationwith 40 billion parameters on both TPUbefore and the pov5please take a look at the graph showingyou the cost effectiveness of using thenew TPU generation compared to theprevious one with both model sizesthanks to the improved performance perdollaryou can find all the details or of ourbenchmarks or even replace to producethem yourself by following the link onthe rightoffers bespoke implementations of itsmain scriptspecifically designed to run on xlathis might be the largest model trainedon tpus using lining so far and now youcan easily do the same thanks tolightning and Google Cloudthank youforeign"
}