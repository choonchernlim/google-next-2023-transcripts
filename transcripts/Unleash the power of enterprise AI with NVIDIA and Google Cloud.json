{
    "title": "Unleash the power of enterprise AI with NVIDIA and Google Cloud",
    "presentation_type": "Breakout",
    "categories": [
        "AI and ML",
        "AIML106"
    ],
    "video_id": "3PLBNRj7RKo",
    "time": "Aug 30 11:15 AM - 12:00 PM CDT",
    "transcript": "foreign[Music]Salvatore director of acceleratedComputing at Nvidia I want to welcomeyou here to Google Cloud next andwelcome you to the session of ours I'mpresenting with my colleague Anne Hechtwho'll be joining us here on the stagein just a few minutes uh and why don'twe just go ahead and get started uh interms of format what we're going to dois we're going to be presenting forabout the first uh half an hour or soand we've left about 15 minutes at theend for question and answerum when you do have a question pleasecome up to the microphone as this isbeing recorded and streamed and that wayyour question will uh will be part ofthat so with that when we go ahead andget startedso that's us as I mentioned so uh againAnn will be joining here in just a fewminutes she's going to talk to you moreabout the software side of things I'mgoing to cover a bit more of the thehardware and infrastructureso again I'm going to give you sort ofthe platform overview we're also goingto talk quite a bit about Nvidia AIEnterprise which is our software layerof our overall platform you know one ofthe really important things about theNvidia platform is you know the gpus arejust the beginning and we're going totalk to you quite a bit about that andthen we'll sort of give you someresources in in summary at the end to uhto kind of take away from thepresentationwe're very excited that we actually havejust been awarded a partner of the Yearaward for generative AI which is a bigpart of this talk todayum as we all know generative AI has kindof set the world on fire I mean it'sreally the kind of one of the mostexciting things going on with AI it'sthe thing that really has touched uhconsumer markets in a very direct wayeven though a lot of us you know reallyall consumers were using AI in one formor another but whether they knew it ornot but now with things like you knowmid-journey and stable diffusion andimage Generation apps and chat GPTpeople know that they're directlyinteracting with AI now and so that'skind of going to be a lot of what wecover heresoum in terms of you know what isgenerative Ai and what it can be usedfor there's no shortage of applicationsfor generative AI in a lot of ways we'rejust scratching the surface you know Imean we spent kind of the first decadeof deep learning you know deep learningreally got started in Earnest in about2012 with the Advent of alexnet whichthose of you who've been in the spacefor a few years probably remember thisuh there used to be a competition calledimagenet uh and uh and a professor fromthe University of Toronto named Alexkuczynski basically was able to build adeep Learning Network on Nvidia gpus andjust come in and blow away thecompetition and that sort of sparkedwhat we now think of as deep learningright and so that first really almostdecade of deep learning was really aboutlearning recognizing and predictingright now as we enter the second decadeof deep learning it's really much moreabout creating and that's what's reallygotten exciting even more exciting stilland this is something that Jensen likesto talk about is that this generation ofAI kind of turns everyone went into anAI programmer because so much of it youcan do can be done in plain language sorather than having to write you know saya python script or something in torch orC code or what have you you canbasically put in your prompts and plainlanguage and be able to generateinteresting things whether that's textimages video 3D Graphics computer codein some cases even things like moleculesand DNA for things like drug Discoverysoagain we're really just still I think ina lot of ways learning all the differentways that can be applied and that'swhat's really kind of exciting aboutthis is seeing the applications thatfrankly we couldn't predict to me in alot of a lot of cases those are reallythe most interesting uh applicationsthat come out of a new technologyso just kind of covering this a littlebitumyou know we talk about the idea of afoundation model and this is a reallyimportant concept and we're actuallyBuilding Services out around foundationmodels uh in three areas these will becoming to Market on uh with Googleactually later this yearum aroundlanguage which is what's called Nemo fordrug discovery which is called bio Nemoand then for image generation which iscalled Picasso right but what's neatabout Foundation models is you startwith as the name implies a pre-trainedmodel that has been trained up to acertain point and can be deployed as isbut then from there you can take yourown data and do additional training withit to customize it for your particulardomain and that's what's really excitingbecause what that can do is dramaticallycut your training time because as youall probably know the training time forthese very very large neural networksespecially things like large languagemodels it can take weeks and in somecases months depending on theinfrastructure that you're running on sothat ability to very quickly train thesemodels using essentially what's calledfine tuning or sometimes what's calledprompt tuningum really allows you to get the modeltrained faster deploy it faster and getyour application to the market fasterrightso this is just sort of an overviewslide of our platform this isum uh what I'll call a simplified viewour platform actually has hundreds ofsoftware libraries we support of courseall the major Frameworksum and I'm going to kind of do a kind ofa very brief touch on the software pieceof what we do because Ann's going togive you kind of the double click onthat but one of the important things torealize as you look at this diagram isthat you know the hardware that you seeliving at the bottom of the diagram iswhat sort of what I think of as thefoundation of our house which is ourplatform the house itself is really thesoftware and again we're gonna we'regonna we're gonna touch on that quite abit more as Ann takes you through hermaterial it's an important conceptbecause you know there is still a whatI'll call a perception which is validbecause of nvidia's Heritage as a GPUcompany that we are still those GPU guyswell in fact in Nvidia we have twice asmany software Engineers as HardwareEngineers it's very purposeful and we dothat because software enabling is what'sbeen key to a lot of our success in theAI domain it takes the form ofFrameworks uh supporting all theFrameworks deeply and contributingsignificant amounts of code andEngineering work to these open sourceFrameworks it includes support forthings like accelerating inferencethrough things like tensor RT as well asour inference serving software calledTriton so these software elements um andagain I don't want to steal too muchAdvanced Thunder are obviously criticalto helping people deploy applicationsbecause as a lot of you know building AIapplications and working with AI ischallenging it's hard and so whatever wecan do to accelerate that to make iteasier to make it faster at the softwarelevel is a critical differentiator tohelp uh your you know Google customersas well as our own direct customers movefaster and get apple locations builtforeignso on Google Cloud today there are awide variety of instances available I'mgoing to highlight four of them you seeup here on the screen I've listed whatthe gpus are along with uh the name ofthe instance that that they power rightthe one that I'm probably most excitedabout with you this morning is the onethat got announced yesterday at Generalavailability which is the A3 instancethis is powered by our h100 GPU which isour current Flagship offeringum this GPU has features in it that wereexpressly purpose built for largelanguage models in particular a featurecalled Transformer engine which allowsyou to selectively and intelligently useFPA Precision on a per layer basis whenyou're training your models so that youcan optimize performance withoutcompromising accuracy and get the modelto actually converge because one of thechallenges of course with reducedPrecision is as the name implies it alsocan compromise your accuracy and if youraccuracy is compromised too much themodel won't converge and so whateverspeed up you got is basically the sameas spinning your tires faster and themutt because you're not going to getanywhere rightso um again so but both a100 T4 stillavailable still a very good solutionsfor for what they're designed to do buth100 and now also Google was the firstmover on L4 which is a great product fordoing inference as well as visualComputing and what I'd call sort oflight training but it's primarily a partthat's mostly purpose-built for forinferencing is very good at thatso a little bit more on this new A3instance from uh from Google again weare very excited to see uh Jensen joinedTK yesterday during the keynote andannounced that this is now coming intoGeneral availabilityum this will be available uh I believestarting in Septemberand uh what we're seeing you know againis it will be you know instances ofeight h100 gpus connected via our Envylink fabric technologyum and you know we'll also then have uhbasically uh enhanced networking which Ibelieve will be a a full bandwidth of1.6 terabits per second so substantialupgrade versus previous GPU uh instancesuh and we'll be again available sort offrom your from your Google Cloud consoleso again Greg glad to see that Google isnow uh brought this uh capability to themarket they joined several other cspswho've also announced Generalavailability for h100 so we are we'reworking with all of our CSP Partnersum to make sure that we can get as muchh100 capacity to the market as we canand so again very excited to see thatGoogle is now going to Generalavailabilityand this is going to be my last slide uhjust a brief touch on uh you know Italked a few minutes ago about theimportance of software and how in a lotof ways it really kind of represents thehouse of our platform well one of thethings we've done with that house iswe've also spent a lot of time workingin concert with Google Cloud tointegrate those Technologies into theGoogle Cloud platform and particularlythings like vertex AI for instanceum as well as some of their hybridofferings so I've got them listed onhere I you know I won't read it down foryouum and again and we'll talk a little bitmore about this you know but one of thereally important ones of course I'lljust touch on briefly is kubernetes uhbecause it is what allows you to againmanage orchestration manage things likeAuto scaling for instance with ourTriton inference serving softwareand it's a technology that we've spent alot of time with Google to make surethat our technology works very well withthe gke engine uh it's just one exampleagain I'm gonna I'm gonna let Ann kindof take you into the deeper the deeperdive on the software side and I thinkwith that that's actually a goodopportunity for me to welcome her to thestage and I'm going to uh turn it overto her and come on upam I onhi everyone thank you for starting yourmorning with us I am going to talk alittle bit more about the software layerbefore I start thereum how many of you use Nvidia gpus onGoogle Cloud today to dotraining andokay great AI development so if I'mdoing anything too basic you want me tospeed up just do something like this andI'll go a little faster I want to make agood use of both of our timeOh wrong wayso Dave already talked about ourplatform a little bit and he showed youthe diagram so we think about it asthree layers an acceleration layer wherewe have our Nvidia gpus and our hardwareand like Dave said most people when theythink of gpus and and seller acceleratedComputing that's what they think of it'sjust that the actual h100 or L4 thatthey might be using but there's actuallylike he said a whole software layer thatsits on top of that that enables youknow our customers who take fulladvantage of that GPU and help themspeed their time to development of an AIapplication and then also put thatapplication into production and supportsthat runtime of the application and thenat the very top layer we provide AImodels pre-trained models as well asFoundation modelsand today I'm going to talk to you andspecifically about that software layerin between those two layers we sometimesrefer to it as the operating system ofAI or the runtime of AI oopsso this diagram shows you a little moredetail about what's included in NvidiaEnterpriseand a very simple way if you can'tremember this is any software that wehave released for AI That's generallyavailable that's not an early accessessentially is included in NvidiaEnterprise and we provide full supportsecurity patching API stability acrossthat platform for Enterprises that areactually doing production gradedevelopment we also do regressiontesting so that we support priorgenerations of our GPU architecturebecause we realize every time you updateyour software you don't want to have toupdate your infrastructure that you'rerunning it on so we support as far backas our V100 which is still available inthe Google marketplace and the Googleinstances as well as up to the h100which will be available later this monthand L4 which we released earlier thisyearand then moving up the stack you'll seethat AI development box let's see if Ican use my little pointer does that workdo you see that here we we support everystep of the AI workflow with tunesoftware so from data prep with Rapidsand accelerated Sparkto model training which is what I thinkmost people are familiar with using gpusfor is that training stage where you'rereally moving to scale and you need highperformance and we provide optimizedpytorch as well as tensorflow as well asTau toolkit talcut toolkit is reallyuseful for using synthetic data and andaccelerating your time to production ofa development of a modeland then down into simulate and testingthis box right herewe provide tensor RT which helps youoptimize your model for inference andthen when you put your model intodeployment you know we provide supportfor triton inference serverum so that you can run and optimize yourmodeland then above that you'll see this boxwhere we have ai use cases and workflowsthat's where we almost I mean every GTCwe release a whole new family of AIFrameworks for very specific use caseswhether it's cyber securitymedical use cases with Frameworks likemonai parabricks for genetic genomesequencingumuh Reba for speech Ai and the mostrecent one is Nemo our Nemo frameworkwhich I'm going to talk about in alittle bit will be coming online as GAlater this month and it will be includedin NBA Nvidia AI Enterpriseand then to the left we see all of themanagement software so of course ifyou're putting AI in production you needto be able to manage your models sostarting at the top we include ourTriton inference service which actuallyallows you to manage that model onceyou've put it in deployment and dooptimizations reduce latency when you'rerunning that model Dave mentioned oursupport for kubernetes so we we supportgke as well as upstream kubernetes andthen we provide software down lower inthe stack for infrastructureoptimization as welland then at the top here you see wherewe list where how we've integrated withGoogleso obviously gke we support we'veintegrated Triton with vertex AI so it'savailable natively there for you to useas well as we made an announcement onhow we're supporting serverless dataprocwith acceleration and availability ofRapids with dataproc how many of you usevertex AIokay do you use vertex AI with gpus orprimarily CPU use casesyeah I'm seeing some heads and then howmany use dataprocokay a few okayso this slide is just a double click onthe performance that you get here let meget out of the way because some of youtaking pictures when you useacceleration for data processing andagain a lot of people think about gpuswhen they hit that training and theirtraining and developing a model butreally even at that first step whenyou're doing data processing and ETLthere's a lot of benefit to actuallyusing accelerated Computing and ouroptimization libraries available withRapids so in this diagram we'recomparing CPU with GPU CPU is in bluewe've normalized to the CPU and how muchtime it takes and then the cost thatyou're actually spending and one of theperceptions and it's true when you renta GPU instance it's going to be moreexpensive than your CPU instance butwhen you're Computing whether or not itmakes sense for you to use gpus andacceleration versus CPU for yourworkflow be thinking about how much timeit's going to take you and yourefficiency that you'll gain from thegpus and that's the point of of thisanalysis is to show you that you knowyou can do it in about a fifth a timeand about a fifth the cost if you'reusing gpus to do this workload of dataprep now every data prep job will notbenefit from acceleration so we actuallyinclude a tool that's what's shown hereon the on the left is the output of thistoolwhere you can run your your do a test ofyour workload and it'll tell you whetheror not there'll be cost savings if youmove this to an accelerated instance ornot or if you should better it's you'rebetter and more efficient just to keepit on a CPUso I hope you go by our booth we'regoing to be showing you a demo of thisas well in this integrationso in this slideum again I'm showing another step of theAI workflow but in this case we'reshowing inference and the benefit ofusing accelerated Computing forinference so the model here that we'rewe're running is the Sam the segmentanything model this is a model thatactually will partition out and segmentliterally every image into a photoyou're probably familiar withsegmentation if you use your iPhone andyou crop people out of your pictures andpeople who maybe have photobombed yourphoto you can crop them out and thenre-render that without the person in itso this basically is able to segment outanything you take a picture of or anyimage in a photo and it doesn't have tobe trained on those imagesso we ran this model again we shouldcompared it to CPU versus GPU becausethat's typically the decision you'remaking when you move into inferences doI need to accelerate this workload or isit better just to run this on a CPUinstance where am I what's mostefficient and cost effective so you cansee in this example you can actuallyhave much higher throughput throughput24 times higher throughput on theaccelerated instance of this model andbecause of that high throughput andobviously therefore also lower latencyyou're able to do the same workload atabout five percent of the cost is if youwere renting out CPU instances againevery model every inference workloadwill not need to be accelerated but youshould evaluate whether or not it makessense for the ones you might be runningespecially in production for yourbusinessso one of the questions we get a lot iswhy did we do Nvidia Enterprise NvidiaEnterprise is our commercial license forour software and I want to make sureeverybody understandswe will always as much as possiblesubmit to GitHub so we just announcedfor exampleum guard rails guardrails is a newframework for safety of generative AIworkloads it's available in GitHub rightnow we will always we will alwayspublish to GitHub we will always makeour software available for freecontainerized compiled on our NGCrepository but we also are now providinga commercial license to our software andthe benefit is we take on the the burdenand it and it it can take teams of 10 20people to support this software layer todo security updates as well as APIstability and long-term support so ifyou want to use an older version ofTriton we have customers who are usingTriton that they actually got two yearsago they want to have security updatesto that version of Triton but they don'twant to have to rev and update the wholeTriton model because if they do thatthey're going to introduce new apiswhich could break the code that they'vedeveloped on top of Triton so we will weprovide them with an API stable versionof Triton that also is maintained andsecureso what I'm showing here is these areactual scatter graph plots of thesedifferent Frameworks these are the mainones I showed in that first slide Iopened with so you see Rapids for dataprep you see um Pi torch for trainingwhich is very typical one everyone usesthat's our accelerated version of ittends to RT when you're preparing forinference and then also Triton and so intotal there's actually4471 software packages in this softwarethere areum 9000 dependencies between thosesoftware packages many of them are opensource and third-party and we maintainthe security the cves and patchingacross all of those as well as the APIstability and the interdependencies whenyou're on a subscription license ofNvidia Enterpriseagain some teams some Enterprises chooseto do that themselves and they buildtheir own platform they optimize it theyuse our software that's available NGCand the free versions of it and that'sperfectly fine too but for thoseEnterprises that want a more stablerelease we highly recommend you you knowget on a subscription for NVIDIAEnterprisewe also within the subscription providereference applications we call them AIworkflows they includeum jupyter notebooks Helm charts how-toguides as well as pre-trained models fora very specific applications of AI fromum building a chat bot cyber securitywith digital fingerprinting arecommendation engine there's I thinkseven total that we have available nowand the newest one is actuallydeveloping a co-pilot using generativeAI in our Nemo framework that's thefirst one shown there and that'll beavailable in mid-september with our Nemoframework releaseI'm going to talk a little bit moreabout Nemo again this is our frameworkfor doing generative AI so we includesome Foundation models we also supportCommunity Foundation models like llama2we support using Lang Chang which I knowis really popular and commonly used nowand then obviously our guard rails whichis an early access now to help keep yourgenerative AI models safe so it doesn'tstart you know rendering results forsomething it wasn't trained to do rightgiving a weather report when it'sactually supposed to be doing arecommendation of a product and then inbetween there right you see it'soptimized for data preparation which isreally important for generative AI rightyou take a model that's already 95percent there but you want to tune it onyour proprietary data so preparing thatdata is very important and optimizingthat data to tune that model and thenwhether you're training and we can helpyou scale it out to thousands of gpuscustomization doing p-tuning using thatyou know pre-trained that informationfrom your own Enterprise and thenactually you know taking that modelmoving it into deployment and optimizingit in your runtimeand again this will be packaged up aspart of our Nvidia Enterprise releaseand available in the marketplaceum on Google uh in in a couple of weekshere when we go to gaso this is just a summary of the valueprop of Nvidia Enterprise startingum you know with obviously the value ofaccelerated Computing I hope you guyswill think about accelerated Computingnot just for training but for your dataprep and for inference and for youractual run time because it really doesadd value and savings across that fullAI process and if you're not sure aboutwhether or not your workload willbenefit from acceleration look for ourtools like we do for dataproc we'reshowing in the in the booth so we canhelp you evaluate whether or notprocessing and accelerated Computinghelps you with the processing thatyou're doing then that next layer isjust how we made it Enterprise gradewith security reliabilitypatching as well as the manageabilityfeatures it's all available on GoogleCloud multiple generations of gpus andthen obviously we've included generativeAI capabilities with the Nemo frameworkthere's two ways to really get startedwe have a 90-day eval program that'savailable on our website on nvidia.comso that gives you full license toeverything in video Enterprise includinglike the AI Frameworks the managementsoftware Etc like the AI workflows arenot available on NGC some of thatmanagement software is not available forfreebut you can get all of that with the90-day eval and you can take that evalsoftware and run it on on a Googleinstance rightalternatively we also have it availablein the marketplace you can starting attwo dollars an hour per GPU you cansubscribe to Nvidia Enterprise and wealso have the ability to purchase itthrough private offer which means youcan also use if you have Enterprisecredits with Google you can use yourEnterprise credits to to access thelicense and purchase the licenseum and then uh let's see the next slideis super quick I just wanted to go oversome of our announcements we talkedabout some of these already the firstone is the h100 coming online later thismonth which is our latest GPUum the availability of Nvidia Enterprisethat's the second one there in themarketplace I hope you check it outum we also we didn't mention thisalready but Pax ml which has been aframework for llmum management development of largelanguage models has been only supportedon tpus we're now supporting it on gpusand it's available from our NGC catalogit's in Early Access when it becomes GAwe will support it as part of NvidiaEnterpriseand then the last one there is oursupport of dataproc serverless dataprocwith accelerationI hope you go by our booth the boothwill have three demos which willhighlight a lot of the use cases alreadywent over so the first one being thedataproc that I mentioned is availablein the booth where we have a Vertex AIdemo showing you how to use Triton withvertex AI for your run time and then thelast one is just showing a gen AI usecase using Nemo framework which isavailable through the Google marketplaceand I think they're showing how todevelop a chat bot or a pilot if youwill Copilotforeign"
}