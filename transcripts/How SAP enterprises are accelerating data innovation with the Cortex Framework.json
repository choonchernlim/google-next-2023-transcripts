{
    "title": "How SAP enterprises are accelerating data innovation with the Cortex Framework",
    "presentation_type": "Breakout",
    "categories": [
        "Data Analysts, Data Scientists, Data Engineers",
        "ANA103"
    ],
    "video_id": "96C-FQEtVZs",
    "time": "Aug 31 01:30 PM - 02:15 PM CDT",
    "transcript": "foreign[Music]good morningand welcometoday's session we will spend timetogether talking about how sapEnterprises are accelerating dataInnovation with the cortex frameworkto introduce you to today my name isAllison Hedrick I'm director ofsolutions of Cortex framework I havealong with me todayhi I'm Lucia subari and I'm thetechnical lead for cortex framework andand I'm Philip I'm director of data atGooglewe have a number of things that we planto cover in today's sessionfirst we want to cover what's top ofmind for many Enterprises and ourclientsnumber two we'll give you an overview ofthe cortex frameworknumber three we'll talk about how groupof ochocario is innovating with thecortex framework and some of the uniquethings that they are doing as part oftheir Journey with uswill help you understand where to learnmore as well as take a portion oftoday's session for those of you who arebold you see the mic in the center aislewill be taking questionsso with that let's get started what'stop of mind for Enterprises today arereally around three main areasnumber one is how to break down the datasilos between different applicationswithin the Enterprise and between andacross datathe second area is around how to improveinsights with data and last but notleast and most importantly for manyEnterprises it's about looking at how toinnovate and do so fasterso how are we helping our clientsaccelerate their Journey with data andAIand what does that meanabout two years ago we introduced thecortex framework into the marketand the cortex framework really focuseson three main things the first isa set of reference architectures thatcan help to simplify delivery of dataand analytics use cases and innovationthe second element is a set ofdeployment accelerators so this includescontent to help kick-start your journeyand the third area focuses onpackage services that are delivered withour partners that can help you tostreamline and fast track yourdeploymentso what's in the cortex frameworkwell first we're focused on theavailability of strategic data setsacross a variety of different datasources delivered through one Foundationthe second element of the frameworkfocuses on how to deliver insightsthrough packaged examples and solutioncomment content for many different usecases we've looked at the common usecase patterns across a variety ofdifferent scenariosand we're providing accelerator andpackage content for different types ofuse casesthe last area is that our framework thecontent we're developing the referencearchitecture examples are based on ouropen data and AI platformso what's in the cortex frameworkit starts with the ability to connect toa variety of different data sets anddata sourcesboth Google deliveredEnterprise such as sap for exampleas well as public and communitywe're flexible with the framework inmeeting you where you are in providing astream line in very flexible way toingest and manage data utilizing Googletooling or your tooling of choiceand in the framework itself we'reproviding packaged data models in Googlebigquery as well as data processingscripts and templateson top of that we make it very easy foryou to share data externally with ourapplication layeras well as realize analytics insightsand visualization faster with ourexample looker blocks and templatesas well as our machine learning contentthat now is starting to incorporate thelatest advancements with large languagemodels and generative AIwe're delivering all of this for acombination of different use casesChief among them include things likesales and marketing supply chainsustainabilityand financeas you take a closer look at one of thecornerstones of the cortex frameworkthere are a variety of different datasets what you see represented here isjust a sampling for purposes of thisdiscussionwe have a very rich set of data modelsthat you can start to take advantage ofand the benefit to you is that we'retaking the guesswork out of how tomanage and integrate data for a varietyof these different packaged use casesand scenarios and what's nice about thefoundation is that you can rapidlyapply the data foundation and contentacross a variety of different use casesChief among themdata based on your sap Erp basedenvironments what I'll do now is turn itover to my colleague Luciawho will provide you with a deep divearound what's in the framework and someof the technical aspects and highlightsso over to you Luciathank you Allison let me unlock mymachine because of courseit would do thatand if we turn it over to the demo Iwould like to show you how you actuallyget started I only have five minutesthough so I'll go as fast as I can butI'm happy to stick around for the Q a oroutside the room for one-on-one chatsour demo starts with GitHub and that isone of the great things about the dataFoundation that we're open source one ofthe reasons why we're open source isthat we know that each customer willhave their own unique business rules toapply and that means that you need to beable to tweak our models and you need tobe able to adapt them to those businessrules to make your business users happyso we're happy to provide this throughan open source frameworknow if you were to get started even ifyou're new to Google cloud in general ifyou haven't done any gcp or bigqueryrelated stuff before we got you coveredso if you head over to the GitHub forwhich we'll have the link afterwardsyou'll see that we have a lot ofdocumentation including the referencearchitectures theerts for the different data models foreverything that we have including sapccS4 Salesforce and the marketing contentthat today has campaign manager takeTalk and Google ads and then when you'reready you're going to click on this I'mgoing to show The Full Experiencelittle button here and this button willbasically clone the repository and startto set you up for a sample demoimplementation that will include testdata so if you're starting here what Iwould recommend is to get a project fromscratch a new one and have all thepermissions that you can for thatproject and and be sure that you canthrow it away and when you're readyyou're going to configure this deployerit's going to ask you what you want toimplement from all of our samples andit's going to check dependencies it'sgoing to check that you have the datasets that you need to get started andall the GCS buckets and you will havethe chance to choose if you want todeploy everything or subsets of ourcontent within our acceleratorsthe beauty about this is that it willshow you what good looks like in thesense that after this 25 minutedeployment finishes you will be able tojump into bigquery and see what the datadoes with our models so again this isgoing to take 25 minutes so I'm going togo into bigquery to show what the endresult is what you will find in bigqueryis in my case I choseall of the data sets so I have a set ofraw Landing data sets and change datacapture data sets and then we have thereporting layer so I cannot go into thedetails of what those other two datasets are but basically what you need tounderstand is that we build this under aprinciple of openness which means thatwe are prepared for you to use anyintegration tool as long as thatreplicates the exact same Sourcestructure so for example in the case ofsap if you were to use any integrationtool as long as you have the tables withthe same names and the same data typesas sap created them we will handle themin our reporting models I went straightinto the sap reporting here but you havemodels for a lot of other things if youwant to play with them we have over ahundred models today and these modelseventually get consumed by ourlooker dashboards now going back to theprinciple of openness that I wasmentioning the lucar dashboards includea lot of out of the boxsamples and dashboards to feed for yourbusiness users around finance and orderto Cache supply chain and inventorymanagement but if you decided to useanother reporting tool we're cool withthat too and you will find all of thelittle joints and filtering that weapplied in Locker in Sample view so thatyou can take that into a tableau forexampleso Allison was mentioning theapplication layer as well and I want togive that a little shout out theapplication layer is a nice way ofgetting onboarded into microservicesthat can tap into Data in the cortexframework so if you were to run ourmachine learning models that come out ofa box or product recommendation orcustomer segmentation you would use forexample the application layer that willdo do everything so that you have eithera microservice exposing an HTTP endpointor a pub sub to write back into forexample an sap system using the app SDKthe other thing that you'll find in themarketplace is demand sensing demandsensing is one of our samples thatcombines large language models andmachine learning to provide forecastingfor sales out of historical order salesorders and different data signals comingfrom Google search Trends and Google adsand for example the weathernow with that I am in my five minutesand I'm going to invite Philip to tellus how they are using groupobotic areacortex in grouponicariothank you Lucia[Applause]so before I talk a little bit about ourcase at Group Chicago how we use cortexI'd like to give you a brief overview ofwhat Chicago is and what we do because Iassume most of you have never heard ofus before sois a Brazilian beauty company so we makeand we sell beauty products for exampleproducts for personal hygiene fragrancesSkin Care shampoo sunscreen makeup andperfumeis a multi-brand company so we have ninenine home Brands our most popular brandis called ubuchicario which has alsobeen voted for the fifth consecutiveyear in a row as the most popular Beautybrand in Brazilbut we also have licensed Brandsinternationally known Brands such asAustralian Gold or bio oilsecond we are an omnichannel company sowe have more than 4 000 physical retailstoreswe have a single brand a multi-brande-commerce sites we sell through b2c B2Bdirect sales and we offer a trueomni-channel experience for ourconsumers through in-store pickup asingle loyalty program and a unifiedcustomer Viewand last but not least a Grupo chicarohas a very high degree of integration sowe operate on the whole value chainthis means we start with research anddevelopment then we produce that Pro theproducts in one of our three factorieswe distribute them and we also sell themthrough our various channels so havingsaid that we don't only considerourselves as a beauty retail company butas an ecosystem because we also offerdigital products and services to otherretailers which go products which go farbeyond just as selling perfume or makeupI brought you some big numbers to givean overview of how this translates inour like day-to-day operationsis now more than 40 years on the marketwe have more than 4 000 physical retailstoresour products are sold in more than 50countries we have 15 000 directemployees we make more than 600 millionitems per year we launched more than 600new products each yearand we also have the largest e-commerceBeauty Marketplace in Latin America andthis makes us also the biggest cosmeticfranchise on the planet so we have ownstores and we also have stores operatedby franchise partnersuh however buchikaru hasn't always beena digital company right we were foundedin the 70sand the company made a huge investmentfrom 2019 onwards so our technologytechnology team back then had about 200people and now about four years later weare nearly 3 000 people and a big partof this investment was also made in dataso the team I'm working withand one of our first tasks was to movedata to the cloud so we started quitelate in comparison to other countries sowe started in 2019 and we choosebigquery as the core of our dataplatformwhy did we choose bigquery because welike the on-demand model and we didn'twant to worry about sizing so we neededspeed that's why we choose bigqueryhowever our data stack is not justlimited to the bigquery basicallyeverything is built on top of a mix ofOpen Source Products for example for thedata catalog and other gcp products sowe use for example Pub sub we use dataproc data flow looker just to mention afew and vertex AI so all of our AImodels or ml models the pipelines areall running on vertex Ai and today wealready have more than 60 applicationsso using AI where we embedded AI intothe process or where we used AI tochange the processI brought here three examplesof how we use AI specifically to ourindustry so we use for example AI toforecast demand to predict demand wehave a very sophisticated model whichdoes this prediction based on a point ofsale and SKU level so we look how mucheach store will sell of each product andthis in turn feeds back into the factoryso we determine how much we want to sellhow much do we need to produce and thisnumber goes back to the factories andthen we use also AI to distribute thisinventory across all the stores So toavoid overstocking or stock outwe also use AI to determine the bestlocations for new stores so we areopening quite a lot quite a few storeseach year and since last year all thelocations are determined by a modelwhich uses more than 2 700 variables todetermine the best locationand the third example I brought you toshare with you is from research anddevelopment we have a product a modelwhich we call internal internally Liraand it's used to automate quality testsas I mentioned before we launched 600new products each year and we also makechanges to existing products so to theformulas of existing products and everytime this happens we need to run dozensof tests to make sure that the productwhen we change the formula still behavesthe way it was designed to behave forexample if we use sunscreen as uh whenwe change the formula we need to makesure that the product maintains itsviscosity if it's exposed for example toa higher temperatures let's say 30degrees Celsius that's I don't know howmuch is a Fahrenheit but imagine hotsummer day we need to make sure that itstill works the same way it was supposedto work previously we had to make allthese tests in the real world so we tookthe sunscreen put it in a box heatedthat boxand put it away that we came back twoweeks later to see if it still maintainscharacteristics and today we can use AIto automate like test and make thosetests you know like a virtualenvironmentwhich helps us to reduce time to Markand also helps us to create much moredetailed audit reports which arerequired by the regulatory organ inBrazil to get those products approvedso as you can imagine this the data usedby these models hasn't always beenavailable in Google Cloud lots of thisdata sets comes from transactionalsystems like sap in our case we areusing ECC and Hanaand we were suffering until 2021 to getthis data ingested into gcp so what wasthe issue these tables these tables hasbeen have been created like more than adecade ago and back then not necessarilyalways what you would consider bestpractices they were applied so we had notime stamp we had to run full ingestionsto get this data into gcp and we'retalking about tables uh huge tables withup to 100 billion rows so we had tosplit these big Tails into severalchunks we need it on the one hand andsap architect who was able to decipherthat schema for our Cloud Engineers sowe needed always two groups of people onthe one side sap experts on the otherside the cloud or or data engineers andthey had to somehow meet in the middleto make these injections happen and someof those it took several weeks up tomonthso which was a very frustrating processas we always not always but sometimes weencountered errors along the way thiscould be errors related to to codingjust coding errors but also problemsrelated to resources so that ourconnection for example got got cut offin the middle of ingestion because wewere putting too much load on theproduction databaseso if you think of a timeline we'retalking here about early 2021 mid-2021and at the end of the year we we thoughtthis is not sustainable not scalable weneed to change something because ourdata engineering team which was alreadyquite big then like 150 people they werespending about 80 percent of the timejust with data ingestion right soinstead of spending time to prepare datato clean data prepared for analysis forfor AI they were spending their timejust to get that data from sap to gcpso we tried SLT which helped us in thesense that we now had CDC but all of thedata filtering and deduplication stillrequired Engineers sothis is where cortex came into play andthis was a real game changer for us weimplemented cortex at the end of 2021and all of the heavy lifting sounderstanding which data is new whichactually do we need to bring into gcpall the mapping the filtering anddeduplication is now being done byCortex and since more than a year andfor us it was like this was a gamechanger because our Engineers were nowable to focus on tasks which create morevalue such as instead of spending theirtime to ingest data they were able tomove data from from the raw layer to thetrusted and refined layer where we havemore actually use for the data right sothis was a huge gain for us we were ableto cut the time the average time fromthe moment someone requested data untilthat data was available we have we usedto it used to take us nine days and I'mnot talking about these uh several theseingestions which took weeks a month butthe average time was nine days we wereable to cut the time to today two hoursas I mentioned before our data Engineersthey can also focus on tasks whichcreate more valueand we were able to reduce deployincidents to zero this was also veryfrustrating once you thought okayeverything should be working then aftera couple of days you got an area had tostart all over again and investigatewhat was going wrongand last but not least a cortex is alsoa huge accelerator for our ECC to S4migration so we are migrating right nowand if we had to do it the old way youcould easily add like one year at leaston top of the time necessary to tomigrate those systemshowever this is not the end so we areaware that cortex has many moreapplications and use cases than just sapdata ingestion we are evaluating thisright now we also see cortex as anenabler for decentralization becausethat group of Chicago we share thevision that data is for everyone wedon't want the data team to become abottleneck so we want to empower otherteams outside of the data team to ingestdata on their own without any dependencyfrom our own data teamso for example developers or dataanalysts they should be able to get datainto gcp without requiring any help fromthe data team and we created a aninternal tool which makes this happen sowe call it our internal ingestiveframework it's called Arc ninja it'sbasically a user-friendly interface withuh ready with connectors which arealready built in and the sap connectoris using cortex in the background sowhat's the benefit here the the userswho who use that tool they don't need toknow anything about sap about cortex howit works they just select origindestination frequency and likeuser-friendly drop down menu press thebutton and then this will ingest it willhappen without any any help from thedata team this in turn also acceleratesour journey to the data to a data meshsomething which you're looking toand uh we we are sure that cortex alsohas other applications besides thatingestionsso this is how we use cortex at Chicago"
}