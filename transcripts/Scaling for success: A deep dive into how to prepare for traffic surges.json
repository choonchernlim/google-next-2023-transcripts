{
    "title": "Scaling for success: A deep dive into how to prepare for traffic surges",
    "presentation_type": "Breakout",
    "categories": [
        "DevOps, IT Ops, Platform Engineers, SREs",
        "OPS302"
    ],
    "video_id": "3rA5kgHbG1A",
    "time": "Aug 30 06:45 PM - 07:30 PM CDT",
    "transcript": "foreign[Music]good afternoon folkswe're here to talk to you about scalingfor successI'm here with my esteemed colleaguesJason awitze I'm Belinda Uncle I runserverless engineeringso roughly 10 years ago almost to theday back in October 1st 2013.it's one of the most infamous sitelaunches and traffic surges in historyno it's not Taylor Swift ticketshealthcare.govmany of you probably heard this beforeit was the healthcare sign ups as partof the Affordable Care Act under theObama Administration and it was a waythat individuals could get health carewhere they had never been able to haveaccess to health care beforeas folks that already know the story nowit was a massive failure they had aquarter million users that crashed thesite in the first two hoursdespite having a quarter million usersdo you know how many people actuallymade it all the way through the site andgot Health Care on the first dayten thousand maybe a hundred thousandsixpeople got health care that daythe repair took months their initialbudget was about 94 million dollarspretty healthy budget for a website Iwould have thoughtbut actually repairing it cost1.7 billion dollarsto the American taxpayerswe are now very aware of this actuallybe just for inflation that's 2.2 billiondollarsit's a lot of moneyfor a traffic surge so we're going totalk to you today about traffic surgesand how to prepare for themnowwhy is this hard you know in a scalablesystem what we need is a system thatperforms with high reliabilitylow latency independent of load whichwill fluctuateall while managing for costnow when we talk about scaling for todaywe're going to talk about Cloud runCloud run is very good at Auto scalingas we're going to tell you more aboutand what is cloud run so Cloud run is afully managed serverless containerruntime environment you can build sitesjobs Services pick any language deployin seconds and underneath the hood ofcloud run is an auto scaling systemwhich automatically scales to the numberof instances needed to handle theincoming traffic it even scales down sowhen traffic goes down maybe at nightyou can go all the way down to zeroinstancessotraditional capacityfor Hardwarea place where you don't have an autoscaleryou got a provision for Peak utilizationeven if that Peak is short or kind oflumpy and inconsistent and as we allknow that can be very expensiveespecially if you end up provisioningfor a lot more than you need it'sclearly not a problem thathealthcare.gov had at the momentbut under provisioning is worse rightbecause you end up with latency errorspotentially some lost traffic lostconversions or lost Revenue all the badthingsso easy just over provision provisionfor Peak provision for what you expectand aim perfectly problem solved it isactually a legitimate approach there's alot of people still doing it so don'tknock itbut throwing money at the problem issuper expensiveandhow good are you at predicting thefuture anywayso predicting when and where load willhappen is actually a very complex in acomplex system is a very difficultproblem do you know which services areCPU bound which are i o bound whathappens when the traffic slows downwhat happens whenit falls overhow lumpy it is may depend on thingslike your load balancers and your cachesand how other parts of the systemperform and doesn't even ramp downwhen you don't need it so you don'toverpayhow well does your load testing actuallysimulate real user traffic when there'sa loadand then there's agility agility isabout how well does the system respondso to those changes in traffic so whenthe utilization needs you know drive usto do to scale up or scale down how welldoes that system respond now that ispartially depend on it on how good yourauto scaler is and we will tell youlater about how great Cloud run is withtheir Auto scaler but that's only partof the story the other part of it is howquickly your application loads on acontainer and can be ready for requestsand this is when we talk about coldstarts you probably heard the word coldstarts beforecold start is the time that it takes toload the application on the containerand be ready to respond to requestsuntil that very first request goesthrough there's a tax that you pay whichis the cold start now we say cold startit's almost like a little bit of a dirtyword in the industry but cold starts arevery normal so from zero requests tothat one first request that firstinstance of that first revision isalways going to have a cold start youcan't avoid itas soon as you need to scale up to asecond instance you're going to pay thatcold start tax again now you may getsecond third fourth 500 000 requeststhrough that first instance you're notgoing to keep paying that tax those arewarmed instancesand again what I'm trying to emphasizeis that this is normal there are someoptimization tricks even if you don'thave an auto scaler you can do thingslike give it more CPU give it more RAMall these things that will boost thatstartup time for your container and yourapplicationyou can look at pre-warming instancesright kind of a you know a little trickyou know so that you basically pay thecold start tax ahead of time before thetraffic surge but ultimately you needthat scaling logic to be nimble enoughto respond to traffic with the rightnumber of instances appropriately andquickly enough and not get bogged downby the cold start tax from your ownapplicationand last much like healthcare.govlearned you can't just scale up oneportion of the system without itpotentially affecting other parts of thesystem let's say I have a web serviceand it you know it does some synchronousright to a database now I scale up athousand instances of that my databaseis going to be struggling in it soundsreally dumb but you see it all the timeit happens all the time in productionsystems lots of opportunities tomitigate that of course right you can doa service in between you can Shard thedatabase you cannot use a databaseinstead of rental right to a queue orwrite to an event pipeline eventuallywrite to the database there's lots ofways to get around it we'll talk moreabout patterns for isolation a littlebit laterso how does Google do this so Google hasbeenfor a very long time on a very diverseset of systems think about all thedifferent products that Google makesand we have we make it incredibly easierfor our developers to build and scalesystems right we don't require that ourdevelopers have to all be scalabilityexperts instead we build a very scalablesystem that makes it easy for them to beable to scale and the way that we dothat is through one of our mostwell-known systems at Google which isBorgnow what is Borg Borg is Google'sinternal cluster manager it's the waythat we run containers at scale insideof Google and Borg was actually theoriginal system that can that kuberneteswas designed after uh and Borg itselfwas really tailor-made for Google'sscalability challengesnow you might be surprised to know thatcloudron is actually a productdescendant of app engine folks mayremember app engine it's still a verypopular product both inside and outsideof Google and with app enginewe could help developers you know writetheir own code and whatever languagewhatever Frameworks all the tools thatthey want package and container host itscale it only build for the resourcesthat you needin practice at Googleapp engine was the first way that weenabled getting user code to run onshared Borg and so cloudron actuallyinherits some of the best parts of appengine as well as the best parts of Borgeverything from containers toscalability all in support of pay as yougoand then the way that we build systemsat Google really supports the ability toscale and there's kind of three majorsort of principal areas that we'll talkaboutoopslike thatthe first is multi-tenancyso with a huge diversity of workloadsmany different resource types requireddifferent capabilities file systems Etcand then we have a very diverse set ofmachine types resources think about thethe many many years that Google's beenin production racking and stackingacross our data centers we then tail theboard is able to tailor resources tomeet the requirements of those workloadswith strong isolation on every instancewe can avoid Noisy Neighbor effectswe've been packed the containers formaximum efficiency while also providingprioritization of workloads whichdevelopers can help control but there'salso an overarching prioritizationmechanism and then we Auto scale andresize beautifully as resource needsFlex based on the needs of the fleetthe second is require stateless servicesfor those of you have heard the phrasecattle not pets all Google workloadsneed to be cattle not pets there aresome exceptions but mostly they need tobe stateless and what that means is ifone dies it's hurtwe don't have to take it to the side anddiagnose it we assume it is going to bereplaced and that means that that jobthat workload cannot require interimState information everything must berestartable and repeatable and thisallows us to get back to a healthy Statevery quicklyand automatically developersintervention is not requiredand so in Google failures are a verynormal occurrence they are notconsidered an eruption and then third isexperimentationso we Advocate very strong productionprinciples that allow us to manage therisk of disruptive changes at Googlewe support experiments with Progressiverollouts it includes the ability to rollback and we Implement inside of ourexperiment system metrics for measuringfor correctness for making sure that hasthe right outcome in production we canthen gradually ramp up changes by apercentage for targeting certain regionswe can add criteria we can verify thechange depict problems early and what itmeans is when we roll out a bug and wedo roll out bugs they happen all thetimebut with this mechanism we limit theimpact so when a bug goes into prod itonly affects a small portion ofcustomers before it's caught and rolledback so we make it safer for ourdevelopers all of these core ideas havemade their way into Cloud run as aproductbut I'm going to stop talking to youabout it might be easier just to showyou so my esteemed colleague would saywould love to give you a demo and talkto you more about scaling with Cloud rungo aheadOkay soI'm here to send many requests to acloud run service but let me explainwhat this example service does first itis a gets a request it will wait for onesecond and then it will compute a fewprime numbers to to burn some CPU Cyclesand I hear you thinking is this a goodexample like raise your hand if you'reComputing prime numbers in productionI was afraid this would happenumno but really your service might looklike this instead right you get arequest and then you need to send arequest or a query to a database to getsome data and that's when the waitingstart and when you get the response backit will um you'll need to process itmaybe put it into an HTML template andrender it back to the user orum yeah well call another system andburn some CPU and that's the same thingas I'm doing in my example serviceI hear you thinkum the whole system needs the skill Imean it's good that you can scale allthose containers and but what about thedatabase I mean Belinda referred to thatit's it's not about the containerscontainers in isolation I don't care ifyou skill 1000 containers the wholesystem needs to scaleI know but this demo is just about Cloudrun and Jason is going to teach us somepatterns how to deal with servingscaling the entire system and servingserving all the requests reliablynow how do I generate load for myexample service I I use an infinite Loopso while through send requestsand I use in fact I use many of thoseinfinite loopsand I I I've also built a littlevisualization right because yeah well ithas to be live because otherwise itwon't be believable so the visualizationrepresents a load generating client as ared circle and this is not a user it'slike the infinite Loop because mostusers don't like refresh all the timeunless well maybe if the system is tooslowand this is what it does right all thoseinfinite Loops send requests to my clubrun service and Cloud run startscontainer instances and I representthose as green squaresand I show the green square as soon as acontainer instance is ready to serverequest right so container instance lifecycle club run decides to start acontainer instance it starts and as soonas it's ready to serve requests I paintthe Box on the screenso that might mean that you want to seea bit of lag in the visualization rightso a bunch of requests coming in andclever on the side too this is a lot Ineed to start container instancesum I will take the container intostartup time before you see the requestnow let's get started and this is thelive part of the demo I live demos arealso always a bit scary I practiced thismorning and this was the uh the thingthe what I saw in Cloud monitoringbut um yeah well let's let's goum I have four modes like no trafficwell that's where we're at now regulartraffic a traffic search and Jason toldme not to press the last button but Iwill do it anyway and there's the numberof requests which will count uphopefully and very important the successpercentagelike maybe if we're scaling out requestmight be a bit slower but it matters ifwe serve them or not so hopefully thiswill say 100.are we readyI don't feel ready but I'm going topress regular trafficand we'll see 10 infinite Loops appearand Cloud run is going to start yeahthree container instances and everycontainer instance is one CPU and 512megabytes of RAM120 requests served all successfulso nowI'll press search traffic and we'll gofrom 10 clients to 2 000 clientsum infinite loopsin well millisecondsthere you goand Cloud run starts to scalethat will take a whileyes there we gookay apparently 60 60 instances isenough to to handle the loadwell it decides that it needs moreright and we're not dropping in droppingrequests that's kind of the importantpartbecause well of course if if there's asudden traffic increase your lensingmight it might increase like if you wantto prevent that you need to have a lotof instances of warm already rightthat's why we have idle minimuminstancesum yeah do you want me to press the lastbuttonI was afraid you wouldokay for six thousand eight thousand tenthousandI wonder why I set the limitwell this this is gonna exercise Cloudrunnumber of requests served isI think I'm gonna stop when it's at onemillion oh it's not really reallydeciding to start morelike by default you can get up to 1000container instances in a cloud runserviceum but that's a maximum that we can liftfor you if you need even more but likekeep in mind this is already uh well 800cores so that's likely enoughfor most workloadsit worked I'm happywas this a good showcool I'll I'll turn off the traffic nowand we'll likely seeCloud run reacts quickly and reduce thenumber of constrained containers becauseit will scale down andthere's a couple things you need to knowwhen Cloud run starts scaling downumby default the pricing model on cloudrun is such that you don't pay for thecontainer instances while they if theydon't handle requests rightso there's no request coming in I'm notpaying for those 958 container instancesand we guarantee that in 15 minutes theywill be stopped but likely a few aredisappearing already so all like on thechart I showed earlier you could seethat it would like gently taper offthat was uh my demo and I'd like toinvite my co-worker Jason on stage who'sgoing to teach us all about like themore difficult parts of I mean gettingcontainer instances goes is easy but thehard part is in actually designing thesystem so Jason coming upthank you that's ityes[Applause]I assume neopolis was we'd say not justme coming on stage but I'll take itum okay we are back to the slides hieverybody I'm Jasonthat was the demouh so Vita just showed you a demo ofsynthetic load to Cloud run and how it'sable to scale up and Belinda talkedabout how we solve for some of thesescaling challenges at Google and so withCloud run what we've tried to do isbring some of those capabilities throughthe product in Cloud run without youhaving to do any of it yourself sonatively Cloud Run is multi-tenant itruns on shared Borg why should you careabout that actually you shouldn't butwhat it does allow us to do is give youthat behavior that we talked about whereyou only pay while the request is inflight now we can do that because of themulti-tenant nature of theinfrastructure and we're able to binpack these workloads efficiently andthat allows us to provide thatcapability to you it automaticallyscales as we saw we use two main signalsone is the number of concurrent requeststhat we're receiving on an instance andthe other is the CPU utilization of aninstance what if it demonstrated wasone core per container so I think we gotup to 900 and something cores in youknow a few seconds which is prettyimpressive you can also do that acrossregions so you can have as many Cloudrun services in as many regions as youwant and scale them sort of as a unitand again that's really advantageouswith the scale to zero and pay for whatyou use model because if a Region's notgetting any traffic it's costing youzero dollars so you can sort of have asmany regions as you like for a very lowincremental costand lastly in the Box comes trafficsplitting which is our sort of versionof the experimentation that Belindatalked about where rolling out a versionof your application is not just rollingout a version of the source code it'salso rolling out the configuration thatyou've chosen as it relates to scalingso Belinda talked a lot about some ofthe challenges and some of the pitfallswith scaling what I wanted to do is takeyou through some actionable tactics sometechniques that you can use to help yourapplication prepare for these unexpectedsurges in traffic there's really fourthings that sort of operate in a in aloop here the first is just tounderstand where you are today measuringwhat your cold start time is for examplewhat your goals are what are you tryingto achieve from a latency standpoint andthat will help inform these other threestepsthe second one is workload optimizationthis is your application what can you doto reduce the cold start time to reducethe execution timeif your application would benefit frommore concurrent requests on an instancedoes that have implications to yourapplication are you going to run intorace conditions and Deadlocks and thingslike thatthe third box there is system designthat's more about the architecture ofyour application is it designed as a asa as a model to scale well or is it justgoing to be hard to scale no matter howgood the infrastructure underneath it isum and lastly is the systemconfiguration that's how how have youconfigured the system itself in thiscase Cloud run what knobs and levershave you exercised there to best fityour applications so we're going to gothrough all three of these uh stepsokay workload optimization blendertalked about cold start and cold startcold stop cold stop keep hearing aboutit I wanted to just do one layer of theonion peeled away just took a little bitmore of an explanation as to what thismeansso the green is the request coming inand the yellow is the response going outin Cloud run the request hits a frontend that we control part of our dataplane and there's a little bit of timespent there to to route that request toan available instance or in the case ofno available instances to struct thesystem to create a new onemost of the time is going to be spent inyour container processing that requestand then the response comes back and thedifference between the two is what wetypically refer to as latencyin a cold start scenario the requestcomes into our infrastructure in thecase where there wasn't an instance orwe felt that there weren't enoughinstances we need to create a new onethere's that penalty that tax thatBelinda talked about that we refer to ascold start which is just additive itjust adds to the latency because we haveto sort of wait for that instance tobecome viable then send the request toit and then it has to process it arequestnow if you look at this andwe've said a lot about how good ourscaling system is you might think toyourself oh actually I had one otherpoint here and that is that if if thatend in latency maybe your client hassome sort of timeout configured so atthe extreme limit you might startgetting errors because your clients aretiming outum you might look at this and say wellokay but why didn't y'all just createthe container before you needed it andconsume that cold start penalty when itwasn't neededum and in fact we do we do exactly thatbut I wanted to just do a little bit ofa further Deep dive into why thatsometimes is not enoughso this is a chart imagine that so onthe x-axis is time on the y-axis is loadload could be number of requests QPSwhatever you like and so you can see inthis chart it's sort of growing slowlysteadily and then we see this uptickthis sort of Rapid surge in trafficthe light blue boxes here arerepresenting the number of instances wesaw in I should have chosen greensquares because that was like it was inwhich demo but neverthelessumand so when the traffic is graduallyRising there's a point at which we havea signal that says hmm we probably mightneed a new instance soon this is what Ireferred to sort of spinning it up aheadof time so that's the red line therelet's say it's CPU utilization and theCPU gets to a certain point where we sayyou know what we think we're going toneed a new instance soonso we start spinning up an instance andit takes a certain amount of time tostart but actually we don't need itimmediatelythose two green lines are representingthe time at which we get the signal okayyou need a new instance and the time atwhich we need the instance because thetraffic hasn't gotten yet to the pointthat we need it so we've gotthat amount of time to react we've gotthat amount of time to create aninstance and get it ready before thetraffic actually before we actually needit so that works finethen later on when this Spike happens wehave the same scenario we get a signalthat's the red line and we need that newinstancewhen the traffic reaches that point butin this case that time is much lessand so what this is saying is thatthis the steepness of the curve intraffic determines how much time we haveto reactnow if your cold start time is longerthan the time that our system hasavailable to react to the change intraffic then you can run into thissituation where you're not going to havean instance ready when the traffic hitsso as good as you know outside ofchanging physics which we're alsoworking onthere is just a fundamental limit that'sjust the nature of how the universeworkshaving said that cloud run does have afancy feature that we call Rapid scalingand you may not have noticed it but youwould have seen it in the demo that wejust saw whererapid increase in traffic and then asort of slow increase in instances andthen a sudden increase in instances andwhat the system is doing is saying okayyou've got new traffic well you'vereally got new traffic and so we weremore aggressively allocate instances toaccommodate for these very steep Curvesin trafficokay so that's all about optimizationumthe shorter you can get that cold starttime the steeper the curve you canhandlethe next thing I want to talk about isdesignBelinda and avici both mentioned somepatterns there are many patterns thatyou can Implement I just chose two fortodayumasync deferral and I don't know if theseare really the names that they'resupposed to be called that's just what Icall them also known as the noteverything has to be synchronous youknow patternumso let's say we have some applicatione-commerce application and there's aendpoint or a step or a function that isordering items so the customer'sordering an itemthere's all sorts of problems with thisnumber of steps by the way raceconditions whatever let's pretend theydon't exist and this is real so we'regoing to check the inventory to see thatthe items available we're going to checkthe payment instrument their credit cardon file as far as we can tell is validthen we're going to create the orderwe're going to charge the customer themoney we're going to update theinventory to say okay we've got one lessitem now because they just bought oneand then we're going to send them aconfirmation name and that's sort of thestep of ordering an itemand then we look throughthe check inventory and check instrumentpretty quickly they're just doing a readoperation on some data source the createorder is a little bit slower because wehave to do an insert and that's a littlebit slower charging customer maybe we'reusing some external payment Gateway andthat is a bit slow uh updating inventorymaybe there's some sort of mutex we haveto lock on to check that the inventoryis still available and Etc and then forsome reason we have a very slow emailserver and that takes foreverand sowhat can we do to make this faster wellif we look at it we might say well noteverything has to be synchronous herethere are some things that we can deferto a later time and so we might look atthe slowest of these steps and soactually we're going to move thosesomewhere else we're going to simplifythe synchronous part of the transactionto just be these three steps and thenwe're going to enqueue a task onto someasynchronous Queueat ideally a cue that has this semanticwe refer to as at least once so thatwe're guaranteed that it's going tosucceed and then you can execute thoselast three steps on another Cloud runservice or you can use some sort ofworkflow if you want to havesophisticated um orchestration of thosestepsthis has a couple of advantages one theexecution time issmaller and so you can back to thatchart before you can scale more quicklyand the second Advantage is that you canpotentially get better reliabilitybecause you're moving some things offthat critical path maybe your mailserver is unreliable within at leastonce queue you can have retrie semanticsand so on that will move that lessreliable component of the critical pathso this is one thing that you can dofrom a system architecture standpointthe other thing is thisimpedance mismatch that Belinda referredto or this idea that the scalability ofthe front end is so good that it ddosedyour own database and we've actuallyseen this happen inside Google where somany front-end Services spun up andopened so many connections to a back endthe back end is melted and that can havecascading failures for other systemsthat depend on that backhand and so whatwe generally do in these cases at Googleis create an intermediary proxy orintermediary service so in the case of atraditional let's say SQL database thathas a fixed number of connectionsavailableyou create a service that is the onlyone authorized to access the databaseand you manage those connections quitetightly and then other services feedinto that intermediate proxy servicein Cloud run you can do this so thegreen boxes could be another Cloud runservice and you can impose a maximum onthe number of instances so if yourdatabase has a fundamental limit or ifit's an external API and you have quotayou can impose a maximum thereum and if you want to get really cleveryou can in that intermediate servicestart thinking about things likeimplementing a priority system so reallyimportant Services come first and lessimportant Services get a lower priorityand that would be mediated by thisintermediate but what I've called here afan in proxyokay lastly is the system configurationso this is how in the context of cloudrunhow do we recommend you said it's onlyreally four things that you need tothink about in Cloud run one is andwe've alluded to both of these I thinkminimum and maximum instances so minimuminstances is rather than scaling to zeroscale to some integer one two threewhatever it may be the zero to onescaling is sort of never going to goaway as a challenge if they're if youwant to scale to zero You're AlwaysForever going to have a cold start onzero to one unless you specify minimuminstances of onewhat that does is it keeps an instancearound but it because if that instanceis Idle like it's around and it's alivebut it's not being used then the CPU ismostly idle and so it's actually chargedin Cloud run at about a 90 discount soyou can have those instances sittingthere waiting for traffic when traffichits you've already paid the cold startfor that instance so you that zero toone scaling event sort of goes awayand maximum which I just referred tobefore is imposing an upper bound sothis sort of lower bound upper bound ofscalingconcurrency I don't know how readablethat chart is but concurrency in Cloudrun refers to how many requests areserved simultaneously on a singleinstancethis is probably relevant or maybefamiliar to those of you who have used afunctions as a service product likeCloud functions or like Amazon Lambdawhere it's a single concurrency modelwhat that means is that a request comesin and it's allocated to an instance theinstance serves that request if anotherrequest comes in at the same time orvery close to it it will allocate a newinstance and it'll go to the secondinstance that's a concurrency of oneand you can see the great efficiencygains that you get by having greaterconcurrencythe more concurrency you have the fewerinstances you need to serve the sameamount of traffic the fuel cold startsyou'll incur the better you can scale soit all sort ofleads to the same conclusion so thatorder item example that I gave before Iwas talking about execution time Iwasn't talking about cold start butshorter execution time means moreconcurrency on that instance typicallyso you can see here in the example thatfor approximately the same amount ofload with concurrency of one the systemneeded 550 instances to serve thattraffic with a concurrency of 80 we onlyneeded 150 instances so that would havebeen 550 cold starts versus 150 goldstuntsthe third thing is instant sizing uhvery often not always but very oftencold start is CPU boundit's a lot of loading of data andloading of libraries and there's i ohappening but it's a lot of CPU actionand so more CPU can reduce the coldstart timeum and then lastly is region selectionuh in gcp not all regions are the samesizeum and so if you really if you need ifyou have a very high scale requirementand you need way more than a thousandinstances maybe you need ten thousandthen you want to be sure that you'rechoosing the right region if you look inCloud console on the quotas page it'lltell you what the absolute maximum is ona per region basisand when I said there's only four thingsthat you need to remember I liedthere are five and the last one iscoming soon that's why it's a fifth oneit's a new feature we're introducingcalled service level minimum instancesI'm not in charge of brandingbut what it really means is that you canset previously for anyone who's usedCloud run will know that minimuminstances feature is on a revision levelor a version level and in order tochange it you needed to redeploy yourapplication which is fine but kind ofannoying what this allows you to do isessentially change that value withoutredeploying the application so the usecase here is things like if you knowthat there's going to be an event maybeyou're launching a product or you'redoing some big announcement and you knowthere's going to be a spike in trafficyou can schedule an increase in thisminimum instances using a cron blackcloud scheduler or something like thatand so you can preemptively spool up abunch of instances and make them warm inanticipation of a surge in trafficthe last slideso just to summarize step number one isto measure what you're doing where areyou trying to go what your goals arespend some time optimizing your coldstart time your execution timelook at those ways to move things offthe critical path if you can make themostynchronous think about the design ofyour systemsometimes these settings take a while todial in what's the right CPU settingwhat's the right concurrency these areall very workload specifics very hardfor us to know exactly ahead of time howmuch CPU would be optimal for yourapplication and so using those gradualroll out these traffic splittingmechanism to iteratively find The SweetSpot in your scaling setup"
}